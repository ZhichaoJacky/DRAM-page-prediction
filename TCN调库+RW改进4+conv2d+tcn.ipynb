{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TCN调库+RW改进4+conv2d+tcn.ipynb","provenance":[{"file_id":"1JtFWMeXCcrMbTDJ2Rg7N1IPK64bSOtP_","timestamp":1622640195520},{"file_id":"1RcfoEvn4I0gV1j5WkMOXl23zI-RSuOEM","timestamp":1622382721188},{"file_id":"1vaNf0HRXR3E-dJO4vpWSs5V0sNgVjC1R","timestamp":1622356193317},{"file_id":"1bQa6QL4ORvg1Loe9WF6fp5aftI9hyLuW","timestamp":1622262491358},{"file_id":"1wQXUyPnj2ll0N7sJ3yvxF4ErF1qwjU1C","timestamp":1620531531429},{"file_id":"1bZPiq4pRkHVgTQUjHCsziJ0OmhigPygK","timestamp":1614738073155},{"file_id":"1APMCIHNRt2NDM1-papP-8oT2xBpw26Th","timestamp":1614324965689},{"file_id":"1ES2hTLd47ynWZ5bZD3Thx8X5Ly_Y8JqR","timestamp":1614003702181},{"file_id":"1iHtyaEmu6GYHWlYXtqnBJRqFLMHlwjL_","timestamp":1613965734903},{"file_id":"1kqc8mHLk9exL_48y3ZnmtmzM9B4_93t3","timestamp":1608117183616},{"file_id":"1PqQDgTJjBTQmMVw7WMGA6tBPeEIqAufP","timestamp":1606922827989},{"file_id":"1p-rTF5Apa_4zzv8RpMaG6bW9IsoK32wW","timestamp":1606836180688},{"file_id":"1PvDnsIyqJbA8muhVQrRGgatHjzNvXmOz","timestamp":1606661874647},{"file_id":"1PIHkfvfEXhpqp7MUa21azXNg8e-_AKHZ","timestamp":1605965023134},{"file_id":"19_JkylFRwWNs7kq8-slLsW7MI2zjrSxY","timestamp":1605940409417}],"collapsed_sections":[],"authorship_tag":"ABX9TyMsOhsZMPAKnzteWjgUYGpU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BhqH-sPqNAUI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623222947730,"user_tz":-480,"elapsed":25988,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"e4787697-04cb-4053-ef15-5fdc07aad769"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iX7UiIiYNW73","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623222949743,"user_tz":-480,"elapsed":2016,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"b01bdc13-d4e2-43bc-ea7d-68ca52c0862f"},"source":["#多输入多输出\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","!/opt/bin/nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed Jun  9 07:15:50 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G50r8UONNZO8"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from pandas import read_csv\n","import math\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Multiply, Lambda\n","from tensorflow.keras.layers import LSTM, GRU, Embedding, Flatten, Reshape, Layer, Average, Add, Lambda, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Conv1D, Convolution1D, MaxPooling1D, concatenate, Input, Bidirectional, RepeatVector, Concatenate, Dot, Permute, TimeDistributed, SpatialDropout1D, AveragePooling1D, UpSampling1D\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.layers import Activation, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","#from tensorflow.keras.optimizers import *\n","tf.compat.v1.disable_eager_execution()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiQEZz15Q1zl"},"source":["#调用self attention\n","#!pip install keras-self-attention\n","#!pip uninstall keras-tcn --no-dependencies\n","#!kill -9 -1\n","#!pip install attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"ty-px0wRNkgx","executionInfo":{"status":"ok","timestamp":1623222953225,"user_tz":-480,"elapsed":2990,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"140fc725-ddd3-463b-dcf3-05067e612582"},"source":["#导入文件\n","#out版本\n","input_file=\"/content/drive/My Drive/Benchmark_PageGap/PageGap_RW/PageGap_526_RW.out\" #521  \n","dataset=read_csv(input_file, header=None, index_col=None, sep=' ')\n","start=2800000 #5060000\n","end=3000000  #5260000\n","datasetRW=dataset[0].values[start:end]\n","#dataset.drop(, axis=1, inplace=True)\n","datasetD=dataset[1].values[start:end]\n","dataset\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>-43</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>-43</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8136228</th>\n","      <td>0</td>\n","      <td>-7552</td>\n","    </tr>\n","    <tr>\n","      <th>8136229</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8136230</th>\n","      <td>1</td>\n","      <td>7552</td>\n","    </tr>\n","    <tr>\n","      <th>8136231</th>\n","      <td>0</td>\n","      <td>-7552</td>\n","    </tr>\n","    <tr>\n","      <th>8136232</th>\n","      <td>1</td>\n","      <td>7552</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8136233 rows × 2 columns</p>\n","</div>"],"text/plain":["         0     1\n","0        0    43\n","1        0   -43\n","2        0    43\n","3        0   -43\n","4        0    40\n","...     ..   ...\n","8136228  0 -7552\n","8136229  0     0\n","8136230  1  7552\n","8136231  0 -7552\n","8136232  1  7552\n","\n","[8136233 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"8qjpIrm88iQR","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1623222954192,"user_tz":-480,"elapsed":970,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"c4568b5a-42b5-467b-c7ca-7ec75cfe3ce0"},"source":["#查看数据分布\n","plt.plot(datasetD, '.')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHFUUa0Bs39K","executionInfo":{"status":"ok","timestamp":1623222954193,"user_tz":-480,"elapsed":26,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"28a7288c-9020-4053-8347-9cb5d707eb28"},"source":["#读写数据\n","datasetRW"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 1, 0, 0])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGeo7YvUWVgU","executionInfo":{"status":"ok","timestamp":1623222954194,"user_tz":-480,"elapsed":23,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"7f31d426-a022-466c-dd4e-9123607efbf3"},"source":["#delta数据\n","datasetD"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([     0,  15703, -15703, ...,   -241, -15296,  15537])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPW886To9k5j","executionInfo":{"status":"ok","timestamp":1623222954194,"user_tz":-480,"elapsed":21,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"c87fff25-68c1-440d-86fd-c16847869bb6"},"source":["#提前先分好训练集和测试集\n","train_size = int(len(datasetD) * 0.8)\n","print(train_size)\n","train_Delta = datasetD[:train_size]\n","test_Delta = datasetD[train_size:]\n","train_RW = datasetRW[:train_size]\n","test_RW = datasetRW[train_size:]\n","print(train_Delta.shape)\n","print(test_Delta.shape)\n","print(train_RW.shape)\n","print(test_RW.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["160000\n","(160000,)\n","(40000,)\n","(160000,)\n","(40000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vongkSjsKYM","executionInfo":{"status":"ok","timestamp":1623222954195,"user_tz":-480,"elapsed":20,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"0b7482cf-fc53-4fde-a4d6-1d810a346a2b"},"source":["#将训练集的delta转成list和set\n","list_a=train_Delta.tolist()\n","print(\"Train_List长度：\",len(list_a))\n","\n","list_b=test_Delta.tolist()\n","print(\"Test_List长度：\",len(list_b))\n","\n","#查看delta的种类\n","set_a=set(list_a)\n","type_n_a=len(set_a)\n","print(\"Train_Set种类：\",type_n_a)\n","\n","set_b=set(list_b)\n","type_n_b=len(set_b)\n","print(\"Test_Set种类：\",type_n_b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train_List长度： 160000\n","Test_List长度： 40000\n","Train_Set种类： 1304\n","Test_Set种类： 566\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhRc4LN87L-h","executionInfo":{"status":"ok","timestamp":1623222954195,"user_tz":-480,"elapsed":18,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"46febaee-32b0-4b39-c249-a358abd3e842"},"source":["value_cnt_a = {}\n","for delta in list_a:\n","\t# get(value, num)函数的作用是获取字典中value对应的键值, num=0指示初始值大小。\n","\tvalue_cnt_a[delta] = value_cnt_a.get(delta, 0) + 1\n","\n","# 打印输出结果\n","print(\"Train字典收集训练集（delta类别-次数）：\",value_cnt_a)\n","print(\"Train各delta值：\",[key for key in value_cnt_a.keys()])\n","Vals_a=[delta for delta in value_cnt_a.values()]\n","print(\"Train各delta值的次数：\",Vals_a)\n","print(\"Train_delta种类：\",len(Vals_a))\n","\n","value_cnt_b = {}\n","for delta in list_b:\n","\t# get(value, num)函数的作用是获取字典中value对应的键值, num=0指示初始值大小。\n","\tvalue_cnt_b[delta] = value_cnt_b.get(delta, 0) + 1\n","\n","# 打印输出结果\n","print(\"Test字典收集训练集（delta类别-次数）：\",value_cnt_b)\n","print(\"Test各delta值：\",[key for key in value_cnt_b.keys()])\n","Vals_b=[delta for delta in value_cnt_b.values()]\n","print(\"Test各delta值的次数：\",Vals_b)\n","print(\"Test_delta种类：\",len(Vals_b))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train字典收集训练集（delta类别-次数）： {0: 33074, 15703: 32, -15703: 1, -256: 16200, -15447: 28, 256: 9467, -15446: 161, 15702: 163, 255: 354, -199: 1, -15247: 1, 1: 519, -15445: 162, 15701: 163, -201: 1, -15245: 1, 15488: 198, -15488: 82, -203: 1, -15242: 1, 213: 263, 15700: 146, -15444: 147, -15701: 1, -206: 1, -15239: 1, 212: 162, -207: 1, -15237: 1, -15443: 166, 15699: 149, -15700: 3, -210: 2, -15233: 1, 15698: 162, -15442: 135, 210: 193, 211: 196, -212: 6, -15486: 7, -15441: 133, 15697: 148, -15698: 24, -470: 1, -15484: 4, -15697: 16, 209: 135, -216: 1, -15481: 3, -15440: 154, 15696: 171, -219: 1, -15477: 4, -15439: 161, 15695: 160, -15696: 19, -220: 1, -15475: 4, 208: 185, 207: 173, -223: 40, -15472: 3, 15694: 132, -15438: 148, 206: 193, -15695: 1, -225: 61, -15469: 9, 15693: 156, -15437: 166, -15693: 2, -1: 318, -227: 159, -15466: 7, 205: 200, 15692: 155, -15436: 162, -230: 68, -15463: 1, -232: 1, -15460: 1, 203: 166, -15435: 153, 204: 194, 15691: 150, -234: 1, -15458: 2, 257: 338, -15692: 1, -15434: 169, -236: 1, -15455: 3, 15690: 167, -15690: 1, -239: 1, -15451: 1, 202: 227, 15689: 167, -15433: 161, -240: 1, -15449: 3, -15432: 148, 15688: 145, -243: 1, -15190: 1, 200: 173, -15689: 1, -15688: 1, -245: 1, -15187: 1, -15431: 162, 15687: 170, -15687: 8, -247: 1, -15184: 1, 15686: 161, -15430: 164, -249: 1, 198: 16, -252: 1, -15178: 1, 15685: 146, -15429: 127, -254: 1, 197: 10, 196: 23, -15428: 148, 15684: 147, -15685: 18, -512: 3, 512: 1, -15173: 1, -15684: 16, -192: 956, -15492: 5, -15427: 159, 15683: 154, -259: 1, -15169: 1, 195: 14, 15168: 64, -15168: 46, -260: 1, 320: 30, -15487: 39, -15426: 170, 15682: 168, -15683: 1, -263: 1, 194: 2, -15425: 277, -265: 1, 15681: 243, -267: 1, -15478: 2, 15680: 112, -15424: 6646, -15681: 7, 15424: 15783, 258: 5, 193: 13, 253: 139, -15678: 157, 192: 451, 252: 167, -15676: 168, -15680: 71, -15679: 131, -15423: 105, -7: 3, -15672: 161, 15679: 21, 254: 165, -9: 2, -15669: 161, -11: 4, -15667: 166, -15677: 135, 15677: 10, 15678: 1, -14: 2, -15664: 173, -15485: 15, -16: 2, -15660: 160, -18: 1, -15658: 165, -15675: 161, 251: 164, 15676: 1, -20: 4, -15655: 171, -15674: 147, 250: 155, -23: 3, -15652: 168, -15482: 2, -24: 3, -15650: 150, 249: 193, -15673: 169, 15674: 1, -27: 1, -15646: 137, 248: 223, -29: 5, -15643: 169, 15672: 3, -31: 2, -15641: 143, 247: 250, -15671: 147, -15480: 1, -33: 2, -15638: 250, -15670: 168, 246: 460, 15671: 1, 15670: 12, -36: 1, -15634: 160, 245: 588, -38: 1, -15632: 174, 15669: 22, 15104: 2, 565: 1, -15104: 1, 182: 2, -40: 2, -15629: 168, 244: 384, -15668: 142, 15668: 1, -43: 2, -15626: 207, -15476: 8, -44: 2, -15624: 189, 243: 348, -47: 3, -15620: 85, 242: 309, -15666: 129, -49: 2, -15617: 83, -15474: 29, -576: 1, -15090: 1, 15666: 27, -15665: 143, 241: 329, -15473: 7, -51: 3, 15665: 2, -53: 1, -15420: 4, 240: 337, -56: 2, -15416: 2, 15664: 30, -15663: 162, 239: 336, -57: 2, -15414: 12, -60: 2, -15411: 3, 238: 300, -15662: 138, 15662: 12, -62: 5, -15408: 1, -15470: 1, 237: 342, -15661: 140, -15406: 17, -15597: 155, -15405: 37, 236: 334, -67: 4, -15594: 149, 15661: 1, -69: 3, -15591: 129, 235: 320, -15659: 112, -71: 3, -15589: 239, -15468: 2, -15467: 37, 15659: 1, -73: 3, -15586: 192, 234: 345, -76: 2, -15582: 135, 233: 347, -15657: 153, -77: 3, -15580: 123, -15465: 8, -15656: 148, 232: 301, -80: 2, -15577: 11, 15657: 2, -82: 3, -15574: 36, 231: 287, 15656: 1, 15655: 5, -84: 1, -15571: 12, 230: 286, -15654: 163, -86: 2, -15568: 26, -15462: 1, 15040: 1, 614: 1, 15654: 27, -89: 1, -15565: 28, 229: 360, -15653: 146, -91: 1, -15563: 18, 228: 355, -93: 1, -15559: 19, -15651: 160, 227: 355, -96: 2, -15556: 24, -97: 1, -15554: 4, 15651: 15, 226: 356, -100: 1, -128: 22, 15650: 3, -15649: 167, -102: 1, 225: 374, -104: 1, -15417: 1, -15457: 2, 15649: 8, 14784: 58, 865: 23, -14784: 22, -14783: 1, 224: 418, -15648: 161, -106: 2, 223: 316, -15647: 146, -109: 1, 863: 12, -110: 2, -15409: 7, 222: 248, -113: 1, -640: 11, -15006: 9, 640: 2, -15454: 24, -15453: 3, 221: 193, -15645: 159, -115: 2, -15402: 1, 15645: 1, -117: 1, -15400: 2, -15644: 150, 220: 170, -120: 1, -15397: 19, 219: 186, -122: 2, -15393: 25, -124: 1, -15391: 2, -15642: 159, 218: 168, -126: 2, -15388: 3, -15450: 2, 217: 160, 15641: 1, -15385: 9, -129: 2, -15513: 3, 15642: 2, -130: 1, -15511: 3, 216: 193, -15640: 170, -133: 2, -15507: 2, -15639: 168, 215: 160, -135: 1, -15504: 3, 15639: 21, 14976: 8, 663: 5, -14976: 3, -137: 1, -15502: 2, 214: 275, -139: 1, -15499: 2, -15637: 224, -2: 3, -15636: 149, -4: 1, -15633: 150, 15637: 1, -15630: 147, 15636: 3, -8: 2, -15628: 160, -15635: 166, -13: 3, -15621: 89, -15: 1, -15619: 100, 15633: 31, -17: 2, -15631: 162, -21: 5, -15418: 1, -15415: 19, -26: 2, -15412: 2, -28: 1, -15403: 1, -15627: 131, -35: 2, -15401: 1, -704: 84, -14923: 14, 704: 32, 15627: 3, -37: 1, -15398: 5, -15394: 43, 201: 288, -15625: 280, -41: 2, -15392: 2, -18477: 2, -1242: 1, 19520: 1, 199: 15, 2852: 1, -18278: 2, 18480: 3, -18480: 3, 2854: 2, -18279: 1, 2855: 1, -15390: 1, 15625: 7, 14912: 2, 714: 1, 15626: 3, -45: 1, -15386: 2, -15384: 6, -19528: 1, 19728: 1, -754: 1, -14670: 1, 191: 7, -52: 1, -54: 2, -15378: 2, 15624: 45, -15623: 83, -59: 1, -15373: 4, 15623: 70, -15369: 2, 15622: 93, -15622: 81, -63: 4, -15367: 2, -15366: 17, -66: 3, 15621: 89, -68: 3, -15553: 1, 15620: 81, -70: 4, -15551: 4, 15619: 101, -72: 4, -15547: 3, -75: 1, -15544: 1, 15618: 89, -15618: 87, -15542: 4, 15617: 84, -79: 1, -15539: 3, 15616: 101, -15616: 100, -15535: 2, -83: 2, -15533: 4, 15615: 89, -15615: 86, -15530: 2, -88: 1, -15527: 5, 15614: 83, -15614: 81, -15422: 1, -90: 1, -15525: 5, -15613: 100, 15613: 101, -92: 1, -15521: 5, 15612: 91, -15612: 85, -95: 1, -15518: 4, 14848: 5, -14848: 3, 765: 1, -14847: 1, -15516: 4, -15611: 84, 15611: 81, -99: 1, -15419: 1, 15610: 95, -15610: 99, -101: 1, -15509: 4, -103: 1, -15609: 87, 15609: 70, 15360: 14395, -15360: 9238, -15359: 161, 15608: 21, -15608: 80, -108: 1, -15501: 1, -15361: 18, -15498: 1, -15607: 92, -112: 1, -15495: 1, 15607: 12, -15351: 3, -15606: 288, 15606: 3, -19546: 1, 19584: 2, -116: 1, -15490: 1, -119: 1, -15605: 436, -15356: 1, -15413: 5, -123: 1, -15354: 3, -15349: 2, -131: 1, -15604: 238, -136: 1, -19570: 1, -3: 2, -15602: 179, -6: 2, -15599: 168, -15596: 171, -15593: 170, -15603: 179, 15604: 2, 13: 1, -19586: 1, 19830: 1, -22249: 1, 22016: 2, 10: 3, -22026: 1, -15365: 1, 15602: 6, -15601: 175, -15579: 21, 15601: 1, 15296: 280, -15296: 225, 306: 5, -15410: 1, -15575: 13, -15600: 209, -30: 1, 15600: 17, -32: 1, 15599: 3, -15564: 13, -15407: 5, -39: 1, -15560: 37, -320: 35, -15279: 4, -15598: 170, -15558: 31, 15598: 20, -1087: 1, -14511: 1, -48: 3, -15357: 1, -15350: 1, -15595: 162, -58: 1, -15346: 2, 15596: 1, -15343: 3, -61: 2, -15342: 4, -15341: 3, -15340: 2, -15339: 3, -64: 2, -15338: 15, -65: 3, -15529: 5, 24: 2, -15528: 4, 22: 1, -15382: 7, 15594: 1, -15526: 2, -15524: 4, -15523: 4, -15522: 3, -15592: 153, -15520: 5, 15592: 1, -15519: 4, -74: 2, -15515: 3, 15591: 4, 15232: 6, -15232: 3, 360: 2, -15231: 1, -15512: 3, -15590: 174, 83: 2, -2202: 1, 2240: 123, -2246: 5, -2262: 2, -18014: 1, 4: 2, -25: 1, 18048: 110, -18026: 2, -19699: 1, 19712: 1, -18243: 1, -18068: 2, -18049: 10, -2229: 2, -2260: 4, -2221: 8, -18047: 3, -2251: 4, -2222: 1, -2227: 1, -2231: 1, -2240: 8, -2238: 3, -2263: 2, -2219: 4, -18038: 4, -18046: 7, -22: 2, -18027: 3, -3871: 1, 21888: 3, -21879: 1, -15372: 3, -18013: 5, -2268: 1, -18037: 2, -18052: 7, -2245: 8, -18055: 3, -2249: 5, -2259: 3, -18025: 5, -19418: 1, 19392: 3, 32: 1, -2235: 4, -18035: 6, -2255: 2, -19391: 2, -18029: 4, -18053: 2, -2230: 2, -18050: 10, -2241: 4, -2243: 6, -18033: 4, -15759: 1, -18063: 2, -21896: 1, -2244: 3, -2217: 2, -18036: 1, -2250: 2, -2220: 2, -2237: 4, -2264: 2, -18040: 1, 3: 2, 15794: 1, -15368: 1, -19490: 1, 19460: 1, -2218: 3, -2261: 1, 2: 1, -18028: 4, 15796: 1, -18012: 3, -2248: 2, -2242: 4, -2247: 2, -18054: 1, -18031: 2, -18070: 2, -2254: 1, -18034: 2, -2236: 2, 2651: 1, -18305: 2, 18304: 4, -18051: 1, -15383: 3, -2213: 1, -2257: 3, -15757: 1, 5: 3, -18044: 2, -2223: 3, -2252: 2, -2228: 1, -2253: 1, -2224: 1, -15387: 6, -18011: 2, -18066: 2, -18015: 2, -18061: 1, -15380: 5, -18306: 1, -18284: 1, -18042: 1, -18067: 1, -18032: 1, -18048: 1, -19648: 1, 19619: 1, -19433: 1, 646: 1, 18816: 5, -19462: 1, 19456: 1, -18787: 1, -15389: 2, -18783: 6, 3423: 3, -19701: 1, 19899: 1, 15590: 96, -6723: 1, 22084: 3, -22084: 3, 22113: 2, -18065: 1, -22095: 1, -22116: 1, 22092: 2, -22090: 2, 22087: 3, -22086: 5, -22087: 1, 22090: 2, -22091: 1, 22085: 4, -22085: 4, 22086: 3, 22100: 1, -22100: 1, 22144: 2, -22144: 1, -22097: 2, -10: 1, 22097: 1, 22094: 1, -22094: 1, -22092: 1, 22105: 1, 17: 1, -15377: 2, 145: 1, -15505: 3, 28: 1, 18259: 1, -194: 1, 36: 16, -15396: 39, 15589: 154, -229: 123, 15502: 2, 88: 2, 38: 5, 37: 8, 139: 1, 15359: 7, -18009: 1, 2649: 1, -15588: 199, 15588: 150, -228: 118, -18019: 1, -15587: 198, -18023: 1, -15395: 33, 15587: 154, 35: 14, -15379: 5, -163: 1, -15331: 13, 15586: 139, -226: 148, -18039: 1, 34: 15, -18043: 1, -15362: 1, 15585: 92, -15585: 258, 2689: 1, 18279: 1, 33: 3, 15394: 1, -34: 1, -18056: 1, 2696: 1, -15584: 307, -2699: 1, 2700: 1, 15584: 53, 18288: 1, 2706: 1, 2710: 1, 288: 2, -15583: 221, 15583: 73, -18073: 1, 18297: 1, -18076: 1, 2716: 1, -18080: 1, 2720: 1, -2724: 1, 18307: 1, -22168: 1, 22391: 1, -2289: 1, 2512: 1, 15582: 54, -22401: 2, -600: 1, 2827: 1, -980: 1, 980: 1, -982: 2, 20928: 4, -22173: 1, 1246: 1, -22174: 1, 6808: 1, -18085: 1, 2725: 1, -20924: 1, 21147: 1, -18087: 1, 18310: 1, -2729: 1, 2729: 1, -15581: 206, -21150: 1, -2731: 1, 2731: 1, -20170: 1, -22172: 1, 6814: 1, 15581: 170, -18093: 1, 18315: 1, -18095: 1, 2735: 1, -2737: 1, 18318: 1, 15580: 125, -2739: 1, 2739: 1, -2741: 1, 2741: 1, -2742: 1, 2742: 1, -2744: 1, 2744: 1, -4415: 1, 19995: 1, -2746: 1, 18326: 1, -15260: 5, -4429: 1, 19776: 7, -18328: 1, 18112: 3, -323: 22, 323: 22, -195: 1, 324: 33, -324: 30, -196: 2, -467: 32, 467: 31, 468: 32, -468: 26, -994: 14, 994: 30, -768: 45, -162: 1, -832: 1, 768: 3, 995: 31, -995: 4, -1930: 1, 2157: 31, -237: 19, -1920: 39, -2157: 11, -173: 1, -1984: 1, -1925: 1, 1920: 3, -2158: 11, 2158: 30, -238: 22, -18096: 1, 18334: 28, -222: 3, -18112: 3, -18334: 24, -20200: 1, 19968: 1, -21982: 1, 792: 1, 1416: 1, -19774: 1, 1672: 1, 2746: 1, -2784: 1, 15832: 1, 2304: 5, -19770: 1, 4429: 1, 13048: 2, -15352: 3, -4183: 1, 4183: 1, 12996: 1, 2368: 3, -2316: 2, -18530: 1, 16200: 1, -15777: 1, -18133: 1, -3060: 1, 21248: 2, -18620: 1, 18560: 14, -18560: 29, -2628: 1, -19830: 1, -1418: 1, 3060: 1, 15772: 1, -2763: 1, -2327: 1, -17450: 1, 8: 1, -18892: 1, 18880: 2, -19772: 1, 892: 1, -15344: 1, -19502: 1, 4142: 1, 13034: 1, -13034: 1, -4431: 1, -15345: 1, -19793: 1, 4433: 1, -19794: 2, -4435: 3, 19840: 2, 177: 2, -4436: 1, 20017: 1, -4425: 1, 20007: 1, -4196: 1, 19549: 1, -15353: 2, 15578: 90, -15578: 14, 15577: 70, 15576: 80, -15576: 27, 15574: 114, -15573: 27, 15572: 74, -15572: 14, -15570: 16, -15324: 20, 15569: 82, -15569: 14, -15323: 72, 15579: 76, 15568: 70, 15566: 115, -15566: 23, 15563: 104, 128: 1, 15562: 121, -15562: 10, 15559: 143, -15322: 78, 15558: 146, 15556: 153, 15555: 140, -15555: 15, 15554: 118, 15552: 1, -15552: 1, 15551: 3, 15550: 3, -15550: 3, -15321: 69, 15549: 3, -15549: 3, 187: 1, 186: 1, -15546: 2, 15347: 1, -15347: 1, 15545: 3, -15545: 2, 15543: 2, -15543: 3, 15542: 2, 15541: 2, 15539: 2, -15320: 73, 15538: 2, -15538: 1, 15537: 2, -15537: 3, 152: 2, 175: 2, -15534: 1, 173: 1, -15319: 71, 15532: 2, -15532: 2, 15575: 67, 15531: 2, -15531: 3, 15529: 2, 15528: 2, 15527: 2, 165: 1, 15524: 1, 15523: 3, 15521: 2, -15318: 104, 15520: 3, 15519: 2, 15518: 2, 15516: 2, 15515: 3, 15514: 2, -15514: 2, 15512: 3, 151: 1, 15573: 89, -15317: 95, 15510: 2, -15510: 2, 15508: 3, -15316: 82, 15507: 1, -15315: 74, 146: 1, -15314: 70, 15504: 2, -15312: 60, -15381: 1, 15571: 76, 15567: 87, -15375: 1, 277: 5, -15308: 73, -19200: 3, 19199: 2, 3627: 1, -15307: 96, -15306: 137, 129: 1, 15560: 126, -15302: 117, 15557: 137, -15301: 126, -15299: 134, -15298: 95, 15553: 14, -15297: 11, 276: 8, -15295: 3, 15547: 2, -15355: 1, 15546: 2, 15570: 63, 183: 2, -3648: 2, 19218: 1, 3648: 1, 181: 1, -15541: 2, 15540: 2, -15540: 2, 12928: 2, -12928: 2, 15536: 1, -15536: 1, 15535: 2, 15533: 2, -15313: 84, 171: 1, -1344: 7, 1344: 7, 15526: 2, -15334: 2, 164: 1, 15522: 1, -15311: 84, -15567: 16, 15509: 2, -15508: 2, 15506: 1, -15506: 1, 15505: 2, -15310: 113, 142: 1, -15309: 101, 271: 4, 15500: 2, 138: 1, 15497: 2, -15305: 108, 15565: 105, 15496: 2, -15304: 130, -19264: 4, 19264: 3, 19265: 1, 15495: 1, -15303: 146, 15493: 1, 132: 1, -15300: 142, -15374: 2, 15564: 66, 15561: 143, -15561: 43, 269: 7, -15371: 1, -15370: 1, -14912: 1, 169: 1, -15337: 1, -20117: 1, 1856: 2, -2836: 1, 21120: 3, -15328: 1, 15525: 1, -5796: 1, -2470: 1, 2496: 1, -20122: 1, -2838: 1, -22365: 1, 22400: 9, -22399: 2, 6: 1, -22406: 1, -22352: 2, -22402: 1, -22404: 1, 15517: 1, 43: 1, 15513: 1, 264: 11, -384: 1, 384: 1, 15511: 1, 49: 1, -19968: 1, 4408: 1, 390: 2, 391: 15, -15167: 1, 15503: 1, -15503: 1, 57: 1, -4416: 1, 4416: 1, -15500: 1, -15497: 1, 62: 1, -2368: 5, -13190: 3, -15557: 39, -13189: 1, 15494: 1, -15494: 1, -197: 1, 15491: 1, -4672: 10, 20229: 2, 4672: 8, 15490: 1, 13184: 59, -13184: 43, 2373: 15, -13183: 1, 261: 8, 15485: 1, 71: 1, 15484: 1, -1216: 5, 1216: 5, 15482: 1, 14336: 41, -14336: 24, 1143: 1, 76: 1, 1219: 13, 1220: 4, 15300: 2, -964: 2, -1220: 14, 964: 14, -14335: 1, 15478: 1, 78: 1, 3005: 14, 18561: 1, 15475: 1, 15473: 1, -3008: 35, 3008: 23, 18562: 3, 18563: 9, 15472: 1, 15469: 1, 703: 2, -14851: 3, -14850: 31, 15467: 1, 87: 1, -14849: 2, 15466: 1}\n","Train各delta值： [0, 15703, -15703, -256, -15447, 256, -15446, 15702, 255, -199, -15247, 1, -15445, 15701, -201, -15245, 15488, -15488, -203, -15242, 213, 15700, -15444, -15701, -206, -15239, 212, -207, -15237, -15443, 15699, -15700, -210, -15233, 15698, -15442, 210, 211, -212, -15486, -15441, 15697, -15698, -470, -15484, -15697, 209, -216, -15481, -15440, 15696, -219, -15477, -15439, 15695, -15696, -220, -15475, 208, 207, -223, -15472, 15694, -15438, 206, -15695, -225, -15469, 15693, -15437, -15693, -1, -227, -15466, 205, 15692, -15436, -230, -15463, -232, -15460, 203, -15435, 204, 15691, -234, -15458, 257, -15692, -15434, -236, -15455, 15690, -15690, -239, -15451, 202, 15689, -15433, -240, -15449, -15432, 15688, -243, -15190, 200, -15689, -15688, -245, -15187, -15431, 15687, -15687, -247, -15184, 15686, -15430, -249, 198, -252, -15178, 15685, -15429, -254, 197, 196, -15428, 15684, -15685, -512, 512, -15173, -15684, -192, -15492, -15427, 15683, -259, -15169, 195, 15168, -15168, -260, 320, -15487, -15426, 15682, -15683, -263, 194, -15425, -265, 15681, -267, -15478, 15680, -15424, -15681, 15424, 258, 193, 253, -15678, 192, 252, -15676, -15680, -15679, -15423, -7, -15672, 15679, 254, -9, -15669, -11, -15667, -15677, 15677, 15678, -14, -15664, -15485, -16, -15660, -18, -15658, -15675, 251, 15676, -20, -15655, -15674, 250, -23, -15652, -15482, -24, -15650, 249, -15673, 15674, -27, -15646, 248, -29, -15643, 15672, -31, -15641, 247, -15671, -15480, -33, -15638, -15670, 246, 15671, 15670, -36, -15634, 245, -38, -15632, 15669, 15104, 565, -15104, 182, -40, -15629, 244, -15668, 15668, -43, -15626, -15476, -44, -15624, 243, -47, -15620, 242, -15666, -49, -15617, -15474, -576, -15090, 15666, -15665, 241, -15473, -51, 15665, -53, -15420, 240, -56, -15416, 15664, -15663, 239, -57, -15414, -60, -15411, 238, -15662, 15662, -62, -15408, -15470, 237, -15661, -15406, -15597, -15405, 236, -67, -15594, 15661, -69, -15591, 235, -15659, -71, -15589, -15468, -15467, 15659, -73, -15586, 234, -76, -15582, 233, -15657, -77, -15580, -15465, -15656, 232, -80, -15577, 15657, -82, -15574, 231, 15656, 15655, -84, -15571, 230, -15654, -86, -15568, -15462, 15040, 614, 15654, -89, -15565, 229, -15653, -91, -15563, 228, -93, -15559, -15651, 227, -96, -15556, -97, -15554, 15651, 226, -100, -128, 15650, -15649, -102, 225, -104, -15417, -15457, 15649, 14784, 865, -14784, -14783, 224, -15648, -106, 223, -15647, -109, 863, -110, -15409, 222, -113, -640, -15006, 640, -15454, -15453, 221, -15645, -115, -15402, 15645, -117, -15400, -15644, 220, -120, -15397, 219, -122, -15393, -124, -15391, -15642, 218, -126, -15388, -15450, 217, 15641, -15385, -129, -15513, 15642, -130, -15511, 216, -15640, -133, -15507, -15639, 215, -135, -15504, 15639, 14976, 663, -14976, -137, -15502, 214, -139, -15499, -15637, -2, -15636, -4, -15633, 15637, -15630, 15636, -8, -15628, -15635, -13, -15621, -15, -15619, 15633, -17, -15631, -21, -15418, -15415, -26, -15412, -28, -15403, -15627, -35, -15401, -704, -14923, 704, 15627, -37, -15398, -15394, 201, -15625, -41, -15392, -18477, -1242, 19520, 199, 2852, -18278, 18480, -18480, 2854, -18279, 2855, -15390, 15625, 14912, 714, 15626, -45, -15386, -15384, -19528, 19728, -754, -14670, 191, -52, -54, -15378, 15624, -15623, -59, -15373, 15623, -15369, 15622, -15622, -63, -15367, -15366, -66, 15621, -68, -15553, 15620, -70, -15551, 15619, -72, -15547, -75, -15544, 15618, -15618, -15542, 15617, -79, -15539, 15616, -15616, -15535, -83, -15533, 15615, -15615, -15530, -88, -15527, 15614, -15614, -15422, -90, -15525, -15613, 15613, -92, -15521, 15612, -15612, -95, -15518, 14848, -14848, 765, -14847, -15516, -15611, 15611, -99, -15419, 15610, -15610, -101, -15509, -103, -15609, 15609, 15360, -15360, -15359, 15608, -15608, -108, -15501, -15361, -15498, -15607, -112, -15495, 15607, -15351, -15606, 15606, -19546, 19584, -116, -15490, -119, -15605, -15356, -15413, -123, -15354, -15349, -131, -15604, -136, -19570, -3, -15602, -6, -15599, -15596, -15593, -15603, 15604, 13, -19586, 19830, -22249, 22016, 10, -22026, -15365, 15602, -15601, -15579, 15601, 15296, -15296, 306, -15410, -15575, -15600, -30, 15600, -32, 15599, -15564, -15407, -39, -15560, -320, -15279, -15598, -15558, 15598, -1087, -14511, -48, -15357, -15350, -15595, -58, -15346, 15596, -15343, -61, -15342, -15341, -15340, -15339, -64, -15338, -65, -15529, 24, -15528, 22, -15382, 15594, -15526, -15524, -15523, -15522, -15592, -15520, 15592, -15519, -74, -15515, 15591, 15232, -15232, 360, -15231, -15512, -15590, 83, -2202, 2240, -2246, -2262, -18014, 4, -25, 18048, -18026, -19699, 19712, -18243, -18068, -18049, -2229, -2260, -2221, -18047, -2251, -2222, -2227, -2231, -2240, -2238, -2263, -2219, -18038, -18046, -22, -18027, -3871, 21888, -21879, -15372, -18013, -2268, -18037, -18052, -2245, -18055, -2249, -2259, -18025, -19418, 19392, 32, -2235, -18035, -2255, -19391, -18029, -18053, -2230, -18050, -2241, -2243, -18033, -15759, -18063, -21896, -2244, -2217, -18036, -2250, -2220, -2237, -2264, -18040, 3, 15794, -15368, -19490, 19460, -2218, -2261, 2, -18028, 15796, -18012, -2248, -2242, -2247, -18054, -18031, -18070, -2254, -18034, -2236, 2651, -18305, 18304, -18051, -15383, -2213, -2257, -15757, 5, -18044, -2223, -2252, -2228, -2253, -2224, -15387, -18011, -18066, -18015, -18061, -15380, -18306, -18284, -18042, -18067, -18032, -18048, -19648, 19619, -19433, 646, 18816, -19462, 19456, -18787, -15389, -18783, 3423, -19701, 19899, 15590, -6723, 22084, -22084, 22113, -18065, -22095, -22116, 22092, -22090, 22087, -22086, -22087, 22090, -22091, 22085, -22085, 22086, 22100, -22100, 22144, -22144, -22097, -10, 22097, 22094, -22094, -22092, 22105, 17, -15377, 145, -15505, 28, 18259, -194, 36, -15396, 15589, -229, 15502, 88, 38, 37, 139, 15359, -18009, 2649, -15588, 15588, -228, -18019, -15587, -18023, -15395, 15587, 35, -15379, -163, -15331, 15586, -226, -18039, 34, -18043, -15362, 15585, -15585, 2689, 18279, 33, 15394, -34, -18056, 2696, -15584, -2699, 2700, 15584, 18288, 2706, 2710, 288, -15583, 15583, -18073, 18297, -18076, 2716, -18080, 2720, -2724, 18307, -22168, 22391, -2289, 2512, 15582, -22401, -600, 2827, -980, 980, -982, 20928, -22173, 1246, -22174, 6808, -18085, 2725, -20924, 21147, -18087, 18310, -2729, 2729, -15581, -21150, -2731, 2731, -20170, -22172, 6814, 15581, -18093, 18315, -18095, 2735, -2737, 18318, 15580, -2739, 2739, -2741, 2741, -2742, 2742, -2744, 2744, -4415, 19995, -2746, 18326, -15260, -4429, 19776, -18328, 18112, -323, 323, -195, 324, -324, -196, -467, 467, 468, -468, -994, 994, -768, -162, -832, 768, 995, -995, -1930, 2157, -237, -1920, -2157, -173, -1984, -1925, 1920, -2158, 2158, -238, -18096, 18334, -222, -18112, -18334, -20200, 19968, -21982, 792, 1416, -19774, 1672, 2746, -2784, 15832, 2304, -19770, 4429, 13048, -15352, -4183, 4183, 12996, 2368, -2316, -18530, 16200, -15777, -18133, -3060, 21248, -18620, 18560, -18560, -2628, -19830, -1418, 3060, 15772, -2763, -2327, -17450, 8, -18892, 18880, -19772, 892, -15344, -19502, 4142, 13034, -13034, -4431, -15345, -19793, 4433, -19794, -4435, 19840, 177, -4436, 20017, -4425, 20007, -4196, 19549, -15353, 15578, -15578, 15577, 15576, -15576, 15574, -15573, 15572, -15572, -15570, -15324, 15569, -15569, -15323, 15579, 15568, 15566, -15566, 15563, 128, 15562, -15562, 15559, -15322, 15558, 15556, 15555, -15555, 15554, 15552, -15552, 15551, 15550, -15550, -15321, 15549, -15549, 187, 186, -15546, 15347, -15347, 15545, -15545, 15543, -15543, 15542, 15541, 15539, -15320, 15538, -15538, 15537, -15537, 152, 175, -15534, 173, -15319, 15532, -15532, 15575, 15531, -15531, 15529, 15528, 15527, 165, 15524, 15523, 15521, -15318, 15520, 15519, 15518, 15516, 15515, 15514, -15514, 15512, 151, 15573, -15317, 15510, -15510, 15508, -15316, 15507, -15315, 146, -15314, 15504, -15312, -15381, 15571, 15567, -15375, 277, -15308, -19200, 19199, 3627, -15307, -15306, 129, 15560, -15302, 15557, -15301, -15299, -15298, 15553, -15297, 276, -15295, 15547, -15355, 15546, 15570, 183, -3648, 19218, 3648, 181, -15541, 15540, -15540, 12928, -12928, 15536, -15536, 15535, 15533, -15313, 171, -1344, 1344, 15526, -15334, 164, 15522, -15311, -15567, 15509, -15508, 15506, -15506, 15505, -15310, 142, -15309, 271, 15500, 138, 15497, -15305, 15565, 15496, -15304, -19264, 19264, 19265, 15495, -15303, 15493, 132, -15300, -15374, 15564, 15561, -15561, 269, -15371, -15370, -14912, 169, -15337, -20117, 1856, -2836, 21120, -15328, 15525, -5796, -2470, 2496, -20122, -2838, -22365, 22400, -22399, 6, -22406, -22352, -22402, -22404, 15517, 43, 15513, 264, -384, 384, 15511, 49, -19968, 4408, 390, 391, -15167, 15503, -15503, 57, -4416, 4416, -15500, -15497, 62, -2368, -13190, -15557, -13189, 15494, -15494, -197, 15491, -4672, 20229, 4672, 15490, 13184, -13184, 2373, -13183, 261, 15485, 71, 15484, -1216, 1216, 15482, 14336, -14336, 1143, 76, 1219, 1220, 15300, -964, -1220, 964, -14335, 15478, 78, 3005, 18561, 15475, 15473, -3008, 3008, 18562, 18563, 15472, 15469, 703, -14851, -14850, 15467, 87, -14849, 15466]\n","Train各delta值的次数： [33074, 32, 1, 16200, 28, 9467, 161, 163, 354, 1, 1, 519, 162, 163, 1, 1, 198, 82, 1, 1, 263, 146, 147, 1, 1, 1, 162, 1, 1, 166, 149, 3, 2, 1, 162, 135, 193, 196, 6, 7, 133, 148, 24, 1, 4, 16, 135, 1, 3, 154, 171, 1, 4, 161, 160, 19, 1, 4, 185, 173, 40, 3, 132, 148, 193, 1, 61, 9, 156, 166, 2, 318, 159, 7, 200, 155, 162, 68, 1, 1, 1, 166, 153, 194, 150, 1, 2, 338, 1, 169, 1, 3, 167, 1, 1, 1, 227, 167, 161, 1, 3, 148, 145, 1, 1, 173, 1, 1, 1, 1, 162, 170, 8, 1, 1, 161, 164, 1, 16, 1, 1, 146, 127, 1, 10, 23, 148, 147, 18, 3, 1, 1, 16, 956, 5, 159, 154, 1, 1, 14, 64, 46, 1, 30, 39, 170, 168, 1, 1, 2, 277, 1, 243, 1, 2, 112, 6646, 7, 15783, 5, 13, 139, 157, 451, 167, 168, 71, 131, 105, 3, 161, 21, 165, 2, 161, 4, 166, 135, 10, 1, 2, 173, 15, 2, 160, 1, 165, 161, 164, 1, 4, 171, 147, 155, 3, 168, 2, 3, 150, 193, 169, 1, 1, 137, 223, 5, 169, 3, 2, 143, 250, 147, 1, 2, 250, 168, 460, 1, 12, 1, 160, 588, 1, 174, 22, 2, 1, 1, 2, 2, 168, 384, 142, 1, 2, 207, 8, 2, 189, 348, 3, 85, 309, 129, 2, 83, 29, 1, 1, 27, 143, 329, 7, 3, 2, 1, 4, 337, 2, 2, 30, 162, 336, 2, 12, 2, 3, 300, 138, 12, 5, 1, 1, 342, 140, 17, 155, 37, 334, 4, 149, 1, 3, 129, 320, 112, 3, 239, 2, 37, 1, 3, 192, 345, 2, 135, 347, 153, 3, 123, 8, 148, 301, 2, 11, 2, 3, 36, 287, 1, 5, 1, 12, 286, 163, 2, 26, 1, 1, 1, 27, 1, 28, 360, 146, 1, 18, 355, 1, 19, 160, 355, 2, 24, 1, 4, 15, 356, 1, 22, 3, 167, 1, 374, 1, 1, 2, 8, 58, 23, 22, 1, 418, 161, 2, 316, 146, 1, 12, 2, 7, 248, 1, 11, 9, 2, 24, 3, 193, 159, 2, 1, 1, 1, 2, 150, 170, 1, 19, 186, 2, 25, 1, 2, 159, 168, 2, 3, 2, 160, 1, 9, 2, 3, 2, 1, 3, 193, 170, 2, 2, 168, 160, 1, 3, 21, 8, 5, 3, 1, 2, 275, 1, 2, 224, 3, 149, 1, 150, 1, 147, 3, 2, 160, 166, 3, 89, 1, 100, 31, 2, 162, 5, 1, 19, 2, 2, 1, 1, 131, 2, 1, 84, 14, 32, 3, 1, 5, 43, 288, 280, 2, 2, 2, 1, 1, 15, 1, 2, 3, 3, 2, 1, 1, 1, 7, 2, 1, 3, 1, 2, 6, 1, 1, 1, 1, 7, 1, 2, 2, 45, 83, 1, 4, 70, 2, 93, 81, 4, 2, 17, 3, 89, 3, 1, 81, 4, 4, 101, 4, 3, 1, 1, 89, 87, 4, 84, 1, 3, 101, 100, 2, 2, 4, 89, 86, 2, 1, 5, 83, 81, 1, 1, 5, 100, 101, 1, 5, 91, 85, 1, 4, 5, 3, 1, 1, 4, 84, 81, 1, 1, 95, 99, 1, 4, 1, 87, 70, 14395, 9238, 161, 21, 80, 1, 1, 18, 1, 92, 1, 1, 12, 3, 288, 3, 1, 2, 1, 1, 1, 436, 1, 5, 1, 3, 2, 1, 238, 1, 1, 2, 179, 2, 168, 171, 170, 179, 2, 1, 1, 1, 1, 2, 3, 1, 1, 6, 175, 21, 1, 280, 225, 5, 1, 13, 209, 1, 17, 1, 3, 13, 5, 1, 37, 35, 4, 170, 31, 20, 1, 1, 3, 1, 1, 162, 1, 2, 1, 3, 2, 4, 3, 2, 3, 2, 15, 3, 5, 2, 4, 1, 7, 1, 2, 4, 4, 3, 153, 5, 1, 4, 2, 3, 4, 6, 3, 2, 1, 3, 174, 2, 1, 123, 5, 2, 1, 2, 1, 110, 2, 1, 1, 1, 2, 10, 2, 4, 8, 3, 4, 1, 1, 1, 8, 3, 2, 4, 4, 7, 2, 3, 1, 3, 1, 3, 5, 1, 2, 7, 8, 3, 5, 3, 5, 1, 3, 1, 4, 6, 2, 2, 4, 2, 2, 10, 4, 6, 4, 1, 2, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 4, 1, 3, 2, 4, 2, 1, 2, 2, 1, 2, 2, 1, 2, 4, 1, 3, 1, 3, 1, 3, 2, 3, 2, 1, 1, 1, 6, 2, 2, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 2, 6, 3, 1, 1, 96, 1, 3, 3, 2, 1, 1, 1, 2, 2, 3, 5, 1, 2, 1, 4, 4, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 16, 39, 154, 123, 2, 2, 5, 8, 1, 7, 1, 1, 199, 150, 118, 1, 198, 1, 33, 154, 14, 5, 1, 13, 139, 148, 1, 15, 1, 1, 92, 258, 1, 1, 3, 1, 1, 1, 1, 307, 1, 1, 53, 1, 1, 1, 2, 221, 73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54, 2, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 206, 1, 1, 1, 1, 1, 1, 170, 1, 1, 1, 1, 1, 1, 125, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 7, 1, 3, 22, 22, 1, 33, 30, 2, 32, 31, 32, 26, 14, 30, 45, 1, 1, 3, 31, 4, 1, 31, 19, 39, 11, 1, 1, 1, 3, 11, 30, 22, 1, 28, 3, 3, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 14, 29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 1, 1, 1, 1, 1, 1, 2, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 1, 121, 10, 143, 78, 146, 153, 140, 15, 118, 1, 1, 3, 3, 3, 69, 3, 3, 1, 1, 2, 1, 1, 3, 2, 2, 3, 2, 2, 2, 73, 2, 1, 2, 3, 2, 2, 1, 1, 71, 2, 2, 67, 2, 3, 2, 2, 2, 1, 1, 3, 2, 104, 3, 2, 2, 2, 3, 2, 2, 3, 1, 89, 95, 2, 2, 3, 82, 1, 74, 1, 70, 2, 60, 1, 76, 87, 1, 5, 73, 3, 2, 1, 96, 137, 1, 126, 117, 137, 126, 134, 95, 14, 11, 8, 3, 2, 1, 2, 63, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 84, 1, 7, 7, 2, 2, 1, 1, 84, 16, 2, 2, 1, 1, 2, 113, 1, 101, 4, 2, 1, 2, 108, 105, 2, 130, 4, 3, 1, 1, 146, 1, 1, 142, 2, 66, 143, 43, 7, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 9, 2, 1, 1, 2, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 2, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 3, 39, 1, 1, 1, 1, 1, 10, 2, 8, 1, 59, 43, 15, 1, 8, 1, 1, 1, 5, 5, 1, 41, 24, 1, 1, 13, 4, 2, 2, 14, 14, 1, 1, 1, 14, 1, 1, 1, 35, 23, 3, 9, 1, 1, 2, 3, 31, 1, 1, 2, 1]\n","Train_delta种类： 1304\n","Test字典收集训练集（delta类别-次数）： {15554: 72, -256: 1669, 256: 1329, -15297: 230, 0: 9111, -15298: 76, 15360: 80, -15360: 65, -15554: 5, 15555: 1, 255: 122, 1: 177, 15463: 1, -15463: 1, 15553: 202, -15553: 18, 257: 37, 194: 6, 193: 18, -192: 517, -15362: 1, 15461: 1, 92: 1, -15361: 1, 15460: 1, -15460: 1, -1: 150, -1472: 1, -14081: 1, 15457: 1, -15457: 1, 15552: 44, -15296: 8332, 254: 153, 15296: 8974, 258: 8, -15295: 135, 15550: 7, 3: 6, -576: 64, 576: 40, 575: 1, -14978: 1, -14977: 21, 577: 1, 15548: 19, 5: 4, 12765: 1, 2560: 1, 228: 2, -7115: 1, 22464: 2, -48: 1, 252: 112, 15547: 4, -15547: 91, -15552: 65, 15546: 37, -15546: 104, -4704: 1, 20032: 1, 224: 3, 15350: 1, 203: 2, 15545: 396, -15545: 171, 15544: 127, -15544: 112, 15543: 483, -15543: 174, -7116: 1, 205: 2, 15542: 175, -15542: 126, 15541: 69, 12: 1, 244: 118, 13: 2, 178: 3, 14: 2, 15538: 110, 15: 1, 15536: 3, 16: 1, 15535: 1, 17: 2, 22: 2, 234: 3, 15534: 1, 18: 1, 15533: 1, 19: 1, 192: 346, -15551: 95, 236: 2, 20: 2, 235: 2, 21: 2, -19780: 1, 19840: 7, 196: 1, 233: 1, -15529: 1, 232: 2, -15528: 2, 231: 2, -15527: 1, 230: 2, -15526: 2, 229: 2, -15525: 2, 15551: 3, -15524: 1, -15359: 4, 227: 3, -15522: 3, 226: 3, -19815: 1, -15321: 2, -15520: 1, 223: 2, -15519: 2, 222: 2, -15518: 1, -15550: 155, 221: 2, -15517: 2, 4: 4, -15300: 1, 46: 1, -15342: 4, 220: 3, -15516: 1, 219: 2, -15515: 2, 218: 2, -15514: 2, 217: 2, -15513: 1, 216: 4, -15512: 2, 215: 1, -15511: 1, 213: 4, -15509: 2, 211: 1, -15507: 1, 210: 3, -15506: 1, 209: 3, -15505: 2, 208: 1, -15504: 1, -15358: 3, 207: 3, -15503: 2, 206: 1, -15502: 1, -15501: 1, 204: 1, -15500: 1, -15549: 177, -15499: 1, 201: 1, -15497: 1, 253: 180, 200: 5, -15496: 2, 199: 2, -15495: 1, -15357: 12, 198: 1, -15494: 1, 197: 2, -15493: 2, 15492: 1, -15492: 1, 195: 1, -15491: 1, -15490: 1, -15489: 1, 191: 7, 15488: 1, 190: 2, -15294: 3, 189: 1, -15292: 12, 187: 1, -15483: 1, -15548: 118, -2: 4, 2: 7, -2597: 1, 2624: 1, -21485: 1, 21248: 2, 30: 2, -15343: 2, -20250: 1, 20288: 1, 251: 88, 248: 99, -19834: 1, -20075: 1, 20096: 1, 15549: 20, -253: 14, -21236: 1, -22493: 1, 22528: 20, 15232: 11, -15232: 8, -15231: 2, 246: 102, 7: 2, -252: 9, 9: 1, 242: 76, -15537: 94, 239: 1, -15293: 2, 237: 1, -4: 1, 15292: 1, -15532: 1, -15530: 1, -15355: 1, 24: 1, 225: 1, -15329: 1, 27: 2, 250: 91, 34: 1, 214: 1, -15510: 1, 37: 1, 249: 144, 212: 1, -15508: 1, -250: 33, -249: 332, -20292: 1, -982: 1, 21312: 2, -22556: 1, -22542: 1, -22532: 1, -22531: 1, 8: 3, -22536: 3, -6: 3, -22522: 2, 40: 1, -15353: 45, -21527: 1, -15330: 1, -2650: 2, 2688: 122, -2709: 5, -2687: 4, -2686: 8, -18466: 2, 18496: 131, -18493: 4, -18520: 7, -18474: 4, -18516: 4, -18478: 2, -18517: 3, -18496: 7, -20149: 1, 20160: 6, -2656: 2, -2690: 11, -18505: 1, -18497: 10, -2691: 3, -2669: 5, -2688: 7, -18484: 2, -2698: 9, -19863: 1, -19844: 1, -18463: 2, -18508: 1, -18482: 2, -15338: 3, -2703: 3, -18473: 2, -2699: 3, -2668: 1, -2685: 3, -2712: 4, -2664: 2, -2689: 2, -2711: 1, -2666: 4, -18487: 4, -18472: 4, -18518: 2, -18475: 4, -2722: 1, -15313: 1, -2648: 1, -21: 2, -2667: 2, -18495: 4, -18476: 4, -18498: 9, -2696: 1, -18488: 3, 15785: 1, -2675: 2, -15796: 1, -15328: 1, -2658: 1, -18486: 8, -18509: 5, -18481: 3, -2701: 2, -2673: 4, -2707: 4, -19929: 1, 17228: 1, -2706: 1, -2660: 3, -10: 2, -2681: 1, -18: 2, -12: 1, -15336: 4, -18456: 2, -18454: 2, -18494: 1, -19873: 1, -19845: 1, 50: 5, -18511: 1, -18483: 1, -30: 2, -15306: 1, -18455: 1, -20: 1, -18489: 1, -18502: 2, -18477: 1, -15323: 1, -22356: 1, 22336: 5, 176: 1, -23385: 1, 23232: 1, -36: 1, -15341: 1, -20110: 1, -2708: 1, -15778: 1, -20138: 1, -18725: 1, -2697: 1, -22328: 1, -22005: 1, 22016: 4, -21986: 1, -22226: 1, 22208: 1, -21998: 2, -18500: 1, -2679: 1, -22366: 1, -17: 1, -22319: 1, -22336: 1, -15324: 2, -19876: 1, 19904: 1, -20139: 1, -20111: 1, -2651: 1, -2713: 2, -2662: 1, -18462: 1, -18499: 1, -18523: 2, -18491: 1, 18718: 1, -18705: 1, -2695: 1, -18460: 1, -18506: 1, -18749: 1, 18752: 1, -15332: 1, -15318: 1, -18450: 1, -18507: 1, -2700: 2, -15319: 2, -18485: 2, -2678: 1, -2682: 2, -2704: 1, -15810: 1, -2663: 1, -2665: 1, -18492: 1, -18521: 1, -1338: 2, 1344: 3, -15302: 4, -1587: 1, -4827: 1, -22521: 2, -7: 1, -22527: 1, 49: 4, -22584: 1, 22530: 1, -22530: 1, 23: 2, -22551: 2, 35: 1, -22563: 1, -22528: 1, -22529: 1, 55: 50, -22583: 1, 2901: 1, -2898: 1, -23: 1, -15344: 1, -18479: 1, -15333: 2, 42: 1, -18459: 1, 12624: 1, 3153: 1, 186: 1, -185: 4, 57: 20, -248: 104, -2915: 1, -15309: 1, -15352: 24, 15295: 5, 56: 10, -18736: 1, -2717: 1, 2752: 9, -2733: 1, -12606: 3, 13946: 4, -13946: 5, -1350: 1, -1351: 4, -13945: 6, -1599: 1, -1600: 1, 13944: 1, -13944: 1, -1352: 1, 1600: 1, -184: 1, 15352: 1, -56: 1, -1408: 1, 1408: 4, -15351: 81, -2661: 1, -18740: 1, 15796: 1, 3176: 1, -2676: 1, -15308: 2, -2694: 2, 168: 1, 247: 122, 183: 1, 184: 1, -247: 386, -18734: 1, -15304: 1, -2754: 1, -12604: 1, -15287: 5, -246: 133, -2923: 1, -2684: 1, 3206: 1, -3185: 1, -15474: 1, -182: 5, -15350: 19, 311: 1, 15231: 1, 65: 1, -183: 4, -9: 1, -15541: 69, 175: 1, -15471: 1, 15351: 2, -55: 1, 15467: 1, -15467: 1, 66: 1, 180: 1, 64: 1, -64: 1, 182: 1, -1442: 1, -1378: 2, -15326: 2, 18560: 6, 15808: 1, -18549: 1, 155: 1, -15459: 1, 245: 68, -245: 51, -15349: 31, 53: 18, 54: 3, 15540: 143, -244: 135, -15540: 110, -2952: 1, -2705: 1, -2753: 1, -18541: 1, 3208: 1, -18513: 1, -18560: 2, 15294: 2, -15348: 24, 15349: 2, -53: 2, -15285: 7, 52: 7, -18775: 1, -2732: 1, -15346: 10, 18672: 1, -15448: 1, 15539: 101, -243: 91, -15539: 101, 243: 104, -15347: 9, 51: 1, -15538: 113, -242: 67, -178: 1, -14: 1, -179: 1, 15431: 1, 107: 1, 241: 82, 15537: 87, -241: 71, -2734: 1, -15314: 1, -15345: 11, -15536: 3, -20201: 1, 4905: 1}\n","Test各delta值： [15554, -256, 256, -15297, 0, -15298, 15360, -15360, -15554, 15555, 255, 1, 15463, -15463, 15553, -15553, 257, 194, 193, -192, -15362, 15461, 92, -15361, 15460, -15460, -1, -1472, -14081, 15457, -15457, 15552, -15296, 254, 15296, 258, -15295, 15550, 3, -576, 576, 575, -14978, -14977, 577, 15548, 5, 12765, 2560, 228, -7115, 22464, -48, 252, 15547, -15547, -15552, 15546, -15546, -4704, 20032, 224, 15350, 203, 15545, -15545, 15544, -15544, 15543, -15543, -7116, 205, 15542, -15542, 15541, 12, 244, 13, 178, 14, 15538, 15, 15536, 16, 15535, 17, 22, 234, 15534, 18, 15533, 19, 192, -15551, 236, 20, 235, 21, -19780, 19840, 196, 233, -15529, 232, -15528, 231, -15527, 230, -15526, 229, -15525, 15551, -15524, -15359, 227, -15522, 226, -19815, -15321, -15520, 223, -15519, 222, -15518, -15550, 221, -15517, 4, -15300, 46, -15342, 220, -15516, 219, -15515, 218, -15514, 217, -15513, 216, -15512, 215, -15511, 213, -15509, 211, -15507, 210, -15506, 209, -15505, 208, -15504, -15358, 207, -15503, 206, -15502, -15501, 204, -15500, -15549, -15499, 201, -15497, 253, 200, -15496, 199, -15495, -15357, 198, -15494, 197, -15493, 15492, -15492, 195, -15491, -15490, -15489, 191, 15488, 190, -15294, 189, -15292, 187, -15483, -15548, -2, 2, -2597, 2624, -21485, 21248, 30, -15343, -20250, 20288, 251, 248, -19834, -20075, 20096, 15549, -253, -21236, -22493, 22528, 15232, -15232, -15231, 246, 7, -252, 9, 242, -15537, 239, -15293, 237, -4, 15292, -15532, -15530, -15355, 24, 225, -15329, 27, 250, 34, 214, -15510, 37, 249, 212, -15508, -250, -249, -20292, -982, 21312, -22556, -22542, -22532, -22531, 8, -22536, -6, -22522, 40, -15353, -21527, -15330, -2650, 2688, -2709, -2687, -2686, -18466, 18496, -18493, -18520, -18474, -18516, -18478, -18517, -18496, -20149, 20160, -2656, -2690, -18505, -18497, -2691, -2669, -2688, -18484, -2698, -19863, -19844, -18463, -18508, -18482, -15338, -2703, -18473, -2699, -2668, -2685, -2712, -2664, -2689, -2711, -2666, -18487, -18472, -18518, -18475, -2722, -15313, -2648, -21, -2667, -18495, -18476, -18498, -2696, -18488, 15785, -2675, -15796, -15328, -2658, -18486, -18509, -18481, -2701, -2673, -2707, -19929, 17228, -2706, -2660, -10, -2681, -18, -12, -15336, -18456, -18454, -18494, -19873, -19845, 50, -18511, -18483, -30, -15306, -18455, -20, -18489, -18502, -18477, -15323, -22356, 22336, 176, -23385, 23232, -36, -15341, -20110, -2708, -15778, -20138, -18725, -2697, -22328, -22005, 22016, -21986, -22226, 22208, -21998, -18500, -2679, -22366, -17, -22319, -22336, -15324, -19876, 19904, -20139, -20111, -2651, -2713, -2662, -18462, -18499, -18523, -18491, 18718, -18705, -2695, -18460, -18506, -18749, 18752, -15332, -15318, -18450, -18507, -2700, -15319, -18485, -2678, -2682, -2704, -15810, -2663, -2665, -18492, -18521, -1338, 1344, -15302, -1587, -4827, -22521, -7, -22527, 49, -22584, 22530, -22530, 23, -22551, 35, -22563, -22528, -22529, 55, -22583, 2901, -2898, -23, -15344, -18479, -15333, 42, -18459, 12624, 3153, 186, -185, 57, -248, -2915, -15309, -15352, 15295, 56, -18736, -2717, 2752, -2733, -12606, 13946, -13946, -1350, -1351, -13945, -1599, -1600, 13944, -13944, -1352, 1600, -184, 15352, -56, -1408, 1408, -15351, -2661, -18740, 15796, 3176, -2676, -15308, -2694, 168, 247, 183, 184, -247, -18734, -15304, -2754, -12604, -15287, -246, -2923, -2684, 3206, -3185, -15474, -182, -15350, 311, 15231, 65, -183, -9, -15541, 175, -15471, 15351, -55, 15467, -15467, 66, 180, 64, -64, 182, -1442, -1378, -15326, 18560, 15808, -18549, 155, -15459, 245, -245, -15349, 53, 54, 15540, -244, -15540, -2952, -2705, -2753, -18541, 3208, -18513, -18560, 15294, -15348, 15349, -53, -15285, 52, -18775, -2732, -15346, 18672, -15448, 15539, -243, -15539, 243, -15347, 51, -15538, -242, -178, -14, -179, 15431, 107, 241, 15537, -241, -2734, -15314, -15345, -15536, -20201, 4905]\n","Test各delta值的次数： [72, 1669, 1329, 230, 9111, 76, 80, 65, 5, 1, 122, 177, 1, 1, 202, 18, 37, 6, 18, 517, 1, 1, 1, 1, 1, 1, 150, 1, 1, 1, 1, 44, 8332, 153, 8974, 8, 135, 7, 6, 64, 40, 1, 1, 21, 1, 19, 4, 1, 1, 2, 1, 2, 1, 112, 4, 91, 65, 37, 104, 1, 1, 3, 1, 2, 396, 171, 127, 112, 483, 174, 1, 2, 175, 126, 69, 1, 118, 2, 3, 2, 110, 1, 3, 1, 1, 2, 2, 3, 1, 1, 1, 1, 346, 95, 2, 2, 2, 2, 1, 7, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 3, 1, 4, 3, 3, 3, 1, 2, 1, 2, 2, 2, 1, 155, 2, 2, 4, 1, 1, 4, 3, 1, 2, 2, 2, 2, 2, 1, 4, 2, 1, 1, 4, 2, 1, 1, 3, 1, 3, 2, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 177, 1, 1, 1, 180, 5, 2, 2, 1, 12, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 7, 1, 2, 3, 1, 12, 1, 1, 118, 4, 7, 1, 1, 1, 2, 2, 2, 1, 1, 88, 99, 1, 1, 1, 20, 14, 1, 1, 20, 11, 8, 2, 102, 2, 9, 1, 76, 94, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 91, 1, 1, 1, 1, 144, 1, 1, 33, 332, 1, 1, 2, 1, 1, 1, 1, 3, 3, 3, 2, 1, 45, 1, 1, 2, 122, 5, 4, 8, 2, 131, 4, 7, 4, 4, 2, 3, 7, 1, 6, 2, 11, 1, 10, 3, 5, 7, 2, 9, 1, 1, 2, 1, 2, 3, 3, 2, 3, 1, 3, 4, 2, 2, 1, 4, 4, 4, 2, 4, 1, 1, 1, 2, 2, 4, 4, 9, 1, 3, 1, 2, 1, 1, 1, 8, 5, 3, 2, 4, 4, 1, 1, 1, 3, 2, 1, 2, 1, 4, 2, 2, 1, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 3, 4, 1, 1, 2, 1, 1, 4, 1, 1, 1, 2, 2, 1, 1, 1, 1, 50, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 4, 20, 104, 1, 1, 24, 5, 10, 1, 1, 9, 1, 3, 4, 5, 1, 4, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 81, 1, 1, 1, 1, 1, 2, 2, 1, 122, 1, 1, 386, 1, 1, 1, 1, 5, 133, 1, 1, 1, 1, 1, 5, 19, 1, 1, 1, 4, 1, 69, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 6, 1, 1, 1, 1, 68, 51, 31, 18, 3, 143, 135, 110, 1, 1, 1, 1, 1, 1, 2, 2, 24, 2, 2, 7, 7, 1, 1, 10, 1, 1, 101, 91, 101, 104, 9, 1, 113, 67, 1, 1, 1, 1, 1, 82, 87, 71, 1, 1, 11, 3, 1, 1]\n","Test_delta种类： 566\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3A3rY5iy3X4Z","executionInfo":{"status":"ok","timestamp":1623222956365,"user_tz":-480,"elapsed":2186,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"400e0443-251f-4f85-ce20-c51175bb4526"},"source":["from random import randint\n","#统计delta计数较少的index，考虑归为新类\n","#字典收集训练集的delta值\n","value_cnt = {}\n","for delta in list_a:\n","\t# get(value, num)函数的作用是获取字典中value对应的键值, num=0指示初始值大小。\n","\tvalue_cnt[delta] = value_cnt.get(delta, 0) + 1\n","\n","# 打印输出结果\n","print(\"字典收集训练集（delta类别-次数）：\",value_cnt)\n","print(\"各delta值：\",[key for key in value_cnt.keys()])\n","Vals=[delta for delta in value_cnt.values()]\n","print(\"各delta值的次数：\",Vals)\n","print(\"delta种类：\",len(Vals))\n","\n","Vals_Record=[delta for delta in value_cnt.keys() if value_cnt[delta] >5] #要保留记录的delta\n","Vals_Thr_Num=[delta for delta in Vals if delta >5] #要保留记录的delta次数\n","Vals_Thr_Dele=[delta for delta in value_cnt.keys() if value_cnt[delta] <=5] #次数小于等于10次的delta，归成一类\n","Vals_Thr_Dele_Num=[delta for delta in Vals if delta <=5] #要归为一类记录的delta次数\n","print(\"要保留的delta：\",Vals_Record)\n","print(\"要保留的delta次数：\",Vals_Thr_Num)\n","print(\"要保留的delta种类数：\",len(Vals_Thr_Num))\n","#将不保留记录的delta都归为一个新类\n","type_new=len(Vals_Thr_Num)+1\n","\n","print(\"要删除的delta：\",Vals_Thr_Dele)\n","print(\"要删除的delta次数：\",Vals_Thr_Num)\n","print(\"要删除的delta种类：\",len(Vals_Thr_Dele))\n","\n","#设置一个新类，统归为初始值0，利用随机生成一个0-type_new之间的数，作为最终新类值\n","init_val=0\n","while init_val in Vals_Record:\n","  init_val=randint(0,type_new)\n","print(\"新类的值: \",init_val)\n","\n","#对于训练集，不保留的类都归为值：init_val\n","print(len(set(train_Delta)))\n","print('不保留的类都归为值：',init_val)\n","for i in Vals_Thr_Dele:\n","  train_Delta[train_Delta==i]=init_val\n","\n","#对于测试集，同样将不保留的值归为：init_val\n","for i in range(len(test_Delta)):\n","  if test_Delta[i] not in Vals_Record:\n","    test_Delta[i]=init_val\n","\n","print(set(test_Delta))\n","\n","print(len(set(train_Delta)))\n","  \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["字典收集训练集（delta类别-次数）： {0: 33074, 15703: 32, -15703: 1, -256: 16200, -15447: 28, 256: 9467, -15446: 161, 15702: 163, 255: 354, -199: 1, -15247: 1, 1: 519, -15445: 162, 15701: 163, -201: 1, -15245: 1, 15488: 198, -15488: 82, -203: 1, -15242: 1, 213: 263, 15700: 146, -15444: 147, -15701: 1, -206: 1, -15239: 1, 212: 162, -207: 1, -15237: 1, -15443: 166, 15699: 149, -15700: 3, -210: 2, -15233: 1, 15698: 162, -15442: 135, 210: 193, 211: 196, -212: 6, -15486: 7, -15441: 133, 15697: 148, -15698: 24, -470: 1, -15484: 4, -15697: 16, 209: 135, -216: 1, -15481: 3, -15440: 154, 15696: 171, -219: 1, -15477: 4, -15439: 161, 15695: 160, -15696: 19, -220: 1, -15475: 4, 208: 185, 207: 173, -223: 40, -15472: 3, 15694: 132, -15438: 148, 206: 193, -15695: 1, -225: 61, -15469: 9, 15693: 156, -15437: 166, -15693: 2, -1: 318, -227: 159, -15466: 7, 205: 200, 15692: 155, -15436: 162, -230: 68, -15463: 1, -232: 1, -15460: 1, 203: 166, -15435: 153, 204: 194, 15691: 150, -234: 1, -15458: 2, 257: 338, -15692: 1, -15434: 169, -236: 1, -15455: 3, 15690: 167, -15690: 1, -239: 1, -15451: 1, 202: 227, 15689: 167, -15433: 161, -240: 1, -15449: 3, -15432: 148, 15688: 145, -243: 1, -15190: 1, 200: 173, -15689: 1, -15688: 1, -245: 1, -15187: 1, -15431: 162, 15687: 170, -15687: 8, -247: 1, -15184: 1, 15686: 161, -15430: 164, -249: 1, 198: 16, -252: 1, -15178: 1, 15685: 146, -15429: 127, -254: 1, 197: 10, 196: 23, -15428: 148, 15684: 147, -15685: 18, -512: 3, 512: 1, -15173: 1, -15684: 16, -192: 956, -15492: 5, -15427: 159, 15683: 154, -259: 1, -15169: 1, 195: 14, 15168: 64, -15168: 46, -260: 1, 320: 30, -15487: 39, -15426: 170, 15682: 168, -15683: 1, -263: 1, 194: 2, -15425: 277, -265: 1, 15681: 243, -267: 1, -15478: 2, 15680: 112, -15424: 6646, -15681: 7, 15424: 15783, 258: 5, 193: 13, 253: 139, -15678: 157, 192: 451, 252: 167, -15676: 168, -15680: 71, -15679: 131, -15423: 105, -7: 3, -15672: 161, 15679: 21, 254: 165, -9: 2, -15669: 161, -11: 4, -15667: 166, -15677: 135, 15677: 10, 15678: 1, -14: 2, -15664: 173, -15485: 15, -16: 2, -15660: 160, -18: 1, -15658: 165, -15675: 161, 251: 164, 15676: 1, -20: 4, -15655: 171, -15674: 147, 250: 155, -23: 3, -15652: 168, -15482: 2, -24: 3, -15650: 150, 249: 193, -15673: 169, 15674: 1, -27: 1, -15646: 137, 248: 223, -29: 5, -15643: 169, 15672: 3, -31: 2, -15641: 143, 247: 250, -15671: 147, -15480: 1, -33: 2, -15638: 250, -15670: 168, 246: 460, 15671: 1, 15670: 12, -36: 1, -15634: 160, 245: 588, -38: 1, -15632: 174, 15669: 22, 15104: 2, 565: 1, -15104: 1, 182: 2, -40: 2, -15629: 168, 244: 384, -15668: 142, 15668: 1, -43: 2, -15626: 207, -15476: 8, -44: 2, -15624: 189, 243: 348, -47: 3, -15620: 85, 242: 309, -15666: 129, -49: 2, -15617: 83, -15474: 29, -576: 1, -15090: 1, 15666: 27, -15665: 143, 241: 329, -15473: 7, -51: 3, 15665: 2, -53: 1, -15420: 4, 240: 337, -56: 2, -15416: 2, 15664: 30, -15663: 162, 239: 336, -57: 2, -15414: 12, -60: 2, -15411: 3, 238: 300, -15662: 138, 15662: 12, -62: 5, -15408: 1, -15470: 1, 237: 342, -15661: 140, -15406: 17, -15597: 155, -15405: 37, 236: 334, -67: 4, -15594: 149, 15661: 1, -69: 3, -15591: 129, 235: 320, -15659: 112, -71: 3, -15589: 239, -15468: 2, -15467: 37, 15659: 1, -73: 3, -15586: 192, 234: 345, -76: 2, -15582: 135, 233: 347, -15657: 153, -77: 3, -15580: 123, -15465: 8, -15656: 148, 232: 301, -80: 2, -15577: 11, 15657: 2, -82: 3, -15574: 36, 231: 287, 15656: 1, 15655: 5, -84: 1, -15571: 12, 230: 286, -15654: 163, -86: 2, -15568: 26, -15462: 1, 15040: 1, 614: 1, 15654: 27, -89: 1, -15565: 28, 229: 360, -15653: 146, -91: 1, -15563: 18, 228: 355, -93: 1, -15559: 19, -15651: 160, 227: 355, -96: 2, -15556: 24, -97: 1, -15554: 4, 15651: 15, 226: 356, -100: 1, -128: 22, 15650: 3, -15649: 167, -102: 1, 225: 374, -104: 1, -15417: 1, -15457: 2, 15649: 8, 14784: 58, 865: 23, -14784: 22, -14783: 1, 224: 418, -15648: 161, -106: 2, 223: 316, -15647: 146, -109: 1, 863: 12, -110: 2, -15409: 7, 222: 248, -113: 1, -640: 11, -15006: 9, 640: 2, -15454: 24, -15453: 3, 221: 193, -15645: 159, -115: 2, -15402: 1, 15645: 1, -117: 1, -15400: 2, -15644: 150, 220: 170, -120: 1, -15397: 19, 219: 186, -122: 2, -15393: 25, -124: 1, -15391: 2, -15642: 159, 218: 168, -126: 2, -15388: 3, -15450: 2, 217: 160, 15641: 1, -15385: 9, -129: 2, -15513: 3, 15642: 2, -130: 1, -15511: 3, 216: 193, -15640: 170, -133: 2, -15507: 2, -15639: 168, 215: 160, -135: 1, -15504: 3, 15639: 21, 14976: 8, 663: 5, -14976: 3, -137: 1, -15502: 2, 214: 275, -139: 1, -15499: 2, -15637: 224, -2: 3, -15636: 149, -4: 1, -15633: 150, 15637: 1, -15630: 147, 15636: 3, -8: 2, -15628: 160, -15635: 166, -13: 3, -15621: 89, -15: 1, -15619: 100, 15633: 31, -17: 2, -15631: 162, -21: 5, -15418: 1, -15415: 19, -26: 2, -15412: 2, -28: 1, -15403: 1, -15627: 131, -35: 2, -15401: 1, -704: 84, -14923: 14, 704: 32, 15627: 3, -37: 1, -15398: 5, -15394: 43, 201: 288, -15625: 280, -41: 2, -15392: 2, -18477: 2, -1242: 1, 19520: 1, 199: 15, 2852: 1, -18278: 2, 18480: 3, -18480: 3, 2854: 2, -18279: 1, 2855: 1, -15390: 1, 15625: 7, 14912: 2, 714: 1, 15626: 3, -45: 1, -15386: 2, -15384: 6, -19528: 1, 19728: 1, -754: 1, -14670: 1, 191: 7, -52: 1, -54: 2, -15378: 2, 15624: 45, -15623: 83, -59: 1, -15373: 4, 15623: 70, -15369: 2, 15622: 93, -15622: 81, -63: 4, -15367: 2, -15366: 17, -66: 3, 15621: 89, -68: 3, -15553: 1, 15620: 81, -70: 4, -15551: 4, 15619: 101, -72: 4, -15547: 3, -75: 1, -15544: 1, 15618: 89, -15618: 87, -15542: 4, 15617: 84, -79: 1, -15539: 3, 15616: 101, -15616: 100, -15535: 2, -83: 2, -15533: 4, 15615: 89, -15615: 86, -15530: 2, -88: 1, -15527: 5, 15614: 83, -15614: 81, -15422: 1, -90: 1, -15525: 5, -15613: 100, 15613: 101, -92: 1, -15521: 5, 15612: 91, -15612: 85, -95: 1, -15518: 4, 14848: 5, -14848: 3, 765: 1, -14847: 1, -15516: 4, -15611: 84, 15611: 81, -99: 1, -15419: 1, 15610: 95, -15610: 99, -101: 1, -15509: 4, -103: 1, -15609: 87, 15609: 70, 15360: 14395, -15360: 9238, -15359: 161, 15608: 21, -15608: 80, -108: 1, -15501: 1, -15361: 18, -15498: 1, -15607: 92, -112: 1, -15495: 1, 15607: 12, -15351: 3, -15606: 288, 15606: 3, -19546: 1, 19584: 2, -116: 1, -15490: 1, -119: 1, -15605: 436, -15356: 1, -15413: 5, -123: 1, -15354: 3, -15349: 2, -131: 1, -15604: 238, -136: 1, -19570: 1, -3: 2, -15602: 179, -6: 2, -15599: 168, -15596: 171, -15593: 170, -15603: 179, 15604: 2, 13: 1, -19586: 1, 19830: 1, -22249: 1, 22016: 2, 10: 3, -22026: 1, -15365: 1, 15602: 6, -15601: 175, -15579: 21, 15601: 1, 15296: 280, -15296: 225, 306: 5, -15410: 1, -15575: 13, -15600: 209, -30: 1, 15600: 17, -32: 1, 15599: 3, -15564: 13, -15407: 5, -39: 1, -15560: 37, -320: 35, -15279: 4, -15598: 170, -15558: 31, 15598: 20, -1087: 1, -14511: 1, -48: 3, -15357: 1, -15350: 1, -15595: 162, -58: 1, -15346: 2, 15596: 1, -15343: 3, -61: 2, -15342: 4, -15341: 3, -15340: 2, -15339: 3, -64: 2, -15338: 15, -65: 3, -15529: 5, 24: 2, -15528: 4, 22: 1, -15382: 7, 15594: 1, -15526: 2, -15524: 4, -15523: 4, -15522: 3, -15592: 153, -15520: 5, 15592: 1, -15519: 4, -74: 2, -15515: 3, 15591: 4, 15232: 6, -15232: 3, 360: 2, -15231: 1, -15512: 3, -15590: 174, 83: 2, -2202: 1, 2240: 123, -2246: 5, -2262: 2, -18014: 1, 4: 2, -25: 1, 18048: 110, -18026: 2, -19699: 1, 19712: 1, -18243: 1, -18068: 2, -18049: 10, -2229: 2, -2260: 4, -2221: 8, -18047: 3, -2251: 4, -2222: 1, -2227: 1, -2231: 1, -2240: 8, -2238: 3, -2263: 2, -2219: 4, -18038: 4, -18046: 7, -22: 2, -18027: 3, -3871: 1, 21888: 3, -21879: 1, -15372: 3, -18013: 5, -2268: 1, -18037: 2, -18052: 7, -2245: 8, -18055: 3, -2249: 5, -2259: 3, -18025: 5, -19418: 1, 19392: 3, 32: 1, -2235: 4, -18035: 6, -2255: 2, -19391: 2, -18029: 4, -18053: 2, -2230: 2, -18050: 10, -2241: 4, -2243: 6, -18033: 4, -15759: 1, -18063: 2, -21896: 1, -2244: 3, -2217: 2, -18036: 1, -2250: 2, -2220: 2, -2237: 4, -2264: 2, -18040: 1, 3: 2, 15794: 1, -15368: 1, -19490: 1, 19460: 1, -2218: 3, -2261: 1, 2: 1, -18028: 4, 15796: 1, -18012: 3, -2248: 2, -2242: 4, -2247: 2, -18054: 1, -18031: 2, -18070: 2, -2254: 1, -18034: 2, -2236: 2, 2651: 1, -18305: 2, 18304: 4, -18051: 1, -15383: 3, -2213: 1, -2257: 3, -15757: 1, 5: 3, -18044: 2, -2223: 3, -2252: 2, -2228: 1, -2253: 1, -2224: 1, -15387: 6, -18011: 2, -18066: 2, -18015: 2, -18061: 1, -15380: 5, -18306: 1, -18284: 1, -18042: 1, -18067: 1, -18032: 1, -18048: 1, -19648: 1, 19619: 1, -19433: 1, 646: 1, 18816: 5, -19462: 1, 19456: 1, -18787: 1, -15389: 2, -18783: 6, 3423: 3, -19701: 1, 19899: 1, 15590: 96, -6723: 1, 22084: 3, -22084: 3, 22113: 2, -18065: 1, -22095: 1, -22116: 1, 22092: 2, -22090: 2, 22087: 3, -22086: 5, -22087: 1, 22090: 2, -22091: 1, 22085: 4, -22085: 4, 22086: 3, 22100: 1, -22100: 1, 22144: 2, -22144: 1, -22097: 2, -10: 1, 22097: 1, 22094: 1, -22094: 1, -22092: 1, 22105: 1, 17: 1, -15377: 2, 145: 1, -15505: 3, 28: 1, 18259: 1, -194: 1, 36: 16, -15396: 39, 15589: 154, -229: 123, 15502: 2, 88: 2, 38: 5, 37: 8, 139: 1, 15359: 7, -18009: 1, 2649: 1, -15588: 199, 15588: 150, -228: 118, -18019: 1, -15587: 198, -18023: 1, -15395: 33, 15587: 154, 35: 14, -15379: 5, -163: 1, -15331: 13, 15586: 139, -226: 148, -18039: 1, 34: 15, -18043: 1, -15362: 1, 15585: 92, -15585: 258, 2689: 1, 18279: 1, 33: 3, 15394: 1, -34: 1, -18056: 1, 2696: 1, -15584: 307, -2699: 1, 2700: 1, 15584: 53, 18288: 1, 2706: 1, 2710: 1, 288: 2, -15583: 221, 15583: 73, -18073: 1, 18297: 1, -18076: 1, 2716: 1, -18080: 1, 2720: 1, -2724: 1, 18307: 1, -22168: 1, 22391: 1, -2289: 1, 2512: 1, 15582: 54, -22401: 2, -600: 1, 2827: 1, -980: 1, 980: 1, -982: 2, 20928: 4, -22173: 1, 1246: 1, -22174: 1, 6808: 1, -18085: 1, 2725: 1, -20924: 1, 21147: 1, -18087: 1, 18310: 1, -2729: 1, 2729: 1, -15581: 206, -21150: 1, -2731: 1, 2731: 1, -20170: 1, -22172: 1, 6814: 1, 15581: 170, -18093: 1, 18315: 1, -18095: 1, 2735: 1, -2737: 1, 18318: 1, 15580: 125, -2739: 1, 2739: 1, -2741: 1, 2741: 1, -2742: 1, 2742: 1, -2744: 1, 2744: 1, -4415: 1, 19995: 1, -2746: 1, 18326: 1, -15260: 5, -4429: 1, 19776: 7, -18328: 1, 18112: 3, -323: 22, 323: 22, -195: 1, 324: 33, -324: 30, -196: 2, -467: 32, 467: 31, 468: 32, -468: 26, -994: 14, 994: 30, -768: 45, -162: 1, -832: 1, 768: 3, 995: 31, -995: 4, -1930: 1, 2157: 31, -237: 19, -1920: 39, -2157: 11, -173: 1, -1984: 1, -1925: 1, 1920: 3, -2158: 11, 2158: 30, -238: 22, -18096: 1, 18334: 28, -222: 3, -18112: 3, -18334: 24, -20200: 1, 19968: 1, -21982: 1, 792: 1, 1416: 1, -19774: 1, 1672: 1, 2746: 1, -2784: 1, 15832: 1, 2304: 5, -19770: 1, 4429: 1, 13048: 2, -15352: 3, -4183: 1, 4183: 1, 12996: 1, 2368: 3, -2316: 2, -18530: 1, 16200: 1, -15777: 1, -18133: 1, -3060: 1, 21248: 2, -18620: 1, 18560: 14, -18560: 29, -2628: 1, -19830: 1, -1418: 1, 3060: 1, 15772: 1, -2763: 1, -2327: 1, -17450: 1, 8: 1, -18892: 1, 18880: 2, -19772: 1, 892: 1, -15344: 1, -19502: 1, 4142: 1, 13034: 1, -13034: 1, -4431: 1, -15345: 1, -19793: 1, 4433: 1, -19794: 2, -4435: 3, 19840: 2, 177: 2, -4436: 1, 20017: 1, -4425: 1, 20007: 1, -4196: 1, 19549: 1, -15353: 2, 15578: 90, -15578: 14, 15577: 70, 15576: 80, -15576: 27, 15574: 114, -15573: 27, 15572: 74, -15572: 14, -15570: 16, -15324: 20, 15569: 82, -15569: 14, -15323: 72, 15579: 76, 15568: 70, 15566: 115, -15566: 23, 15563: 104, 128: 1, 15562: 121, -15562: 10, 15559: 143, -15322: 78, 15558: 146, 15556: 153, 15555: 140, -15555: 15, 15554: 118, 15552: 1, -15552: 1, 15551: 3, 15550: 3, -15550: 3, -15321: 69, 15549: 3, -15549: 3, 187: 1, 186: 1, -15546: 2, 15347: 1, -15347: 1, 15545: 3, -15545: 2, 15543: 2, -15543: 3, 15542: 2, 15541: 2, 15539: 2, -15320: 73, 15538: 2, -15538: 1, 15537: 2, -15537: 3, 152: 2, 175: 2, -15534: 1, 173: 1, -15319: 71, 15532: 2, -15532: 2, 15575: 67, 15531: 2, -15531: 3, 15529: 2, 15528: 2, 15527: 2, 165: 1, 15524: 1, 15523: 3, 15521: 2, -15318: 104, 15520: 3, 15519: 2, 15518: 2, 15516: 2, 15515: 3, 15514: 2, -15514: 2, 15512: 3, 151: 1, 15573: 89, -15317: 95, 15510: 2, -15510: 2, 15508: 3, -15316: 82, 15507: 1, -15315: 74, 146: 1, -15314: 70, 15504: 2, -15312: 60, -15381: 1, 15571: 76, 15567: 87, -15375: 1, 277: 5, -15308: 73, -19200: 3, 19199: 2, 3627: 1, -15307: 96, -15306: 137, 129: 1, 15560: 126, -15302: 117, 15557: 137, -15301: 126, -15299: 134, -15298: 95, 15553: 14, -15297: 11, 276: 8, -15295: 3, 15547: 2, -15355: 1, 15546: 2, 15570: 63, 183: 2, -3648: 2, 19218: 1, 3648: 1, 181: 1, -15541: 2, 15540: 2, -15540: 2, 12928: 2, -12928: 2, 15536: 1, -15536: 1, 15535: 2, 15533: 2, -15313: 84, 171: 1, -1344: 7, 1344: 7, 15526: 2, -15334: 2, 164: 1, 15522: 1, -15311: 84, -15567: 16, 15509: 2, -15508: 2, 15506: 1, -15506: 1, 15505: 2, -15310: 113, 142: 1, -15309: 101, 271: 4, 15500: 2, 138: 1, 15497: 2, -15305: 108, 15565: 105, 15496: 2, -15304: 130, -19264: 4, 19264: 3, 19265: 1, 15495: 1, -15303: 146, 15493: 1, 132: 1, -15300: 142, -15374: 2, 15564: 66, 15561: 143, -15561: 43, 269: 7, -15371: 1, -15370: 1, -14912: 1, 169: 1, -15337: 1, -20117: 1, 1856: 2, -2836: 1, 21120: 3, -15328: 1, 15525: 1, -5796: 1, -2470: 1, 2496: 1, -20122: 1, -2838: 1, -22365: 1, 22400: 9, -22399: 2, 6: 1, -22406: 1, -22352: 2, -22402: 1, -22404: 1, 15517: 1, 43: 1, 15513: 1, 264: 11, -384: 1, 384: 1, 15511: 1, 49: 1, -19968: 1, 4408: 1, 390: 2, 391: 15, -15167: 1, 15503: 1, -15503: 1, 57: 1, -4416: 1, 4416: 1, -15500: 1, -15497: 1, 62: 1, -2368: 5, -13190: 3, -15557: 39, -13189: 1, 15494: 1, -15494: 1, -197: 1, 15491: 1, -4672: 10, 20229: 2, 4672: 8, 15490: 1, 13184: 59, -13184: 43, 2373: 15, -13183: 1, 261: 8, 15485: 1, 71: 1, 15484: 1, -1216: 5, 1216: 5, 15482: 1, 14336: 41, -14336: 24, 1143: 1, 76: 1, 1219: 13, 1220: 4, 15300: 2, -964: 2, -1220: 14, 964: 14, -14335: 1, 15478: 1, 78: 1, 3005: 14, 18561: 1, 15475: 1, 15473: 1, -3008: 35, 3008: 23, 18562: 3, 18563: 9, 15472: 1, 15469: 1, 703: 2, -14851: 3, -14850: 31, 15467: 1, 87: 1, -14849: 2, 15466: 1}\n","各delta值： [0, 15703, -15703, -256, -15447, 256, -15446, 15702, 255, -199, -15247, 1, -15445, 15701, -201, -15245, 15488, -15488, -203, -15242, 213, 15700, -15444, -15701, -206, -15239, 212, -207, -15237, -15443, 15699, -15700, -210, -15233, 15698, -15442, 210, 211, -212, -15486, -15441, 15697, -15698, -470, -15484, -15697, 209, -216, -15481, -15440, 15696, -219, -15477, -15439, 15695, -15696, -220, -15475, 208, 207, -223, -15472, 15694, -15438, 206, -15695, -225, -15469, 15693, -15437, -15693, -1, -227, -15466, 205, 15692, -15436, -230, -15463, -232, -15460, 203, -15435, 204, 15691, -234, -15458, 257, -15692, -15434, -236, -15455, 15690, -15690, -239, -15451, 202, 15689, -15433, -240, -15449, -15432, 15688, -243, -15190, 200, -15689, -15688, -245, -15187, -15431, 15687, -15687, -247, -15184, 15686, -15430, -249, 198, -252, -15178, 15685, -15429, -254, 197, 196, -15428, 15684, -15685, -512, 512, -15173, -15684, -192, -15492, -15427, 15683, -259, -15169, 195, 15168, -15168, -260, 320, -15487, -15426, 15682, -15683, -263, 194, -15425, -265, 15681, -267, -15478, 15680, -15424, -15681, 15424, 258, 193, 253, -15678, 192, 252, -15676, -15680, -15679, -15423, -7, -15672, 15679, 254, -9, -15669, -11, -15667, -15677, 15677, 15678, -14, -15664, -15485, -16, -15660, -18, -15658, -15675, 251, 15676, -20, -15655, -15674, 250, -23, -15652, -15482, -24, -15650, 249, -15673, 15674, -27, -15646, 248, -29, -15643, 15672, -31, -15641, 247, -15671, -15480, -33, -15638, -15670, 246, 15671, 15670, -36, -15634, 245, -38, -15632, 15669, 15104, 565, -15104, 182, -40, -15629, 244, -15668, 15668, -43, -15626, -15476, -44, -15624, 243, -47, -15620, 242, -15666, -49, -15617, -15474, -576, -15090, 15666, -15665, 241, -15473, -51, 15665, -53, -15420, 240, -56, -15416, 15664, -15663, 239, -57, -15414, -60, -15411, 238, -15662, 15662, -62, -15408, -15470, 237, -15661, -15406, -15597, -15405, 236, -67, -15594, 15661, -69, -15591, 235, -15659, -71, -15589, -15468, -15467, 15659, -73, -15586, 234, -76, -15582, 233, -15657, -77, -15580, -15465, -15656, 232, -80, -15577, 15657, -82, -15574, 231, 15656, 15655, -84, -15571, 230, -15654, -86, -15568, -15462, 15040, 614, 15654, -89, -15565, 229, -15653, -91, -15563, 228, -93, -15559, -15651, 227, -96, -15556, -97, -15554, 15651, 226, -100, -128, 15650, -15649, -102, 225, -104, -15417, -15457, 15649, 14784, 865, -14784, -14783, 224, -15648, -106, 223, -15647, -109, 863, -110, -15409, 222, -113, -640, -15006, 640, -15454, -15453, 221, -15645, -115, -15402, 15645, -117, -15400, -15644, 220, -120, -15397, 219, -122, -15393, -124, -15391, -15642, 218, -126, -15388, -15450, 217, 15641, -15385, -129, -15513, 15642, -130, -15511, 216, -15640, -133, -15507, -15639, 215, -135, -15504, 15639, 14976, 663, -14976, -137, -15502, 214, -139, -15499, -15637, -2, -15636, -4, -15633, 15637, -15630, 15636, -8, -15628, -15635, -13, -15621, -15, -15619, 15633, -17, -15631, -21, -15418, -15415, -26, -15412, -28, -15403, -15627, -35, -15401, -704, -14923, 704, 15627, -37, -15398, -15394, 201, -15625, -41, -15392, -18477, -1242, 19520, 199, 2852, -18278, 18480, -18480, 2854, -18279, 2855, -15390, 15625, 14912, 714, 15626, -45, -15386, -15384, -19528, 19728, -754, -14670, 191, -52, -54, -15378, 15624, -15623, -59, -15373, 15623, -15369, 15622, -15622, -63, -15367, -15366, -66, 15621, -68, -15553, 15620, -70, -15551, 15619, -72, -15547, -75, -15544, 15618, -15618, -15542, 15617, -79, -15539, 15616, -15616, -15535, -83, -15533, 15615, -15615, -15530, -88, -15527, 15614, -15614, -15422, -90, -15525, -15613, 15613, -92, -15521, 15612, -15612, -95, -15518, 14848, -14848, 765, -14847, -15516, -15611, 15611, -99, -15419, 15610, -15610, -101, -15509, -103, -15609, 15609, 15360, -15360, -15359, 15608, -15608, -108, -15501, -15361, -15498, -15607, -112, -15495, 15607, -15351, -15606, 15606, -19546, 19584, -116, -15490, -119, -15605, -15356, -15413, -123, -15354, -15349, -131, -15604, -136, -19570, -3, -15602, -6, -15599, -15596, -15593, -15603, 15604, 13, -19586, 19830, -22249, 22016, 10, -22026, -15365, 15602, -15601, -15579, 15601, 15296, -15296, 306, -15410, -15575, -15600, -30, 15600, -32, 15599, -15564, -15407, -39, -15560, -320, -15279, -15598, -15558, 15598, -1087, -14511, -48, -15357, -15350, -15595, -58, -15346, 15596, -15343, -61, -15342, -15341, -15340, -15339, -64, -15338, -65, -15529, 24, -15528, 22, -15382, 15594, -15526, -15524, -15523, -15522, -15592, -15520, 15592, -15519, -74, -15515, 15591, 15232, -15232, 360, -15231, -15512, -15590, 83, -2202, 2240, -2246, -2262, -18014, 4, -25, 18048, -18026, -19699, 19712, -18243, -18068, -18049, -2229, -2260, -2221, -18047, -2251, -2222, -2227, -2231, -2240, -2238, -2263, -2219, -18038, -18046, -22, -18027, -3871, 21888, -21879, -15372, -18013, -2268, -18037, -18052, -2245, -18055, -2249, -2259, -18025, -19418, 19392, 32, -2235, -18035, -2255, -19391, -18029, -18053, -2230, -18050, -2241, -2243, -18033, -15759, -18063, -21896, -2244, -2217, -18036, -2250, -2220, -2237, -2264, -18040, 3, 15794, -15368, -19490, 19460, -2218, -2261, 2, -18028, 15796, -18012, -2248, -2242, -2247, -18054, -18031, -18070, -2254, -18034, -2236, 2651, -18305, 18304, -18051, -15383, -2213, -2257, -15757, 5, -18044, -2223, -2252, -2228, -2253, -2224, -15387, -18011, -18066, -18015, -18061, -15380, -18306, -18284, -18042, -18067, -18032, -18048, -19648, 19619, -19433, 646, 18816, -19462, 19456, -18787, -15389, -18783, 3423, -19701, 19899, 15590, -6723, 22084, -22084, 22113, -18065, -22095, -22116, 22092, -22090, 22087, -22086, -22087, 22090, -22091, 22085, -22085, 22086, 22100, -22100, 22144, -22144, -22097, -10, 22097, 22094, -22094, -22092, 22105, 17, -15377, 145, -15505, 28, 18259, -194, 36, -15396, 15589, -229, 15502, 88, 38, 37, 139, 15359, -18009, 2649, -15588, 15588, -228, -18019, -15587, -18023, -15395, 15587, 35, -15379, -163, -15331, 15586, -226, -18039, 34, -18043, -15362, 15585, -15585, 2689, 18279, 33, 15394, -34, -18056, 2696, -15584, -2699, 2700, 15584, 18288, 2706, 2710, 288, -15583, 15583, -18073, 18297, -18076, 2716, -18080, 2720, -2724, 18307, -22168, 22391, -2289, 2512, 15582, -22401, -600, 2827, -980, 980, -982, 20928, -22173, 1246, -22174, 6808, -18085, 2725, -20924, 21147, -18087, 18310, -2729, 2729, -15581, -21150, -2731, 2731, -20170, -22172, 6814, 15581, -18093, 18315, -18095, 2735, -2737, 18318, 15580, -2739, 2739, -2741, 2741, -2742, 2742, -2744, 2744, -4415, 19995, -2746, 18326, -15260, -4429, 19776, -18328, 18112, -323, 323, -195, 324, -324, -196, -467, 467, 468, -468, -994, 994, -768, -162, -832, 768, 995, -995, -1930, 2157, -237, -1920, -2157, -173, -1984, -1925, 1920, -2158, 2158, -238, -18096, 18334, -222, -18112, -18334, -20200, 19968, -21982, 792, 1416, -19774, 1672, 2746, -2784, 15832, 2304, -19770, 4429, 13048, -15352, -4183, 4183, 12996, 2368, -2316, -18530, 16200, -15777, -18133, -3060, 21248, -18620, 18560, -18560, -2628, -19830, -1418, 3060, 15772, -2763, -2327, -17450, 8, -18892, 18880, -19772, 892, -15344, -19502, 4142, 13034, -13034, -4431, -15345, -19793, 4433, -19794, -4435, 19840, 177, -4436, 20017, -4425, 20007, -4196, 19549, -15353, 15578, -15578, 15577, 15576, -15576, 15574, -15573, 15572, -15572, -15570, -15324, 15569, -15569, -15323, 15579, 15568, 15566, -15566, 15563, 128, 15562, -15562, 15559, -15322, 15558, 15556, 15555, -15555, 15554, 15552, -15552, 15551, 15550, -15550, -15321, 15549, -15549, 187, 186, -15546, 15347, -15347, 15545, -15545, 15543, -15543, 15542, 15541, 15539, -15320, 15538, -15538, 15537, -15537, 152, 175, -15534, 173, -15319, 15532, -15532, 15575, 15531, -15531, 15529, 15528, 15527, 165, 15524, 15523, 15521, -15318, 15520, 15519, 15518, 15516, 15515, 15514, -15514, 15512, 151, 15573, -15317, 15510, -15510, 15508, -15316, 15507, -15315, 146, -15314, 15504, -15312, -15381, 15571, 15567, -15375, 277, -15308, -19200, 19199, 3627, -15307, -15306, 129, 15560, -15302, 15557, -15301, -15299, -15298, 15553, -15297, 276, -15295, 15547, -15355, 15546, 15570, 183, -3648, 19218, 3648, 181, -15541, 15540, -15540, 12928, -12928, 15536, -15536, 15535, 15533, -15313, 171, -1344, 1344, 15526, -15334, 164, 15522, -15311, -15567, 15509, -15508, 15506, -15506, 15505, -15310, 142, -15309, 271, 15500, 138, 15497, -15305, 15565, 15496, -15304, -19264, 19264, 19265, 15495, -15303, 15493, 132, -15300, -15374, 15564, 15561, -15561, 269, -15371, -15370, -14912, 169, -15337, -20117, 1856, -2836, 21120, -15328, 15525, -5796, -2470, 2496, -20122, -2838, -22365, 22400, -22399, 6, -22406, -22352, -22402, -22404, 15517, 43, 15513, 264, -384, 384, 15511, 49, -19968, 4408, 390, 391, -15167, 15503, -15503, 57, -4416, 4416, -15500, -15497, 62, -2368, -13190, -15557, -13189, 15494, -15494, -197, 15491, -4672, 20229, 4672, 15490, 13184, -13184, 2373, -13183, 261, 15485, 71, 15484, -1216, 1216, 15482, 14336, -14336, 1143, 76, 1219, 1220, 15300, -964, -1220, 964, -14335, 15478, 78, 3005, 18561, 15475, 15473, -3008, 3008, 18562, 18563, 15472, 15469, 703, -14851, -14850, 15467, 87, -14849, 15466]\n","各delta值的次数： [33074, 32, 1, 16200, 28, 9467, 161, 163, 354, 1, 1, 519, 162, 163, 1, 1, 198, 82, 1, 1, 263, 146, 147, 1, 1, 1, 162, 1, 1, 166, 149, 3, 2, 1, 162, 135, 193, 196, 6, 7, 133, 148, 24, 1, 4, 16, 135, 1, 3, 154, 171, 1, 4, 161, 160, 19, 1, 4, 185, 173, 40, 3, 132, 148, 193, 1, 61, 9, 156, 166, 2, 318, 159, 7, 200, 155, 162, 68, 1, 1, 1, 166, 153, 194, 150, 1, 2, 338, 1, 169, 1, 3, 167, 1, 1, 1, 227, 167, 161, 1, 3, 148, 145, 1, 1, 173, 1, 1, 1, 1, 162, 170, 8, 1, 1, 161, 164, 1, 16, 1, 1, 146, 127, 1, 10, 23, 148, 147, 18, 3, 1, 1, 16, 956, 5, 159, 154, 1, 1, 14, 64, 46, 1, 30, 39, 170, 168, 1, 1, 2, 277, 1, 243, 1, 2, 112, 6646, 7, 15783, 5, 13, 139, 157, 451, 167, 168, 71, 131, 105, 3, 161, 21, 165, 2, 161, 4, 166, 135, 10, 1, 2, 173, 15, 2, 160, 1, 165, 161, 164, 1, 4, 171, 147, 155, 3, 168, 2, 3, 150, 193, 169, 1, 1, 137, 223, 5, 169, 3, 2, 143, 250, 147, 1, 2, 250, 168, 460, 1, 12, 1, 160, 588, 1, 174, 22, 2, 1, 1, 2, 2, 168, 384, 142, 1, 2, 207, 8, 2, 189, 348, 3, 85, 309, 129, 2, 83, 29, 1, 1, 27, 143, 329, 7, 3, 2, 1, 4, 337, 2, 2, 30, 162, 336, 2, 12, 2, 3, 300, 138, 12, 5, 1, 1, 342, 140, 17, 155, 37, 334, 4, 149, 1, 3, 129, 320, 112, 3, 239, 2, 37, 1, 3, 192, 345, 2, 135, 347, 153, 3, 123, 8, 148, 301, 2, 11, 2, 3, 36, 287, 1, 5, 1, 12, 286, 163, 2, 26, 1, 1, 1, 27, 1, 28, 360, 146, 1, 18, 355, 1, 19, 160, 355, 2, 24, 1, 4, 15, 356, 1, 22, 3, 167, 1, 374, 1, 1, 2, 8, 58, 23, 22, 1, 418, 161, 2, 316, 146, 1, 12, 2, 7, 248, 1, 11, 9, 2, 24, 3, 193, 159, 2, 1, 1, 1, 2, 150, 170, 1, 19, 186, 2, 25, 1, 2, 159, 168, 2, 3, 2, 160, 1, 9, 2, 3, 2, 1, 3, 193, 170, 2, 2, 168, 160, 1, 3, 21, 8, 5, 3, 1, 2, 275, 1, 2, 224, 3, 149, 1, 150, 1, 147, 3, 2, 160, 166, 3, 89, 1, 100, 31, 2, 162, 5, 1, 19, 2, 2, 1, 1, 131, 2, 1, 84, 14, 32, 3, 1, 5, 43, 288, 280, 2, 2, 2, 1, 1, 15, 1, 2, 3, 3, 2, 1, 1, 1, 7, 2, 1, 3, 1, 2, 6, 1, 1, 1, 1, 7, 1, 2, 2, 45, 83, 1, 4, 70, 2, 93, 81, 4, 2, 17, 3, 89, 3, 1, 81, 4, 4, 101, 4, 3, 1, 1, 89, 87, 4, 84, 1, 3, 101, 100, 2, 2, 4, 89, 86, 2, 1, 5, 83, 81, 1, 1, 5, 100, 101, 1, 5, 91, 85, 1, 4, 5, 3, 1, 1, 4, 84, 81, 1, 1, 95, 99, 1, 4, 1, 87, 70, 14395, 9238, 161, 21, 80, 1, 1, 18, 1, 92, 1, 1, 12, 3, 288, 3, 1, 2, 1, 1, 1, 436, 1, 5, 1, 3, 2, 1, 238, 1, 1, 2, 179, 2, 168, 171, 170, 179, 2, 1, 1, 1, 1, 2, 3, 1, 1, 6, 175, 21, 1, 280, 225, 5, 1, 13, 209, 1, 17, 1, 3, 13, 5, 1, 37, 35, 4, 170, 31, 20, 1, 1, 3, 1, 1, 162, 1, 2, 1, 3, 2, 4, 3, 2, 3, 2, 15, 3, 5, 2, 4, 1, 7, 1, 2, 4, 4, 3, 153, 5, 1, 4, 2, 3, 4, 6, 3, 2, 1, 3, 174, 2, 1, 123, 5, 2, 1, 2, 1, 110, 2, 1, 1, 1, 2, 10, 2, 4, 8, 3, 4, 1, 1, 1, 8, 3, 2, 4, 4, 7, 2, 3, 1, 3, 1, 3, 5, 1, 2, 7, 8, 3, 5, 3, 5, 1, 3, 1, 4, 6, 2, 2, 4, 2, 2, 10, 4, 6, 4, 1, 2, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 4, 1, 3, 2, 4, 2, 1, 2, 2, 1, 2, 2, 1, 2, 4, 1, 3, 1, 3, 1, 3, 2, 3, 2, 1, 1, 1, 6, 2, 2, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 2, 6, 3, 1, 1, 96, 1, 3, 3, 2, 1, 1, 1, 2, 2, 3, 5, 1, 2, 1, 4, 4, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 16, 39, 154, 123, 2, 2, 5, 8, 1, 7, 1, 1, 199, 150, 118, 1, 198, 1, 33, 154, 14, 5, 1, 13, 139, 148, 1, 15, 1, 1, 92, 258, 1, 1, 3, 1, 1, 1, 1, 307, 1, 1, 53, 1, 1, 1, 2, 221, 73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54, 2, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 206, 1, 1, 1, 1, 1, 1, 170, 1, 1, 1, 1, 1, 1, 125, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 7, 1, 3, 22, 22, 1, 33, 30, 2, 32, 31, 32, 26, 14, 30, 45, 1, 1, 3, 31, 4, 1, 31, 19, 39, 11, 1, 1, 1, 3, 11, 30, 22, 1, 28, 3, 3, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 14, 29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 1, 1, 1, 1, 1, 1, 2, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 1, 121, 10, 143, 78, 146, 153, 140, 15, 118, 1, 1, 3, 3, 3, 69, 3, 3, 1, 1, 2, 1, 1, 3, 2, 2, 3, 2, 2, 2, 73, 2, 1, 2, 3, 2, 2, 1, 1, 71, 2, 2, 67, 2, 3, 2, 2, 2, 1, 1, 3, 2, 104, 3, 2, 2, 2, 3, 2, 2, 3, 1, 89, 95, 2, 2, 3, 82, 1, 74, 1, 70, 2, 60, 1, 76, 87, 1, 5, 73, 3, 2, 1, 96, 137, 1, 126, 117, 137, 126, 134, 95, 14, 11, 8, 3, 2, 1, 2, 63, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 84, 1, 7, 7, 2, 2, 1, 1, 84, 16, 2, 2, 1, 1, 2, 113, 1, 101, 4, 2, 1, 2, 108, 105, 2, 130, 4, 3, 1, 1, 146, 1, 1, 142, 2, 66, 143, 43, 7, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 9, 2, 1, 1, 2, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 2, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 3, 39, 1, 1, 1, 1, 1, 10, 2, 8, 1, 59, 43, 15, 1, 8, 1, 1, 1, 5, 5, 1, 41, 24, 1, 1, 13, 4, 2, 2, 14, 14, 1, 1, 1, 14, 1, 1, 1, 35, 23, 3, 9, 1, 1, 2, 3, 31, 1, 1, 2, 1]\n","delta种类： 1304\n","要保留的delta： [0, 15703, -256, -15447, 256, -15446, 15702, 255, 1, -15445, 15701, 15488, -15488, 213, 15700, -15444, 212, -15443, 15699, 15698, -15442, 210, 211, -212, -15486, -15441, 15697, -15698, -15697, 209, -15440, 15696, -15439, 15695, -15696, 208, 207, -223, 15694, -15438, 206, -225, -15469, 15693, -15437, -1, -227, -15466, 205, 15692, -15436, -230, 203, -15435, 204, 15691, 257, -15434, 15690, 202, 15689, -15433, -15432, 15688, 200, -15431, 15687, -15687, 15686, -15430, 198, 15685, -15429, 197, 196, -15428, 15684, -15685, -15684, -192, -15427, 15683, 195, 15168, -15168, 320, -15487, -15426, 15682, -15425, 15681, 15680, -15424, -15681, 15424, 193, 253, -15678, 192, 252, -15676, -15680, -15679, -15423, -15672, 15679, 254, -15669, -15667, -15677, 15677, -15664, -15485, -15660, -15658, -15675, 251, -15655, -15674, 250, -15652, -15650, 249, -15673, -15646, 248, -15643, -15641, 247, -15671, -15638, -15670, 246, 15670, -15634, 245, -15632, 15669, -15629, 244, -15668, -15626, -15476, -15624, 243, -15620, 242, -15666, -15617, -15474, 15666, -15665, 241, -15473, 240, 15664, -15663, 239, -15414, 238, -15662, 15662, 237, -15661, -15406, -15597, -15405, 236, -15594, -15591, 235, -15659, -15589, -15467, -15586, 234, -15582, 233, -15657, -15580, -15465, -15656, 232, -15577, -15574, 231, -15571, 230, -15654, -15568, 15654, -15565, 229, -15653, -15563, 228, -15559, -15651, 227, -15556, 15651, 226, -128, -15649, 225, 15649, 14784, 865, -14784, 224, -15648, 223, -15647, 863, -15409, 222, -640, -15006, -15454, 221, -15645, -15644, 220, -15397, 219, -15393, -15642, 218, 217, -15385, 216, -15640, -15639, 215, 15639, 14976, 214, -15637, -15636, -15633, -15630, -15628, -15635, -15621, -15619, 15633, -15631, -15415, -15627, -704, -14923, 704, -15394, 201, -15625, 199, 15625, -15384, 191, 15624, -15623, 15623, 15622, -15622, -15366, 15621, 15620, 15619, 15618, -15618, 15617, 15616, -15616, 15615, -15615, 15614, -15614, -15613, 15613, 15612, -15612, -15611, 15611, 15610, -15610, -15609, 15609, 15360, -15360, -15359, 15608, -15608, -15361, -15607, 15607, -15606, -15605, -15604, -15602, -15599, -15596, -15593, -15603, 15602, -15601, -15579, 15296, -15296, -15575, -15600, 15600, -15564, -15560, -320, -15598, -15558, 15598, -15595, -15338, -15382, -15592, 15232, -15590, 2240, 18048, -18049, -2221, -2240, -18046, -18052, -2245, -18035, -18050, -2243, -15387, -18783, 15590, 36, -15396, 15589, -229, 37, 15359, -15588, 15588, -228, -15587, -15395, 15587, 35, -15331, 15586, -226, 34, 15585, -15585, -15584, 15584, -15583, 15583, 15582, -15581, 15581, 15580, 19776, -323, 323, 324, -324, -467, 467, 468, -468, -994, 994, -768, 995, 2157, -237, -1920, -2157, -2158, 2158, -238, 18334, -18334, 18560, -18560, 15578, -15578, 15577, 15576, -15576, 15574, -15573, 15572, -15572, -15570, -15324, 15569, -15569, -15323, 15579, 15568, 15566, -15566, 15563, 15562, -15562, 15559, -15322, 15558, 15556, 15555, -15555, 15554, -15321, -15320, -15319, 15575, -15318, 15573, -15317, -15316, -15315, -15314, -15312, 15571, 15567, -15308, -15307, -15306, 15560, -15302, 15557, -15301, -15299, -15298, 15553, -15297, 276, 15570, -15313, -1344, 1344, -15311, -15567, -15310, -15309, -15305, 15565, -15304, -15303, -15300, 15564, 15561, -15561, 269, 22400, 264, 391, -15557, -4672, 4672, 13184, -13184, 2373, 261, 14336, -14336, 1219, -1220, 964, 3005, -3008, 3008, 18563, -14850]\n","要保留的delta次数： [33074, 32, 16200, 28, 9467, 161, 163, 354, 519, 162, 163, 198, 82, 263, 146, 147, 162, 166, 149, 162, 135, 193, 196, 6, 7, 133, 148, 24, 16, 135, 154, 171, 161, 160, 19, 185, 173, 40, 132, 148, 193, 61, 9, 156, 166, 318, 159, 7, 200, 155, 162, 68, 166, 153, 194, 150, 338, 169, 167, 227, 167, 161, 148, 145, 173, 162, 170, 8, 161, 164, 16, 146, 127, 10, 23, 148, 147, 18, 16, 956, 159, 154, 14, 64, 46, 30, 39, 170, 168, 277, 243, 112, 6646, 7, 15783, 13, 139, 157, 451, 167, 168, 71, 131, 105, 161, 21, 165, 161, 166, 135, 10, 173, 15, 160, 165, 161, 164, 171, 147, 155, 168, 150, 193, 169, 137, 223, 169, 143, 250, 147, 250, 168, 460, 12, 160, 588, 174, 22, 168, 384, 142, 207, 8, 189, 348, 85, 309, 129, 83, 29, 27, 143, 329, 7, 337, 30, 162, 336, 12, 300, 138, 12, 342, 140, 17, 155, 37, 334, 149, 129, 320, 112, 239, 37, 192, 345, 135, 347, 153, 123, 8, 148, 301, 11, 36, 287, 12, 286, 163, 26, 27, 28, 360, 146, 18, 355, 19, 160, 355, 24, 15, 356, 22, 167, 374, 8, 58, 23, 22, 418, 161, 316, 146, 12, 7, 248, 11, 9, 24, 193, 159, 150, 170, 19, 186, 25, 159, 168, 160, 9, 193, 170, 168, 160, 21, 8, 275, 224, 149, 150, 147, 160, 166, 89, 100, 31, 162, 19, 131, 84, 14, 32, 43, 288, 280, 15, 7, 6, 7, 45, 83, 70, 93, 81, 17, 89, 81, 101, 89, 87, 84, 101, 100, 89, 86, 83, 81, 100, 101, 91, 85, 84, 81, 95, 99, 87, 70, 14395, 9238, 161, 21, 80, 18, 92, 12, 288, 436, 238, 179, 168, 171, 170, 179, 6, 175, 21, 280, 225, 13, 209, 17, 13, 37, 35, 170, 31, 20, 162, 15, 7, 153, 6, 174, 123, 110, 10, 8, 8, 7, 7, 8, 6, 10, 6, 6, 6, 96, 16, 39, 154, 123, 8, 7, 199, 150, 118, 198, 33, 154, 14, 13, 139, 148, 15, 92, 258, 307, 53, 221, 73, 54, 206, 170, 125, 7, 22, 22, 33, 30, 32, 31, 32, 26, 14, 30, 45, 31, 31, 19, 39, 11, 11, 30, 22, 28, 24, 14, 29, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 121, 10, 143, 78, 146, 153, 140, 15, 118, 69, 73, 71, 67, 104, 89, 95, 82, 74, 70, 60, 76, 87, 73, 96, 137, 126, 117, 137, 126, 134, 95, 14, 11, 8, 63, 84, 7, 7, 84, 16, 113, 101, 108, 105, 130, 146, 142, 66, 143, 43, 7, 9, 11, 15, 39, 10, 8, 59, 43, 15, 8, 41, 24, 13, 14, 14, 14, 35, 23, 9, 31]\n","要保留的delta种类数： 478\n","要删除的delta： [-15703, -199, -15247, -201, -15245, -203, -15242, -15701, -206, -15239, -207, -15237, -15700, -210, -15233, -470, -15484, -216, -15481, -219, -15477, -220, -15475, -15472, -15695, -15693, -15463, -232, -15460, -234, -15458, -15692, -236, -15455, -15690, -239, -15451, -240, -15449, -243, -15190, -15689, -15688, -245, -15187, -247, -15184, -249, -252, -15178, -254, -512, 512, -15173, -15492, -259, -15169, -260, -15683, -263, 194, -265, -267, -15478, 258, -7, -9, -11, 15678, -14, -16, -18, 15676, -20, -23, -15482, -24, 15674, -27, -29, 15672, -31, -15480, -33, 15671, -36, -38, 15104, 565, -15104, 182, -40, 15668, -43, -44, -47, -49, -576, -15090, -51, 15665, -53, -15420, -56, -15416, -57, -60, -15411, -62, -15408, -15470, -67, 15661, -69, -71, -15468, 15659, -73, -76, -77, -80, 15657, -82, 15656, 15655, -84, -86, -15462, 15040, 614, -89, -91, -93, -96, -97, -15554, -100, 15650, -102, -104, -15417, -15457, -14783, -106, -109, -110, -113, 640, -15453, -115, -15402, 15645, -117, -15400, -120, -122, -124, -15391, -126, -15388, -15450, 15641, -129, -15513, 15642, -130, -15511, -133, -15507, -135, -15504, 663, -14976, -137, -15502, -139, -15499, -2, -4, 15637, 15636, -8, -13, -15, -17, -21, -15418, -26, -15412, -28, -15403, -35, -15401, 15627, -37, -15398, -41, -15392, -18477, -1242, 19520, 2852, -18278, 18480, -18480, 2854, -18279, 2855, -15390, 14912, 714, 15626, -45, -15386, -19528, 19728, -754, -14670, -52, -54, -15378, -59, -15373, -15369, -63, -15367, -66, -68, -15553, -70, -15551, -72, -15547, -75, -15544, -15542, -79, -15539, -15535, -83, -15533, -15530, -88, -15527, -15422, -90, -15525, -92, -15521, -95, -15518, 14848, -14848, 765, -14847, -15516, -99, -15419, -101, -15509, -103, -108, -15501, -15498, -112, -15495, -15351, 15606, -19546, 19584, -116, -15490, -119, -15356, -15413, -123, -15354, -15349, -131, -136, -19570, -3, -6, 15604, 13, -19586, 19830, -22249, 22016, 10, -22026, -15365, 15601, 306, -15410, -30, -32, 15599, -15407, -39, -15279, -1087, -14511, -48, -15357, -15350, -58, -15346, 15596, -15343, -61, -15342, -15341, -15340, -15339, -64, -65, -15529, 24, -15528, 22, 15594, -15526, -15524, -15523, -15522, -15520, 15592, -15519, -74, -15515, 15591, -15232, 360, -15231, -15512, 83, -2202, -2246, -2262, -18014, 4, -25, -18026, -19699, 19712, -18243, -18068, -2229, -2260, -18047, -2251, -2222, -2227, -2231, -2238, -2263, -2219, -18038, -22, -18027, -3871, 21888, -21879, -15372, -18013, -2268, -18037, -18055, -2249, -2259, -18025, -19418, 19392, 32, -2235, -2255, -19391, -18029, -18053, -2230, -2241, -18033, -15759, -18063, -21896, -2244, -2217, -18036, -2250, -2220, -2237, -2264, -18040, 3, 15794, -15368, -19490, 19460, -2218, -2261, 2, -18028, 15796, -18012, -2248, -2242, -2247, -18054, -18031, -18070, -2254, -18034, -2236, 2651, -18305, 18304, -18051, -15383, -2213, -2257, -15757, 5, -18044, -2223, -2252, -2228, -2253, -2224, -18011, -18066, -18015, -18061, -15380, -18306, -18284, -18042, -18067, -18032, -18048, -19648, 19619, -19433, 646, 18816, -19462, 19456, -18787, -15389, 3423, -19701, 19899, -6723, 22084, -22084, 22113, -18065, -22095, -22116, 22092, -22090, 22087, -22086, -22087, 22090, -22091, 22085, -22085, 22086, 22100, -22100, 22144, -22144, -22097, -10, 22097, 22094, -22094, -22092, 22105, 17, -15377, 145, -15505, 28, 18259, -194, 15502, 88, 38, 139, -18009, 2649, -18019, -18023, -15379, -163, -18039, -18043, -15362, 2689, 18279, 33, 15394, -34, -18056, 2696, -2699, 2700, 18288, 2706, 2710, 288, -18073, 18297, -18076, 2716, -18080, 2720, -2724, 18307, -22168, 22391, -2289, 2512, -22401, -600, 2827, -980, 980, -982, 20928, -22173, 1246, -22174, 6808, -18085, 2725, -20924, 21147, -18087, 18310, -2729, 2729, -21150, -2731, 2731, -20170, -22172, 6814, -18093, 18315, -18095, 2735, -2737, 18318, -2739, 2739, -2741, 2741, -2742, 2742, -2744, 2744, -4415, 19995, -2746, 18326, -15260, -4429, -18328, 18112, -195, -196, -162, -832, 768, -995, -1930, -173, -1984, -1925, 1920, -18096, -222, -18112, -20200, 19968, -21982, 792, 1416, -19774, 1672, 2746, -2784, 15832, 2304, -19770, 4429, 13048, -15352, -4183, 4183, 12996, 2368, -2316, -18530, 16200, -15777, -18133, -3060, 21248, -18620, -2628, -19830, -1418, 3060, 15772, -2763, -2327, -17450, 8, -18892, 18880, -19772, 892, -15344, -19502, 4142, 13034, -13034, -4431, -15345, -19793, 4433, -19794, -4435, 19840, 177, -4436, 20017, -4425, 20007, -4196, 19549, -15353, 128, 15552, -15552, 15551, 15550, -15550, 15549, -15549, 187, 186, -15546, 15347, -15347, 15545, -15545, 15543, -15543, 15542, 15541, 15539, 15538, -15538, 15537, -15537, 152, 175, -15534, 173, 15532, -15532, 15531, -15531, 15529, 15528, 15527, 165, 15524, 15523, 15521, 15520, 15519, 15518, 15516, 15515, 15514, -15514, 15512, 151, 15510, -15510, 15508, 15507, 146, 15504, -15381, -15375, 277, -19200, 19199, 3627, 129, -15295, 15547, -15355, 15546, 183, -3648, 19218, 3648, 181, -15541, 15540, -15540, 12928, -12928, 15536, -15536, 15535, 15533, 171, 15526, -15334, 164, 15522, 15509, -15508, 15506, -15506, 15505, 142, 271, 15500, 138, 15497, 15496, -19264, 19264, 19265, 15495, 15493, 132, -15374, -15371, -15370, -14912, 169, -15337, -20117, 1856, -2836, 21120, -15328, 15525, -5796, -2470, 2496, -20122, -2838, -22365, -22399, 6, -22406, -22352, -22402, -22404, 15517, 43, 15513, -384, 384, 15511, 49, -19968, 4408, 390, -15167, 15503, -15503, 57, -4416, 4416, -15500, -15497, 62, -2368, -13190, -13189, 15494, -15494, -197, 15491, 20229, 15490, -13183, 15485, 71, 15484, -1216, 1216, 15482, 1143, 76, 1220, 15300, -964, -14335, 15478, 78, 18561, 15475, 15473, 18562, 15472, 15469, 703, -14851, 15467, 87, -14849, 15466]\n","要删除的delta次数： [33074, 32, 16200, 28, 9467, 161, 163, 354, 519, 162, 163, 198, 82, 263, 146, 147, 162, 166, 149, 162, 135, 193, 196, 6, 7, 133, 148, 24, 16, 135, 154, 171, 161, 160, 19, 185, 173, 40, 132, 148, 193, 61, 9, 156, 166, 318, 159, 7, 200, 155, 162, 68, 166, 153, 194, 150, 338, 169, 167, 227, 167, 161, 148, 145, 173, 162, 170, 8, 161, 164, 16, 146, 127, 10, 23, 148, 147, 18, 16, 956, 159, 154, 14, 64, 46, 30, 39, 170, 168, 277, 243, 112, 6646, 7, 15783, 13, 139, 157, 451, 167, 168, 71, 131, 105, 161, 21, 165, 161, 166, 135, 10, 173, 15, 160, 165, 161, 164, 171, 147, 155, 168, 150, 193, 169, 137, 223, 169, 143, 250, 147, 250, 168, 460, 12, 160, 588, 174, 22, 168, 384, 142, 207, 8, 189, 348, 85, 309, 129, 83, 29, 27, 143, 329, 7, 337, 30, 162, 336, 12, 300, 138, 12, 342, 140, 17, 155, 37, 334, 149, 129, 320, 112, 239, 37, 192, 345, 135, 347, 153, 123, 8, 148, 301, 11, 36, 287, 12, 286, 163, 26, 27, 28, 360, 146, 18, 355, 19, 160, 355, 24, 15, 356, 22, 167, 374, 8, 58, 23, 22, 418, 161, 316, 146, 12, 7, 248, 11, 9, 24, 193, 159, 150, 170, 19, 186, 25, 159, 168, 160, 9, 193, 170, 168, 160, 21, 8, 275, 224, 149, 150, 147, 160, 166, 89, 100, 31, 162, 19, 131, 84, 14, 32, 43, 288, 280, 15, 7, 6, 7, 45, 83, 70, 93, 81, 17, 89, 81, 101, 89, 87, 84, 101, 100, 89, 86, 83, 81, 100, 101, 91, 85, 84, 81, 95, 99, 87, 70, 14395, 9238, 161, 21, 80, 18, 92, 12, 288, 436, 238, 179, 168, 171, 170, 179, 6, 175, 21, 280, 225, 13, 209, 17, 13, 37, 35, 170, 31, 20, 162, 15, 7, 153, 6, 174, 123, 110, 10, 8, 8, 7, 7, 8, 6, 10, 6, 6, 6, 96, 16, 39, 154, 123, 8, 7, 199, 150, 118, 198, 33, 154, 14, 13, 139, 148, 15, 92, 258, 307, 53, 221, 73, 54, 206, 170, 125, 7, 22, 22, 33, 30, 32, 31, 32, 26, 14, 30, 45, 31, 31, 19, 39, 11, 11, 30, 22, 28, 24, 14, 29, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 121, 10, 143, 78, 146, 153, 140, 15, 118, 69, 73, 71, 67, 104, 89, 95, 82, 74, 70, 60, 76, 87, 73, 96, 137, 126, 117, 137, 126, 134, 95, 14, 11, 8, 63, 84, 7, 7, 84, 16, 113, 101, 108, 105, 130, 146, 142, 66, 143, 43, 7, 9, 11, 15, 39, 10, 8, 59, 43, 15, 8, 41, 24, 13, 14, 14, 14, 35, 23, 9, 31]\n","要删除的delta种类： 826\n","新类的值:  118\n","1304\n","不保留的类都归为值： 118\n","{0, 15360, -15360, 1, -15359, -15338, 34, 35, -15324, 37, -15323, -15321, -15319, -15318, -15314, -15313, -15309, -15308, -15306, -15304, -15302, -15300, -15298, -15297, -15296, 118, 15488, 18560, 191, 192, 15553, 15554, 15555, 193, 196, 195, 197, 200, 201, 199, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, -256, 256, 257, -192, 1344, 15232, -18560, -15474, -15467, 15296, 198, -1, -15361}\n","479\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDlKfU0_rMjg","executionInfo":{"status":"ok","timestamp":1623222956366,"user_tz":-480,"elapsed":17,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"74586932-f611-48cf-c2a9-bf9d51d9949e"},"source":["trainl=test_Delta.tolist()\n","setl=set(trainl)\n","typel=len(setl)\n","print(\"Train_Set种类：\",typel)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train_Set种类： 104\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYbl776YBhY7","executionInfo":{"status":"ok","timestamp":1623222956366,"user_tz":-480,"elapsed":12,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"bd855e93-30aa-49e4-e593-dead4237f61c"},"source":["#编码转换\n","from numpy import array\n","from numpy import argmax\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n","\n","#查看train_Delta和test_Delta种类：\n","listD_train=train_Delta.tolist()\n","listD_test=test_Delta.tolist()\n","set_d_train=set(listD_train)\n","set_d_test=set(listD_test)\n","print(\"查看train_Delta种类：\",len(set_d_train))\n","print(\"查看test_Delta种类：\",len(set_d_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["查看train_Delta种类： 479\n","查看test_Delta种类： 104\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eocxAHlCoj8Y","executionInfo":{"status":"ok","timestamp":1623222956367,"user_tz":-480,"elapsed":9,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"97a1fa70-5c4d-48dc-9e3c-1cf1978b08a9"},"source":["train_Delta[500:590]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-15446,  15702,   -256,    256,   -256, -15446,      0,      0,\n","        15702,   -256, -15446,  15702,   -256,    256,   -256, -15445,\n","            0,      0,  15701,   -256, -15445,  15701,   -256,    256,\n","         -256, -15445,      0,      0,  15702,   -256, -15446,  15702,\n","         -256, -15446,      0,  15702,   -256, -15446,  15702,   -256,\n","       -15446,      0,  15702,   -256, -15446,  15702,   -256, -15446,\n","            0,      0,  15702,   -256,    256,   -256,    256,   -256,\n","          118,    118,  15702,   -256, -15446,  15702,   -256, -15446,\n","        15702,   -256, -15446,      0,  15702,   -256, -15446,  15702,\n","         -256, -15446,      0,  15702,   -256, -15446,  15702,   -256,\n","       -15446,      0,  15702,   -256, -15446,  15702,   -256, -15446,\n","        15702,   -256])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quqBWxbx4hXZ","executionInfo":{"status":"ok","timestamp":1623222957864,"user_tz":-480,"elapsed":1503,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"eb3bc162-7238-4a1b-f25c-1a58abd5245e"},"source":["#编码的时候先合并train和test的数据，相当于用同样的方法transform\n","whole_delta=np.concatenate((train_Delta,test_Delta))\n","#integer encoding方法（LabelEncoder）\n","values=whole_delta\n","label_encoder=LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(values)\n","print(\"训练集推出的Integer_encoder：\",integer_encoded)\n","\n","#找种类的最大值\n","max_integer_encoded=max(integer_encoded)+1\n","print(\"最大的integer_encoded_num：\",max_integer_encoded)\n","max_integer=type_new-1\n","num_bit=int(math.log(max_integer, 2)+1)\n","print(\"需要转的二进制：\",max_integer)\n","print(\"应该用多少bit表示：\",num_bit)\n","\n","# define convert_to_binary method\n","# 每个值转特定位数的binary编码\n","def convert_to_binary(origin, num_bit=1):\n","  binary_encoded = []\n","  num_bit_str=str(num_bit)\n","  for element in origin:\n","    binary_encoded.append(list(map(int,list(('{:0'+num_bit_str+'b}').format(element)))))\n","  return np.array(binary_encoded)\n","\n","binary_encoded = convert_to_binary(integer_encoded, num_bit)\n","print(\"Binary_encoded：\",binary_encoded)\n","\n","# define convert_to_onehot method\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","print(\"Onehot_encoded：\",onehot_encoded)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["训练集推出的Integer_encoder： [268 472 274 ... 274 226 274]\n","最大的integer_encoded_num： 479\n","需要转的二进制： 478\n","应该用多少bit表示： 9\n","Binary_encoded： [[1 0 0 ... 1 0 0]\n"," [1 1 1 ... 0 0 0]\n"," [1 0 0 ... 0 1 0]\n"," ...\n"," [1 0 0 ... 0 1 0]\n"," [0 1 1 ... 0 1 0]\n"," [1 0 0 ... 0 1 0]]\n","Onehot_encoded： [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMujBqqHW5bZ","executionInfo":{"status":"ok","timestamp":1623222957864,"user_tz":-480,"elapsed":9,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"5e1c143a-3845-481c-87c0-29e87c6f36cf"},"source":["\n","print(type(integer_encoded))\n","print(\"整数编码shape：\",integer_encoded.shape)\n","print(type(onehot_encoded))\n","print(\"独热编码shape：\",onehot_encoded.shape)\n","print(type(binary_encoded))\n","print(\"二进制编码：\",binary_encoded.shape)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","整数编码shape： (200000, 1)\n","<class 'numpy.ndarray'>\n","独热编码shape： (200000, 479)\n","<class 'numpy.ndarray'>\n","二进制编码： (200000, 9)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpfO6fCUOQKv","executionInfo":{"status":"ok","timestamp":1623222957865,"user_tz":-480,"elapsed":7,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"1b5f3945-27e9-4440-f3a3-6cc0a23c1902"},"source":["#读写数据\n","RW = datasetRW[:]\n","RW = np.reshape(RW, (RW.shape[0],1))\n","RW.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200000, 1)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"WdRNDnIBOMjV"},"source":["#创建数据集的输入和输出\n","def create_dataset(datasetX, datasetY, look_back=1, pridict_size=1):\n","\tdataX, dataY = [], []\n","\tfor i in range(len(datasetX)-look_back-pridict_size):\n","\t\ta = datasetX[i:(i+look_back), :]\n","\t\tdataX.append(a)\n","\t\tdataY.append(datasetY[i + look_back:(i+look_back+pridict_size), :])\n","\treturn np.array(dataX), np.array(dataY)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_Hseg2COYOZ","executionInfo":{"status":"ok","timestamp":1623222958725,"user_tz":-480,"elapsed":864,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"28d9317d-bb1c-4970-a15b-cafe1e8c793e"},"source":["#确定lookback的size\n","look_back = 80\n","pridict_size = 2\n","type_new\n","\n","#按lookback的size创建训练集输入和输出,trainX的维度((train_size-look_back-pridict_size),look_back)\n","#训练集的数据\n","integer_encoded_train=integer_encoded[:train_size]\n","binary_encoded_train=binary_encoded[:train_size]\n","integer_encoded_test=integer_encoded[train_size:]\n","binary_encoded_test=binary_encoded[train_size:]\n","#训练集的读写数据\n","RW_train=RW[:train_size]\n","RW_test=RW[train_size:]\n","\n","#valuesX, valuesY = create_dataset(integer_encoded, onehot_encoded, look_back, pridict_size)    #onehot_encoded\n","trainX, trainY = create_dataset(integer_encoded_train, binary_encoded_train, look_back, pridict_size)    #binary_encoded\n","testX, testY = create_dataset(integer_encoded_test, binary_encoded_test, look_back, pridict_size)\n","\n","trainX_RW, trainY_RW = create_dataset(RW_train, RW_train, look_back, pridict_size) \n","testX_RW, testY_RW = create_dataset(RW_test, RW_test, look_back, pridict_size)\n","\n","\n","print('TrainX Shape: ',trainX.shape)\n","print('TrainY Shape: ',trainY.shape)\n","print('TestX Shape: ',testX.shape)\n","print('TestY Shape: ',testY.shape)\n","\n","print('TrainX_RW Shape: ',trainX_RW.shape)\n","print('TrainY_RW Shape: ',trainY_RW.shape)\n","print('TestX_RW Shape: ',testX_RW.shape)\n","print('TestY_RW Shape: ',testY_RW.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TrainX Shape:  (159948, 50, 1)\n","TrainY Shape:  (159948, 2, 9)\n","TestX Shape:  (39948, 50, 1)\n","TestY Shape:  (39948, 2, 9)\n","TrainX_RW Shape:  (159948, 50, 1)\n","TrainY_RW Shape:  (159948, 2, 1)\n","TestX_RW Shape:  (39948, 50, 1)\n","TestY_RW Shape:  (39948, 2, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xL-kObVNO30F","executionInfo":{"status":"ok","timestamp":1623222958726,"user_tz":-480,"elapsed":6,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"ca8f848e-e916-465e-a44b-a9ddd448f9ae"},"source":["# reshape input to be [samples, time steps, features]\n","trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[2]*trainX.shape[1]))\n","print(\"feature:\",trainX.shape)\n","testX = np.reshape(testX, (testX.shape[0], testX.shape[2]*testX.shape[1]))\n","print('TrainX Shape: ',trainX.shape)\n","print('TrainY Shape: ',trainY.shape)\n","\n","print('trainX type: ',type(trainX))\n","\n","trainX_RW=np.reshape(trainX_RW, (trainX_RW.shape[0], trainX_RW.shape[2]*trainX_RW.shape[1]))\n","testX_RW = np.reshape(testX_RW, (testX_RW.shape[0], testX_RW.shape[2]*testX_RW.shape[1]))\n","print('trainX_RW Shape: ',trainX_RW.shape)\n","print('trainY_RW Shape: ',trainY_RW.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["feature: (159948, 50)\n","TrainX Shape:  (159948, 50)\n","TrainY Shape:  (159948, 2, 9)\n","trainX type:  <class 'numpy.ndarray'>\n","trainX_RW Shape:  (159948, 50)\n","trainY_RW Shape:  (159948, 2, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DLswuLF1oeT6"},"source":["from keras import backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlXRb3xl9f4C"},"source":["\n","import inspect\n","from typing import List\n","\n","from tensorflow.keras import backend as K, Model, Input, optimizers\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda\n","from tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization\n","\n","\n","def is_power_of_two(num: int):\n","    return num != 0 and ((num & (num - 1)) == 0)\n","\n","\n","def adjust_dilations(dilations: list):\n","    if all([is_power_of_two(i) for i in dilations]):\n","        return dilations\n","    else:\n","        new_dilations = [2 ** i for i in dilations]\n","        return new_dilations\n","\n","\n","class ResidualBlock(Layer):\n","\n","    def __init__(self,\n","                 dilation_rate: int,\n","                 nb_filters: int,\n","                 kernel_size: int,\n","                 padding: str,\n","                 activation: str = 'relu',\n","                 dropout_rate: float = 0,\n","                 kernel_initializer: str = 'he_normal',\n","                 use_batch_norm: bool = False,\n","                 use_layer_norm: bool = False,\n","                 use_weight_norm: bool = False,\n","                 **kwargs):\n","        \"\"\"Defines the residual block for the WaveNet TCN\n","        Args:\n","            x: The previous layer in the model\n","            training: boolean indicating whether the layer should behave in training mode or in inference mode\n","            dilation_rate: The dilation power of 2 we are using for this residual block\n","            nb_filters: The number of convolutional filters to use in this block\n","            kernel_size: The size of the convolutional kernel\n","            padding: The padding used in the convolutional layers, 'same' or 'causal'.\n","            activation: The final activation used in o = Activation(x + F(x))\n","            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n","            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n","            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n","            use_weight_norm: Whether to use weight normalization in the residual layers or not.\n","            kwargs: Any initializers for Layer class.\n","        \"\"\"\n","\n","        self.dilation_rate = dilation_rate\n","        self.nb_filters = nb_filters\n","        self.kernel_size = kernel_size\n","        self.padding = padding\n","        self.activation = activation\n","        self.dropout_rate = dropout_rate\n","        self.use_batch_norm = use_batch_norm\n","        self.use_layer_norm = use_layer_norm\n","        self.use_weight_norm = use_weight_norm\n","        self.kernel_initializer = kernel_initializer\n","        self.layers = []\n","        self.layers_outputs = []\n","        self.shape_match_conv = None\n","        self.res_output_shape = None\n","        self.final_activation = None\n","\n","        super(ResidualBlock, self).__init__(**kwargs)\n","\n","    def _build_layer(self, layer):\n","        \"\"\"Helper function for building layer\n","        Args:\n","            layer: Appends layer to internal layer list and builds it based on the current output\n","                   shape of ResidualBlocK. Updates current output shape.\n","        \"\"\"\n","        self.layers.append(layer)\n","        self.layers[-1].build(self.res_output_shape)\n","        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n","\n","    def build(self, input_shape):\n","\n","        with K.name_scope(self.name):  # name scope used to make sure weights get unique names\n","            self.layers = []\n","            self.res_output_shape = input_shape\n","\n","            for k in range(2):\n","                name = 'conv1D_{}'.format(k)\n","                with K.name_scope(name):  # name scope used to make sure weights get unique names\n","                    conv = Conv1D(\n","                        filters=self.nb_filters,\n","                        kernel_size=self.kernel_size,\n","                        dilation_rate=self.dilation_rate,\n","                        padding=self.padding,\n","                        name=name,\n","                        kernel_initializer=self.kernel_initializer\n","                    )\n","                    if self.use_weight_norm:\n","                        from tensorflow_addons.layers import WeightNormalization\n","                        # wrap it. WeightNormalization API is different than BatchNormalization or LayerNormalization.\n","                        with K.name_scope('norm_{}'.format(k)):\n","                            conv = WeightNormalization(conv)\n","                    self._build_layer(conv)\n","\n","                with K.name_scope('norm_{}'.format(k)):\n","                    if self.use_batch_norm:\n","                        self._build_layer(BatchNormalization())\n","                    elif self.use_layer_norm:\n","                        self._build_layer(LayerNormalization())\n","                    elif self.use_weight_norm:\n","                        pass  # done above.\n","\n","                self._build_layer(Activation(self.activation))\n","                self._build_layer(SpatialDropout1D(rate=self.dropout_rate))\n","\n","            if self.nb_filters != input_shape[-1]:\n","                # 1x1 conv to match the shapes (channel dimension).\n","                name = 'matching_conv1D'\n","                with K.name_scope(name):\n","                    # make and build this layer separately because it directly uses input_shape\n","                    self.shape_match_conv = Conv1D(filters=self.nb_filters,\n","                                                   kernel_size=1,\n","                                                   padding='same',\n","                                                   name=name,\n","                                                   kernel_initializer=self.kernel_initializer)\n","            else:\n","                name = 'matching_identity'\n","                self.shape_match_conv = Lambda(lambda x: x, name=name)\n","\n","            with K.name_scope(name):\n","                self.shape_match_conv.build(input_shape)\n","                self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n","\n","            self._build_layer(Activation(self.activation))\n","            self.final_activation = Activation(self.activation)\n","            self.final_activation.build(self.res_output_shape)  # probably isn't necessary\n","\n","            # this is done to force Keras to add the layers in the list to self._layers\n","            for layer in self.layers:\n","                self.__setattr__(layer.name, layer)\n","            self.__setattr__(self.shape_match_conv.name, self.shape_match_conv)\n","            self.__setattr__(self.final_activation.name, self.final_activation)\n","\n","            super(ResidualBlock, self).build(input_shape)  # done to make sure self.built is set True\n","\n","    def call(self, inputs, training=None):\n","        \"\"\"\n","        Returns: A tuple where the first element is the residual model tensor, and the second\n","                 is the skip connection tensor.\n","        \"\"\"\n","        x = inputs\n","        self.layers_outputs = [x]\n","        for layer in self.layers:\n","            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n","            x = layer(x, training=training) if training_flag else layer(x)\n","            self.layers_outputs.append(x)\n","        x2 = self.shape_match_conv(inputs)\n","        self.layers_outputs.append(x2)\n","        res_x = layers.add([x2, x])\n","        self.layers_outputs.append(res_x)\n","\n","        res_act_x = self.final_activation(res_x)\n","        self.layers_outputs.append(res_act_x)\n","        return [res_act_x, x]\n","\n","    def compute_output_shape(self, input_shape):\n","        return [self.res_output_shape, self.res_output_shape]\n","\n","\n","class TCN(Layer):\n","    \"\"\"Creates a TCN layer.\n","        Input shape:\n","            A tensor of shape (batch_size, timesteps, input_dim).\n","        Args:\n","            nb_filters: The number of filters to use in the convolutional layers. Can be a list.\n","            kernel_size: The size of the kernel to use in each convolutional layer.\n","            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n","            nb_stacks : The number of stacks of residual blocks to use.\n","            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n","            use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n","            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n","            activation: The activation used in the residual blocks o = Activation(x + F(x)).\n","            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n","            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n","            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n","            use_weight_norm: Whether to use weight normalization in the residual layers or not.\n","            kwargs: Any other arguments for configuring parent class Layer. For example \"name=str\", Name of the model.\n","                    Use unique names when using multiple TCN.\n","        Returns:\n","            A TCN layer.\n","        \"\"\"\n","\n","    def __init__(self,\n","                 nb_filters=64,\n","                 kernel_size=2,\n","                 nb_stacks=1,\n","                 dilations=(1, 2, 4, 8, 16, 32),\n","                 padding='causal',\n","                 use_skip_connections=False,\n","                 dropout_rate=0.0,\n","                 return_sequences=False,\n","                 activation='relu',\n","                 kernel_initializer='he_normal',\n","                 use_batch_norm=False,\n","                 use_layer_norm=False,\n","                 use_weight_norm=False,\n","                 **kwargs):\n","\n","        self.return_sequences = return_sequences\n","        self.dropout_rate = dropout_rate\n","        self.use_skip_connections = use_skip_connections\n","        self.dilations = dilations\n","        self.nb_stacks = nb_stacks\n","        self.kernel_size = kernel_size\n","        self.nb_filters = nb_filters\n","        self.activation = activation\n","        self.padding = padding\n","        self.kernel_initializer = kernel_initializer\n","        self.use_batch_norm = use_batch_norm\n","        self.use_layer_norm = use_layer_norm\n","        self.use_weight_norm = use_weight_norm\n","        self.skip_connections = []\n","        self.residual_blocks = []\n","        self.layers_outputs = []\n","        self.build_output_shape = None\n","        self.slicer_layer = None  # in case return_sequence=False\n","        self.output_slice_index = None  # in case return_sequence=False\n","        self.padding_same_and_time_dim_unknown = False  # edge case if padding='same' and time_dim = None\n","\n","        if self.use_batch_norm + self.use_layer_norm + self.use_weight_norm > 1:\n","            raise ValueError('Only one normalization can be specified at once.')\n","\n","        if isinstance(self.nb_filters, list):\n","            assert len(self.nb_filters) == len(self.dilations)\n","\n","        if padding != 'causal' and padding != 'same':\n","            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n","\n","        # initialize parent class\n","        super(TCN, self).__init__(**kwargs)\n","\n","    @property\n","    def receptive_field(self):\n","        return 1 + self.nb_stacks * sum([d * self.kernel_size for d in self.dilations])\n","\n","    def build(self, input_shape):\n","\n","        # member to hold current output shape of the layer for building purposes\n","        self.build_output_shape = input_shape\n","\n","        # list to hold all the member ResidualBlocks\n","        self.residual_blocks = []\n","        total_num_blocks = self.nb_stacks * len(self.dilations)\n","        if not self.use_skip_connections:\n","            total_num_blocks += 1  # cheap way to do a false case for below\n","\n","        for s in range(self.nb_stacks):\n","            for i, d in enumerate(self.dilations):\n","                res_block_filters = self.nb_filters[i] if isinstance(self.nb_filters, list) else self.nb_filters\n","                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n","                                                          nb_filters=res_block_filters,\n","                                                          kernel_size=self.kernel_size,\n","                                                          padding=self.padding,\n","                                                          activation=self.activation,\n","                                                          dropout_rate=self.dropout_rate,\n","                                                          use_batch_norm=self.use_batch_norm,\n","                                                          use_layer_norm=self.use_layer_norm,\n","                                                          use_weight_norm=self.use_weight_norm,\n","                                                          kernel_initializer=self.kernel_initializer,\n","                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n","                # build newest residual block\n","                self.residual_blocks[-1].build(self.build_output_shape)\n","                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n","\n","        # this is done to force keras to add the layers in the list to self._layers\n","        for layer in self.residual_blocks:\n","            self.__setattr__(layer.name, layer)\n","\n","        self.output_slice_index = None\n","        if self.padding == 'same':\n","            time = self.build_output_shape.as_list()[1]\n","            if time is not None:  # if time dimension is defined. e.g. shape = (bs, 500, input_dim).\n","                self.output_slice_index = int(self.build_output_shape.as_list()[1] / 2)\n","            else:\n","                # It will known at call time. c.f. self.call.\n","                self.padding_same_and_time_dim_unknown = True\n","\n","        else:\n","            self.output_slice_index = -1  # causal case.\n","        self.slicer_layer = Lambda(lambda tt: tt[:, self.output_slice_index, :])\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\"\n","        Overridden in case keras uses it somewhere... no idea. Just trying to avoid future errors.\n","        \"\"\"\n","        if not self.built:\n","            self.build(input_shape)\n","        if not self.return_sequences:\n","            batch_size = self.build_output_shape[0]\n","            batch_size = batch_size.value if hasattr(batch_size, 'value') else batch_size\n","            nb_filters = self.build_output_shape[-1]\n","            return [batch_size, nb_filters]\n","        else:\n","            # Compatibility tensorflow 1.x\n","            return [v.value if hasattr(v, 'value') else v for v in self.build_output_shape]\n","\n","    def call(self, inputs, training=None):\n","        x = inputs\n","        self.layers_outputs = [x]\n","        self.skip_connections = []\n","        for layer in self.residual_blocks:\n","            try:\n","                x, skip_out = layer(x, training=training)\n","            except TypeError:  # compatibility with tensorflow 1.x\n","                x, skip_out = layer(K.cast(x, 'float32'), training=training)\n","            self.skip_connections.append(skip_out)\n","            self.layers_outputs.append(x)\n","\n","        if self.use_skip_connections:\n","            x = layers.add(self.skip_connections)\n","            self.layers_outputs.append(x)\n","\n","        if not self.return_sequences:\n","            # case: time dimension is unknown. e.g. (bs, None, input_dim).\n","            if self.padding_same_and_time_dim_unknown:\n","                self.output_slice_index = K.shape(self.layers_outputs[-1])[1] // 2\n","            x = self.slicer_layer(x)\n","            self.layers_outputs.append(x)\n","        return x\n","\n","    def get_config(self):\n","        \"\"\"\n","        Returns the config of a the layer. This is used for saving and loading from a model\n","        :return: python dictionary with specs to rebuild layer\n","        \"\"\"\n","        config = super(TCN, self).get_config()\n","        config['nb_filters'] = self.nb_filters\n","        config['kernel_size'] = self.kernel_size\n","        config['nb_stacks'] = self.nb_stacks\n","        config['dilations'] = self.dilations\n","        config['padding'] = self.padding\n","        config['use_skip_connections'] = self.use_skip_connections\n","        config['dropout_rate'] = self.dropout_rate\n","        config['return_sequences'] = self.return_sequences\n","        config['activation'] = self.activation\n","        config['use_batch_norm'] = self.use_batch_norm\n","        config['use_layer_norm'] = self.use_layer_norm\n","        config['use_weight_norm'] = self.use_weight_norm\n","        config['kernel_initializer'] = self.kernel_initializer\n","        return config\n","\n","\n","def compiled_tcn(num_feat,  # type: int\n","                 num_classes,  # type: int\n","                 nb_filters,  # type: int\n","                 kernel_size,  # type: int\n","                 dilations,  # type: List[int]\n","                 nb_stacks,  # type: int\n","                 max_len,  # type: int\n","                 output_len=1,  # type: int\n","                 padding='causal',  # type: str\n","                 use_skip_connections=False,  # type: bool\n","                 return_sequences=True,\n","                 regression=False,  # type: bool\n","                 dropout_rate=0.05,  # type: float\n","                 name='tcn',  # type: str,\n","                 kernel_initializer='he_normal',  # type: str,\n","                 activation='relu',  # type:str,\n","                 opt='adam',\n","                 lr=0.002,\n","                 use_batch_norm=False,\n","                 use_layer_norm=False,\n","                 use_weight_norm=False):\n","    # type: (...) -> Model\n","    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n","    Classification uses a sparse categorical loss. Please input class ids and not one-hot encodings.\n","    Args:\n","        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n","        num_classes: The size of the final dense layer, how many classes we are predicting.\n","        nb_filters: The number of filters to use in the convolutional layers.\n","        kernel_size: The size of the kernel to use in each convolutional layer.\n","        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n","        nb_stacks : The number of stacks of residual blocks to use.\n","        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n","        padding: The padding to use in the convolutional layers.\n","        use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n","        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n","        regression: Whether the output should be continuous or discrete.\n","        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","        activation: The activation used in the residual blocks o = Activation(x + F(x)).\n","        name: Name of the model. Useful when having multiple TCN.\n","        kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n","        opt: Optimizer name.\n","        lr: Learning rate.\n","        use_batch_norm: Whether to use batch normalization in the residual layers or not.\n","        use_layer_norm: Whether to use layer normalization in the residual layers or not.\n","        use_weight_norm: Whether to use weight normalization in the residual layers or not.\n","    Returns:\n","        A compiled keras TCN.\n","    \"\"\"\n","\n","    dilations = adjust_dilations(dilations)\n","\n","    input_layer = Input(shape=(max_len, num_feat))\n","\n","    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, padding,\n","            use_skip_connections, dropout_rate, return_sequences,\n","            activation, kernel_initializer, use_batch_norm, use_layer_norm,\n","            use_weight_norm, name=name)(input_layer)\n","\n","    print('x.shape=', x.shape)\n","\n","    def get_opt():\n","        if opt == 'adam':\n","            return optimizers.Adam(lr=lr, clipnorm=1.)\n","        elif opt == 'rmsprop':\n","            return optimizers.RMSprop(lr=lr, clipnorm=1.)\n","        else:\n","            raise Exception('Only Adam and RMSProp are available here')\n","\n","    if not regression:\n","        # classification\n","        x = Dense(num_classes)(x)\n","        x = Activation('softmax')(x)\n","        output_layer = x\n","        model = Model(input_layer, output_layer)\n","\n","        # https://github.com/keras-team/keras/pull/11373\n","        # It's now in Keras@master but still not available with pip.\n","        # TODO remove later.\n","        def accuracy(y_true, y_pred):\n","            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n","            if K.ndim(y_true) == K.ndim(y_pred):\n","                y_true = K.squeeze(y_true, -1)\n","            # convert dense predictions to labels\n","            y_pred_labels = K.argmax(y_pred, axis=-1)\n","            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n","            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n","\n","        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=[accuracy])\n","    else:\n","        # regression\n","        x = Dense(output_len)(x)\n","        x = Activation('linear')(x)\n","        output_layer = x\n","        model = Model(input_layer, output_layer)\n","        model.compile(get_opt(), loss='mean_squared_error')\n","    print('model.x = {}'.format(input_layer.shape))\n","    print('model.y = {}'.format(output_layer.shape))\n","    return model\n","\n","\n","def tcn_full_summary(model: Model, expand_residual_blocks=True):\n","    layers = model._layers.copy()  # store existing layers\n","    model._layers.clear()  # clear layers\n","\n","    for i in range(len(layers)):\n","        if isinstance(layers[i], TCN):\n","            for layer in layers[i]._layers:\n","                if not isinstance(layer, ResidualBlock):\n","                    if not hasattr(layer, '__iter__'):\n","                        model._layers.append(layer)\n","                else:\n","                    if expand_residual_blocks:\n","                        for lyr in layer._layers:\n","                            if not hasattr(lyr, '__iter__'):\n","                                model._layers.append(lyr)\n","                    else:\n","                        model._layers.append(layer)\n","        else:\n","            model._layers.append(layers[i])\n","\n","    model.summary()  # print summary\n","\n","    # restore original layers\n","    model._layers.clear()\n","    [model._layers.append(lyr) for lyr in layers]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYqQyzc8cwgF","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1623222960532,"user_tz":-480,"elapsed":7,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"bf06fb68-394f-4ac5-ea86-2e5089f03752"},"source":["'''\n","from keras.utils import to_categorical\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Multiply, Lambda\n","from keras.layers import LSTM, GRU, Embedding, Flatten, Reshape, Layer, merge, Average, Lambda, Conv2D, MaxPool2D, GlobalAveragePooling2D, Conv1D, MaxPooling1D, concatenate, Input, Bidirectional, RepeatVector, Concatenate, Dot, Permute, TimeDistributed, SpatialDropout1D\n","'''\n","#from keras_self_attention import SeqSelfAttention\n","#from tcn import TCN, tcn_full_summary\n","#from attention import Attention"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfrom keras.utils import to_categorical\\nfrom keras.models import Sequential, Model\\nfrom keras.layers import Dense, Multiply, Lambda\\nfrom keras.layers import LSTM, GRU, Embedding, Flatten, Reshape, Layer, merge, Average, Lambda, Conv2D, MaxPool2D, GlobalAveragePooling2D, Conv1D, MaxPooling1D, concatenate, Input, Bidirectional, RepeatVector, Concatenate, Dot, Permute, TimeDistributed, SpatialDropout1D\\n'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"fHHpncfNBYPV"},"source":["#tri-positional encoding\n","from tensorflow.keras import backend as K\n","\n","class TrigPosEmbedding(keras.layers.Layer):\n","    \"\"\"Position embedding use sine and cosine functions.\n","    See: https://arxiv.org/pdf/1706.03762\n","    Expand mode:\n","        # Input shape\n","            2D tensor with shape: `(batch_size, sequence_length)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n","    Add mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","    Concat mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim + output_dim)`.\n","    \"\"\"\n","    MODE_EXPAND = 'expand'\n","    MODE_ADD = 'add'\n","    MODE_CONCAT = 'concat'\n","\n","    def __init__(self,\n","                 mode=MODE_ADD,\n","                 output_dim=None,\n","                 **kwargs):\n","        \"\"\"\n","        :param output_dim: The embedding dimension.\n","        :param kwargs:\n","        \"\"\"\n","        if mode in [self.MODE_EXPAND, self.MODE_CONCAT]:\n","            if output_dim is None:\n","                raise NotImplementedError('`output_dim` is required in `%s` mode' % mode)\n","            if output_dim % 2 != 0:\n","                raise NotImplementedError('It does not make sense to use an odd output dimension: %d' % output_dim)\n","        self.mode = mode\n","        self.output_dim = output_dim\n","        self.supports_masking = True\n","        super(TrigPosEmbedding, self).__init__(**kwargs)\n","\n","    def get_config(self):\n","        config = {\n","            'mode': self.mode,\n","            'output_dim': self.output_dim,\n","        }\n","        base_config = super(TrigPosEmbedding, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.mode == self.MODE_EXPAND:\n","            return input_shape + (self.output_dim,)\n","        if self.mode == self.MODE_CONCAT:\n","            return input_shape[:-1] + (input_shape[-1] + self.output_dim,)\n","        return input_shape\n","\n","    def call(self, inputs, mask=None):\n","        input_shape = K.shape(inputs)\n","        if self.mode == self.MODE_ADD:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n","            pos_input = K.tile(K.expand_dims(K.arange(0, seq_len), axis=0), [batch_size, 1])\n","        elif self.mode == self.MODE_CONCAT:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], self.output_dim\n","            pos_input = K.tile(K.expand_dims(K.arange(0, seq_len), axis=0), [batch_size, 1])\n","        else:\n","            output_dim = self.output_dim\n","            pos_input = inputs\n","        if K.dtype(pos_input) != K.floatx():\n","            pos_input = K.cast(pos_input, K.floatx())\n","        evens = K.arange(0, output_dim // 2) * 2\n","        odds = K.arange(0, output_dim // 2) * 2 + 1\n","        even_embd = K.sin(\n","            K.dot(\n","                K.expand_dims(pos_input, -1),\n","                K.expand_dims(1.0 / K.pow(\n","                    10000.0,\n","                    K.cast(evens, K.floatx()) / K.cast(output_dim, K.floatx())\n","                ), 0)\n","            )\n","        )\n","        odd_embd = K.cos(\n","            K.dot(\n","                K.expand_dims(pos_input, -1),\n","                K.expand_dims(1.0 / K.pow(\n","                    10000.0, K.cast((odds - 1), K.floatx()) / K.cast(output_dim, K.floatx())\n","                ), 0)\n","            )\n","        )\n","        embd = K.stack([even_embd, odd_embd], axis=-1)\n","        output = K.reshape(embd, [-1, K.shape(inputs)[1], output_dim])\n","        if self.mode == self.MODE_CONCAT:\n","            output = K.concatenate([inputs, output], axis=-1)\n","        if self.mode == self.MODE_ADD:\n","            output += inputs\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VO_ZQ-FeHWsV"},"source":["#Trainable Embedding\n","\n","class PositionEmbedding(keras.layers.Layer):\n","    \"\"\"Turn integers (positions) into dense vectors of fixed size.\n","    eg. [[-4], [10]] -> [[0.25, 0.1], [0.6, -0.2]]\n","    Expand mode: negative integers (relative position) could be used in this mode.\n","        # Input shape\n","            2D tensor with shape: `(batch_size, sequence_length)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n","    Add mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","    Concat mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim + output_dim)`.\n","    \"\"\"\n","    MODE_EXPAND = 'expand'\n","    MODE_ADD = 'add'\n","    MODE_CONCAT = 'concat'\n","\n","    def __init__(self,\n","                 input_dim,\n","                 output_dim,\n","                 mode=MODE_EXPAND,\n","                 embeddings_initializer='uniform',\n","                 embeddings_regularizer=None,\n","                 activity_regularizer=None,\n","                 embeddings_constraint=None,\n","                 mask_zero=False,\n","                 **kwargs):\n","        \"\"\"\n","        :param input_dim: The maximum absolute value of positions.\n","        :param output_dim: The embedding dimension.\n","        :param embeddings_initializer:\n","        :param embeddings_regularizer:\n","        :param activity_regularizer:\n","        :param embeddings_constraint:\n","        :param mask_zero: The index that represents padding. Only works in `append` mode.\n","        :param kwargs:\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.mode = mode\n","        self.embeddings_initializer = keras.initializers.get(embeddings_initializer)\n","        self.embeddings_regularizer = keras.regularizers.get(embeddings_regularizer)\n","        self.activity_regularizer = keras.regularizers.get(activity_regularizer)\n","        self.embeddings_constraint = keras.constraints.get(embeddings_constraint)\n","        self.mask_zero = mask_zero\n","        self.supports_masking = mask_zero is not False\n","\n","        self.embeddings = None\n","        super(PositionEmbedding, self).__init__(**kwargs)\n","\n","    def get_config(self):\n","        config = {'input_dim': self.input_dim,\n","                  'output_dim': self.output_dim,\n","                  'mode': self.mode,\n","                  'embeddings_initializer': keras.initializers.serialize(self.embeddings_initializer),\n","                  'embeddings_regularizer': keras.regularizers.serialize(self.embeddings_regularizer),\n","                  'activity_regularizer': keras.regularizers.serialize(self.activity_regularizer),\n","                  'embeddings_constraint': keras.constraints.serialize(self.embeddings_constraint),\n","                  'mask_zero': self.mask_zero}\n","        base_config = super(PositionEmbedding, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def build(self, input_shape):\n","        if self.mode == self.MODE_EXPAND:\n","            self.embeddings = self.add_weight(\n","                shape=(self.input_dim * 2 + 1, self.output_dim),\n","                initializer=self.embeddings_initializer,\n","                name='embeddings',\n","                regularizer=self.embeddings_regularizer,\n","                constraint=self.embeddings_constraint,\n","            )\n","        else:\n","            self.embeddings = self.add_weight(\n","                shape=(self.input_dim, self.output_dim),\n","                initializer=self.embeddings_initializer,\n","                name='embeddings',\n","                regularizer=self.embeddings_regularizer,\n","                constraint=self.embeddings_constraint,\n","            )\n","        super(PositionEmbedding, self).build(input_shape)\n","\n","    def compute_mask(self, inputs, mask=None):\n","        if self.mode == self.MODE_EXPAND:\n","            if self.mask_zero:\n","                output_mask = K.not_equal(inputs, self.mask_zero)\n","            else:\n","                output_mask = None\n","        else:\n","            output_mask = mask\n","        return output_mask\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.mode == self.MODE_EXPAND:\n","            return input_shape + (self.output_dim,)\n","        if self.mode == self.MODE_CONCAT:\n","            return input_shape[:-1] + (input_shape[-1] + self.output_dim,)\n","        return input_shape\n","\n","    def call(self, inputs, **kwargs):\n","        if self.mode == self.MODE_EXPAND:\n","            if K.dtype(inputs) != 'int32':\n","                inputs = K.cast(inputs, 'int32')\n","            return K.gather(\n","                self.embeddings,\n","                K.minimum(K.maximum(inputs, -self.input_dim), self.input_dim) + self.input_dim,\n","            )\n","        input_shape = K.shape(inputs)\n","        if self.mode == self.MODE_ADD:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n","        else:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], self.output_dim\n","        pos_embeddings = K.tile(\n","            K.expand_dims(self.embeddings[:seq_len, :self.output_dim], axis=0),\n","            [batch_size, 1, 1],\n","        )\n","        if self.mode == self.MODE_ADD:\n","            return inputs + pos_embeddings\n","        return K.concatenate([inputs, pos_embeddings], axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Pjzua27aIwb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSF2T5WVSxPK","executionInfo":{"status":"ok","timestamp":1623222964303,"user_tz":-480,"elapsed":3775,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"349396c6-cd53-4831-a9e9-db06233081e7"},"source":["#读写\n","keras.backend.set_image_data_format('channels_first')\n","input_RW=Input(shape=(look_back,))\n","em_R=Embedding(2, 32, input_length=look_back)(input_RW)\n","em_R=Dropout(0.2)(em_R)\n","#R=Reshape((look_back,1))(input_RW)\n","\n","#Delta值\n","input_D=Input(shape=(look_back,))\n","em=Embedding(type_new, 32, input_length=look_back)(input_D)\n","em=TrigPosEmbedding(output_dim=look_back, mode=TrigPosEmbedding.MODE_ADD)(em)\n","#em=PositionEmbedding(input_dim=type_new , output_dim=look_back, mode=PositionEmbedding.MODE_ADD)(em)\n","\n","#将RW和Delta合为一体\n","merges = Add()([em, em_R])\n","#merges = Concatenate(axis=2)([em, em_R])\n","#merges = Reshape((look_back, 32, 1))(merges)\n","merges = Reshape((1, look_back, 32))(merges)\n","spec_cnn = merges\n","\n","pool_size = [2, 1]\n","nb_cnn2d_filt = 128\n","dropout_rate = 0.0\n","\n","# CONVOLUTIONAL LAYERS =========================================================\n","for i, convCnt in enumerate(pool_size):\n","    #spec_cnn = Conv1D(filters=nb_cnn2d_filt, kernel_size=(3), padding='same')(spec_cnn)\n","    spec_cnn = Conv2D(filters=nb_cnn2d_filt, kernel_size=(3, 3), padding='same')(spec_cnn)\n","    spec_cnn = BatchNormalization()(spec_cnn)\n","    spec_cnn = Activation('relu')(spec_cnn)\n","    #spec_cnn = MaxPooling1D(pool_size=(pool_size[i]))(spec_cnn)\n","    spec_cnn = MaxPooling2D(pool_size=(1, pool_size[i]))(spec_cnn)\n","    spec_cnn = Dropout(dropout_rate)(spec_cnn)\n","spec_cnn = Permute((2, 1, 3))(spec_cnn)\n","resblock_input = Reshape((50, -1))(spec_cnn)\n","resblock_input = TimeDistributed((Dense(128)))(resblock_input)\n","#resblock_input = spec_cnn\n","\n","# TCN layer ===================================================================\n","# residual blocks ------------------------\n","skip_connections = []\n","\n","for d in range(10):\n","\n","        # 1D convolution\n","    spec_conv1d = keras.layers.Convolution1D(filters=256,\n","                                                kernel_size=(3),\n","                                                padding='same',\n","                                                dilation_rate=2**d)(resblock_input)\n","    spec_conv1d = BatchNormalization()(spec_conv1d)\n","\n","        # activations\n","    tanh_out = keras.layers.Activation('tanh')(spec_conv1d)\n","    sigm_out = keras.layers.Activation('sigmoid')(spec_conv1d)\n","    spec_act = keras.layers.Multiply()([tanh_out, sigm_out])\n","\n","        # spatial dropout\n","    spec_drop = keras.layers.SpatialDropout1D(rate=0.5)(spec_act)\n","\n","        # 1D convolution\n","    skip_output = keras.layers.Convolution1D(filters=128,\n","                                                 kernel_size=(1),\n","                                                 padding='same')(spec_drop)\n","\n","    res_output = keras.layers.Add()([resblock_input, skip_output])\n","\n","    if skip_output is not None:\n","        skip_connections.append(skip_output)\n","\n","    resblock_input = res_output\n","# ---------------------------------------\n","# Residual blocks sum\n","spec_sum = keras.layers.Add()(skip_connections)\n","spec_sum = keras.layers.Activation('relu')(spec_sum)\n","\n","    # 1D convolution\n","spec_conv1d_2 = keras.layers.Convolution1D(filters=128,\n","                                          kernel_size=(1),\n","                                          padding='same')(spec_sum)\n","spec_conv1d_2 = keras.layers.Activation('relu')(spec_conv1d_2)\n","\n","    # 1D convolution\n","spec_tcn = keras.layers.Convolution1D(filters=128,\n","                                          kernel_size=(1),\n","                                          padding='same')(spec_conv1d_2)\n","spec_tcn = keras.layers.Activation('tanh')(spec_tcn)\n","\n","#attention\n","#attention = Attention(128)(spec_tcn)\n","#attention = RepeatVector(50)(attention)\n","\n","#TCN调库\n","#tcn1=TCN(kernel_size=5, nb_stacks=2,return_sequences=True, dilations=(1,2,4,8,16,32), use_layer_norm=True)(em) #merges\n","#conv_out = Conv1D(8, 1, activation=\"relu\")(tcn1)\n","\n","fnn_size = [128, 128]\n","# Delta_part ==================================================================\n","Delta_part = spec_tcn #attention\n","for nb_fnn_filt in fnn_size:\n","    Delta_part = TimeDistributed(Dense(nb_fnn_filt))(Delta_part)\n","    Delta_part = Dropout(dropout_rate)(Delta_part)\n","Delta_part = Flatten()(Delta_part)\n","Delta_part = Dense(num_bit*pridict_size)(Delta_part)\n","Delta_part = Reshape((pridict_size, num_bit))(Delta_part)\n","Delta_part = Activation('sigmoid', name='Delta_part')(Delta_part)\n","\n","    # RW_part ==================================================================\n","RW_part = spec_tcn\n","for nb_fnn_filt in fnn_size:\n","    RW_part = TimeDistributed(Dense(nb_fnn_filt))(RW_part)\n","    RW_part = Dropout(dropout_rate)(RW_part)\n","\n","RW_part = Flatten()(RW_part)\n","RW_part = Dense(1*pridict_size)(RW_part)\n","RW_part = Reshape((pridict_size, 1))(RW_part)\n","RW_part = Activation('sigmoid', name='RW_part')(RW_part)\n","\n","# Output of convolutional layers\n","#conv_out = Flatten()(conv_out)\n","\n","\n","# Concatenate with categorical features\n","#x = concatenate([conv_out] + cat_flatten)\n","#x = Dense(pridict_size*num_bit, activation=\"relu\")(conv_out) #Delta_part\n","#x = Reshape((pridict_size,num_bit))(x)\n","#outputs = Activation('sigmoid')(x)\n","\n","\n","\n","# Define model interface\n","model = Model(inputs=[input_D, input_RW], outputs=[Delta_part, RW_part]) #, input_RW , outputs_rw\n","model.compile(loss=['binary_crossentropy', 'binary_crossentropy'], optimizer='adam', metrics=['acc'])#tf.compat.v1.keras.metrics.binary_accuracy\n","early_stopping=EarlyStopping(monitor='val_loss',patience=15,verbose=2)\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 50)]         0                                            \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            [(None, 50)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 50, 32)       15328       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 50, 32)       64          input_1[0][0]                    \n","__________________________________________________________________________________________________\n","trig_pos_embedding (TrigPosEmbe (None, 50, 32)       0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 50, 32)       0           embedding[0][0]                  \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 50, 32)       0           trig_pos_embedding[0][0]         \n","                                                                 dropout[0][0]                    \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 1, 50, 32)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 128, 50, 32)  1280        reshape[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 128, 50, 32)  128         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 128, 50, 32)  0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 128, 50, 16)  0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 128, 50, 16)  0           max_pooling2d[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 128, 50, 16)  147584      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 128, 50, 16)  64          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 128, 50, 16)  0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 128, 50, 16)  0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 128, 50, 16)  0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","permute (Permute)               (None, 50, 128, 16)  0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 50, 2048)     0           permute[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 50, 128)      262272      reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 50, 256)      98560       time_distributed[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 50, 256)      1024        conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 50, 256)      0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 50, 256)      0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","multiply (Multiply)             (None, 50, 256)      0           activation_2[0][0]               \n","                                                                 activation_3[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d (SpatialDropo (None, 50, 256)      0           multiply[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 50, 128)      32896       spatial_dropout1d[0][0]          \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 50, 128)      0           time_distributed[0][0]           \n","                                                                 conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 50, 256)      98560       add_1[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 50, 256)      1024        conv1d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 50, 256)      0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 50, 256)      0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, 50, 256)      0           activation_4[0][0]               \n","                                                                 activation_5[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d_1 (SpatialDro (None, 50, 256)      0           multiply_1[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_3 (Conv1D)               (None, 50, 128)      32896       spatial_dropout1d_1[0][0]        \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 50, 128)      0           add_1[0][0]                      \n","                                                                 conv1d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_4 (Conv1D)               (None, 50, 256)      98560       add_2[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 50, 256)      1024        conv1d_4[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 50, 256)      0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 50, 256)      0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, 50, 256)      0           activation_6[0][0]               \n","                                                                 activation_7[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d_2 (SpatialDro (None, 50, 256)      0           multiply_2[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_5 (Conv1D)               (None, 50, 128)      32896       spatial_dropout1d_2[0][0]        \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 50, 128)      0           add_2[0][0]                      \n","                                                                 conv1d_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_6 (Conv1D)               (None, 50, 256)      98560       add_3[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 50, 256)      1024        conv1d_6[0][0]                   \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 50, 256)      0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 50, 256)      0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","multiply_3 (Multiply)           (None, 50, 256)      0           activation_8[0][0]               \n","                                                                 activation_9[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d_3 (SpatialDro (None, 50, 256)      0           multiply_3[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_7 (Conv1D)               (None, 50, 128)      32896       spatial_dropout1d_3[0][0]        \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 50, 128)      0           add_3[0][0]                      \n","                                                                 conv1d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_8 (Conv1D)               (None, 50, 256)      98560       add_4[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 50, 256)      1024        conv1d_8[0][0]                   \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 50, 256)      0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 50, 256)      0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","multiply_4 (Multiply)           (None, 50, 256)      0           activation_10[0][0]              \n","                                                                 activation_11[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_4 (SpatialDro (None, 50, 256)      0           multiply_4[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_9 (Conv1D)               (None, 50, 128)      32896       spatial_dropout1d_4[0][0]        \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 50, 128)      0           add_4[0][0]                      \n","                                                                 conv1d_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_10 (Conv1D)              (None, 50, 256)      98560       add_5[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 50, 256)      1024        conv1d_10[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 50, 256)      0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 50, 256)      0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","multiply_5 (Multiply)           (None, 50, 256)      0           activation_12[0][0]              \n","                                                                 activation_13[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_5 (SpatialDro (None, 50, 256)      0           multiply_5[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_11 (Conv1D)              (None, 50, 128)      32896       spatial_dropout1d_5[0][0]        \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 50, 128)      0           add_5[0][0]                      \n","                                                                 conv1d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_12 (Conv1D)              (None, 50, 256)      98560       add_6[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 50, 256)      1024        conv1d_12[0][0]                  \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 50, 256)      0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 50, 256)      0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","multiply_6 (Multiply)           (None, 50, 256)      0           activation_14[0][0]              \n","                                                                 activation_15[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_6 (SpatialDro (None, 50, 256)      0           multiply_6[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_13 (Conv1D)              (None, 50, 128)      32896       spatial_dropout1d_6[0][0]        \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 50, 128)      0           add_6[0][0]                      \n","                                                                 conv1d_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_14 (Conv1D)              (None, 50, 256)      98560       add_7[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 50, 256)      1024        conv1d_14[0][0]                  \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 50, 256)      0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 50, 256)      0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","multiply_7 (Multiply)           (None, 50, 256)      0           activation_16[0][0]              \n","                                                                 activation_17[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_7 (SpatialDro (None, 50, 256)      0           multiply_7[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_15 (Conv1D)              (None, 50, 128)      32896       spatial_dropout1d_7[0][0]        \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 50, 128)      0           add_7[0][0]                      \n","                                                                 conv1d_15[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_16 (Conv1D)              (None, 50, 256)      98560       add_8[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 50, 256)      1024        conv1d_16[0][0]                  \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 50, 256)      0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 50, 256)      0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","multiply_8 (Multiply)           (None, 50, 256)      0           activation_18[0][0]              \n","                                                                 activation_19[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_8 (SpatialDro (None, 50, 256)      0           multiply_8[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_17 (Conv1D)              (None, 50, 128)      32896       spatial_dropout1d_8[0][0]        \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 50, 128)      0           add_8[0][0]                      \n","                                                                 conv1d_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_18 (Conv1D)              (None, 50, 256)      98560       add_9[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 50, 256)      1024        conv1d_18[0][0]                  \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 50, 256)      0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 50, 256)      0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","multiply_9 (Multiply)           (None, 50, 256)      0           activation_20[0][0]              \n","                                                                 activation_21[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_9 (SpatialDro (None, 50, 256)      0           multiply_9[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_19 (Conv1D)              (None, 50, 128)      32896       spatial_dropout1d_9[0][0]        \n","__________________________________________________________________________________________________\n","add_11 (Add)                    (None, 50, 128)      0           conv1d_1[0][0]                   \n","                                                                 conv1d_3[0][0]                   \n","                                                                 conv1d_5[0][0]                   \n","                                                                 conv1d_7[0][0]                   \n","                                                                 conv1d_9[0][0]                   \n","                                                                 conv1d_11[0][0]                  \n","                                                                 conv1d_13[0][0]                  \n","                                                                 conv1d_15[0][0]                  \n","                                                                 conv1d_17[0][0]                  \n","                                                                 conv1d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 50, 128)      0           add_11[0][0]                     \n","__________________________________________________________________________________________________\n","conv1d_20 (Conv1D)              (None, 50, 128)      16512       activation_22[0][0]              \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 50, 128)      0           conv1d_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_21 (Conv1D)              (None, 50, 128)      16512       activation_23[0][0]              \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 50, 128)      0           conv1d_21[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 50, 128)      16512       activation_24[0][0]              \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 50, 128)      16512       activation_24[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 50, 128)      0           time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 50, 128)      0           time_distributed_3[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 50, 128)      16512       dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_4 (TimeDistrib (None, 50, 128)      16512       dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 50, 128)      0           time_distributed_2[0][0]         \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 50, 128)      0           time_distributed_4[0][0]         \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 6400)         0           dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6400)         0           dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 18)           115218      flatten[0][0]                    \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 2)            12802       flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 2, 9)         0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 2, 1)         0           dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","Delta_part (Activation)         (None, 2, 9)         0           reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","RW_part (Activation)            (None, 2, 1)         0           reshape_3[0][0]                  \n","==================================================================================================\n","Total params: 1,978,612\n","Trainable params: 1,973,396\n","Non-trainable params: 5,216\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p5gmEdpxq27K"},"source":["#prediction=model.predict([trainX[:5,:],trainX_RW[:5,:]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_2Bxgy8q94q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623228081588,"user_tz":-480,"elapsed":505772,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"b7e88ec9-fbd6-4d92-e05c-210f64808843"},"source":["#prediction\n","history=model.fit([trainX,trainX_RW],[trainY, trainY_RW], epochs=15, batch_size=128, \n","                  verbose=1\n","                  , validation_data=([testX,testX_RW], [testY,testY_RW]),\n","                  callbacks=[early_stopping])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 159948 samples, validate on 39948 samples\n","Epoch 1/30\n","159948/159948 [==============================] - ETA: 0s - loss: 0.7317 - Delta_part_loss: 0.4493 - RW_part_loss: 0.2824 - Delta_part_acc: 0.7791 - RW_part_acc: 0.8798"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["159948/159948 [==============================] - 225s 1ms/sample - loss: 0.7317 - Delta_part_loss: 0.4493 - RW_part_loss: 0.2824 - Delta_part_acc: 0.7791 - RW_part_acc: 0.8798 - val_loss: 0.8577 - val_Delta_part_loss: 0.4582 - val_RW_part_loss: 0.4001 - val_Delta_part_acc: 0.7897 - val_RW_part_acc: 0.8221\n","Epoch 2/30\n","159948/159948 [==============================] - 203s 1ms/sample - loss: 0.5179 - Delta_part_loss: 0.3397 - RW_part_loss: 0.1782 - Delta_part_acc: 0.8457 - RW_part_acc: 0.9342 - val_loss: 0.8284 - val_Delta_part_loss: 0.4443 - val_RW_part_loss: 0.3834 - val_Delta_part_acc: 0.7840 - val_RW_part_acc: 0.8185\n","Epoch 3/30\n","159948/159948 [==============================] - 202s 1ms/sample - loss: 0.4642 - Delta_part_loss: 0.3015 - RW_part_loss: 0.1627 - Delta_part_acc: 0.8681 - RW_part_acc: 0.9411 - val_loss: 0.8250 - val_Delta_part_loss: 0.4387 - val_RW_part_loss: 0.3859 - val_Delta_part_acc: 0.7908 - val_RW_part_acc: 0.8255\n","Epoch 4/30\n","159948/159948 [==============================] - 202s 1ms/sample - loss: 0.4302 - Delta_part_loss: 0.2762 - RW_part_loss: 0.1540 - Delta_part_acc: 0.8819 - RW_part_acc: 0.9451 - val_loss: 0.7988 - val_Delta_part_loss: 0.4183 - val_RW_part_loss: 0.3790 - val_Delta_part_acc: 0.8123 - val_RW_part_acc: 0.8232\n","Epoch 5/30\n","159948/159948 [==============================] - 201s 1ms/sample - loss: 0.4085 - Delta_part_loss: 0.2601 - RW_part_loss: 0.1485 - Delta_part_acc: 0.8903 - RW_part_acc: 0.9473 - val_loss: 0.8179 - val_Delta_part_loss: 0.4226 - val_RW_part_loss: 0.3948 - val_Delta_part_acc: 0.8026 - val_RW_part_acc: 0.8126\n","Epoch 6/30\n","159948/159948 [==============================] - 205s 1ms/sample - loss: 0.3904 - Delta_part_loss: 0.2470 - RW_part_loss: 0.1434 - Delta_part_acc: 0.8968 - RW_part_acc: 0.9494 - val_loss: 0.8255 - val_Delta_part_loss: 0.4262 - val_RW_part_loss: 0.4002 - val_Delta_part_acc: 0.8088 - val_RW_part_acc: 0.8330\n","Epoch 7/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3763 - Delta_part_loss: 0.2373 - RW_part_loss: 0.1390 - Delta_part_acc: 0.9015 - RW_part_acc: 0.9510 - val_loss: 0.7842 - val_Delta_part_loss: 0.4105 - val_RW_part_loss: 0.3730 - val_Delta_part_acc: 0.8127 - val_RW_part_acc: 0.8236\n","Epoch 8/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3647 - Delta_part_loss: 0.2289 - RW_part_loss: 0.1358 - Delta_part_acc: 0.9057 - RW_part_acc: 0.9523 - val_loss: 0.8153 - val_Delta_part_loss: 0.4093 - val_RW_part_loss: 0.4058 - val_Delta_part_acc: 0.8095 - val_RW_part_acc: 0.8173\n","Epoch 9/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3544 - Delta_part_loss: 0.2219 - RW_part_loss: 0.1325 - Delta_part_acc: 0.9094 - RW_part_acc: 0.9534 - val_loss: 0.8292 - val_Delta_part_loss: 0.4154 - val_RW_part_loss: 0.4138 - val_Delta_part_acc: 0.8160 - val_RW_part_acc: 0.8316\n","Epoch 10/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3462 - Delta_part_loss: 0.2167 - RW_part_loss: 0.1296 - Delta_part_acc: 0.9119 - RW_part_acc: 0.9544 - val_loss: 0.7695 - val_Delta_part_loss: 0.4142 - val_RW_part_loss: 0.3549 - val_Delta_part_acc: 0.8116 - val_RW_part_acc: 0.8420\n","Epoch 11/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3384 - Delta_part_loss: 0.2114 - RW_part_loss: 0.1270 - Delta_part_acc: 0.9144 - RW_part_acc: 0.9552 - val_loss: 0.8059 - val_Delta_part_loss: 0.4123 - val_RW_part_loss: 0.3933 - val_Delta_part_acc: 0.8210 - val_RW_part_acc: 0.8228\n","Epoch 12/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3322 - Delta_part_loss: 0.2079 - RW_part_loss: 0.1243 - Delta_part_acc: 0.9160 - RW_part_acc: 0.9561 - val_loss: 0.8006 - val_Delta_part_loss: 0.4160 - val_RW_part_loss: 0.3839 - val_Delta_part_acc: 0.8148 - val_RW_part_acc: 0.8367\n","Epoch 13/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3274 - Delta_part_loss: 0.2046 - RW_part_loss: 0.1228 - Delta_part_acc: 0.9174 - RW_part_acc: 0.9564 - val_loss: 0.8444 - val_Delta_part_loss: 0.4115 - val_RW_part_loss: 0.4326 - val_Delta_part_acc: 0.8222 - val_RW_part_acc: 0.8131\n","Epoch 14/30\n","159948/159948 [==============================] - 202s 1ms/sample - loss: 0.3199 - Delta_part_loss: 0.2006 - RW_part_loss: 0.1192 - Delta_part_acc: 0.9193 - RW_part_acc: 0.9579 - val_loss: 0.7965 - val_Delta_part_loss: 0.4074 - val_RW_part_loss: 0.3880 - val_Delta_part_acc: 0.8197 - val_RW_part_acc: 0.8339\n","Epoch 15/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3157 - Delta_part_loss: 0.1980 - RW_part_loss: 0.1176 - Delta_part_acc: 0.9206 - RW_part_acc: 0.9584 - val_loss: 0.8494 - val_Delta_part_loss: 0.4265 - val_RW_part_loss: 0.4234 - val_Delta_part_acc: 0.8166 - val_RW_part_acc: 0.8054\n","Epoch 16/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3106 - Delta_part_loss: 0.1955 - RW_part_loss: 0.1150 - Delta_part_acc: 0.9215 - RW_part_acc: 0.9592 - val_loss: 0.8398 - val_Delta_part_loss: 0.4161 - val_RW_part_loss: 0.4247 - val_Delta_part_acc: 0.8216 - val_RW_part_acc: 0.8217\n","Epoch 17/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3067 - Delta_part_loss: 0.1934 - RW_part_loss: 0.1134 - Delta_part_acc: 0.9225 - RW_part_acc: 0.9598 - val_loss: 0.8469 - val_Delta_part_loss: 0.4172 - val_RW_part_loss: 0.4293 - val_Delta_part_acc: 0.8262 - val_RW_part_acc: 0.8145\n","Epoch 18/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.3024 - Delta_part_loss: 0.1910 - RW_part_loss: 0.1114 - Delta_part_acc: 0.9235 - RW_part_acc: 0.9599 - val_loss: 0.8475 - val_Delta_part_loss: 0.4139 - val_RW_part_loss: 0.4328 - val_Delta_part_acc: 0.8206 - val_RW_part_acc: 0.8281\n","Epoch 19/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.2982 - Delta_part_loss: 0.1889 - RW_part_loss: 0.1093 - Delta_part_acc: 0.9245 - RW_part_acc: 0.9609 - val_loss: 0.9580 - val_Delta_part_loss: 0.4491 - val_RW_part_loss: 0.5086 - val_Delta_part_acc: 0.8114 - val_RW_part_acc: 0.7992\n","Epoch 20/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.2936 - Delta_part_loss: 0.1866 - RW_part_loss: 0.1070 - Delta_part_acc: 0.9253 - RW_part_acc: 0.9618 - val_loss: 0.8320 - val_Delta_part_loss: 0.4216 - val_RW_part_loss: 0.4100 - val_Delta_part_acc: 0.8206 - val_RW_part_acc: 0.8347\n","Epoch 21/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.2913 - Delta_part_loss: 0.1853 - RW_part_loss: 0.1060 - Delta_part_acc: 0.9259 - RW_part_acc: 0.9617 - val_loss: 0.9035 - val_Delta_part_loss: 0.4231 - val_RW_part_loss: 0.4789 - val_Delta_part_acc: 0.8220 - val_RW_part_acc: 0.8178\n","Epoch 22/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.2860 - Delta_part_loss: 0.1827 - RW_part_loss: 0.1033 - Delta_part_acc: 0.9270 - RW_part_acc: 0.9625 - val_loss: 0.8778 - val_Delta_part_loss: 0.4277 - val_RW_part_loss: 0.4508 - val_Delta_part_acc: 0.8188 - val_RW_part_acc: 0.8248\n","Epoch 23/30\n","159948/159948 [==============================] - 203s 1ms/sample - loss: 0.2835 - Delta_part_loss: 0.1815 - RW_part_loss: 0.1020 - Delta_part_acc: 0.9276 - RW_part_acc: 0.9630 - val_loss: 0.8583 - val_Delta_part_loss: 0.4292 - val_RW_part_loss: 0.4303 - val_Delta_part_acc: 0.8133 - val_RW_part_acc: 0.8307\n","Epoch 24/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.2810 - Delta_part_loss: 0.1804 - RW_part_loss: 0.1006 - Delta_part_acc: 0.9279 - RW_part_acc: 0.9633 - val_loss: 0.8760 - val_Delta_part_loss: 0.4358 - val_RW_part_loss: 0.4393 - val_Delta_part_acc: 0.8139 - val_RW_part_acc: 0.8214\n","Epoch 25/30\n","159948/159948 [==============================] - 204s 1ms/sample - loss: 0.2781 - Delta_part_loss: 0.1793 - RW_part_loss: 0.0987 - Delta_part_acc: 0.9283 - RW_part_acc: 0.9640 - val_loss: 0.9048 - val_Delta_part_loss: 0.4314 - val_RW_part_loss: 0.4729 - val_Delta_part_acc: 0.8161 - val_RW_part_acc: 0.8197\n","Epoch 00025: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EVS9W5tciC5I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623228081591,"user_tz":-480,"elapsed":22,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"be3565e2-c327-4640-f88f-a01421e8778b"},"source":["'''\n","history=model.fit([trainX,trainX_RW],[trainY, trainY_RW], epochs=20, batch_size=128, \n","                  verbose=1\n","                  , validation_data=([testX,testX_RW], [testY,testY_RW]),\n","                  callbacks=[early_stopping])\n","                  \n","history=model.fit([trainX],[trainY], epochs=20, batch_size=128, \n","                  verbose=1\n","                  , validation_data=([testX], [testY]),\n","                  callbacks=[early_stopping])\n","\n","history=model.fit([trainX,trainRW_X],trainY, epochs=5, batch_size=32, \n","                  verbose=1, validation_data=([testX,testRW_X], testY),\n","                  callbacks=[early_stopping])\n","'''\n","'''\n","history=model.fit(trainRW_X, trainRW_Y, \n","                  epochs=20, batch_size=32, \n","                  verbose=1, validation_data=(testRW_X, testRW_Y),\n","                  callbacks=[early_stopping])'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nhistory=model.fit(trainRW_X, trainRW_Y, \\n                  epochs=20, batch_size=32, \\n                  verbose=1, validation_data=(testRW_X, testRW_Y),\\n                  callbacks=[early_stopping])'"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"akGL7oWxmaBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623228081593,"user_tz":-480,"elapsed":15,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"72adf0a8-0cf0-4999-9289-aaad216a4d5c"},"source":["plt.plot(history.history['Delta_part_acc'], label='trainD')\n","plt.plot(history.history['RW_part_acc'], label='trainRW')\n","plt.plot(history.history['val_Delta_part_acc'], label='testD')\n","plt.plot(history.history['val_RW_part_acc'], label='testRW')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"KfRNS26Eksy9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623228081595,"user_tz":-480,"elapsed":13,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"115fc74c-ce87-4abe-9b3d-8e7f7471fafa"},"source":["'''\n","plt.plot(history.history['activation_48_acc'], label='trainD')\n","plt.plot(history.history['activation_49_acc'], label='trainRW')\n","plt.plot(history.history['val_activation_48_acc'], label='testD')\n","plt.plot(history.history['val_activation_49_acc'], label='testRW')\n","plt.legend()\n","plt.show()\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nplt.plot(history.history['activation_48_acc'], label='trainD')\\nplt.plot(history.history['activation_49_acc'], label='trainRW')\\nplt.plot(history.history['val_activation_48_acc'], label='testD')\\nplt.plot(history.history['val_activation_49_acc'], label='testRW')\\nplt.legend()\\nplt.show()\\n\""]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"r8C7r5fhZxjq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623228082151,"user_tz":-480,"elapsed":567,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"681588c1-d915-4ff5-8b5b-66b3b357e7bb"},"source":["prediction = model.predict([trainX[:2,:],trainX_RW[:2,:]])\n","prediction"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[array([[[4.4077489e-04, 9.9800259e-01, 9.9889272e-01, 9.9883586e-01,\n","          9.9999952e-01, 9.9949646e-01, 9.9997509e-01, 9.9999964e-01,\n","          2.9542206e-03],\n","         [3.0497552e-03, 9.9602377e-01, 2.8448678e-03, 1.4455547e-02,\n","          9.9936074e-01, 9.6522820e-01, 6.6586308e-02, 9.9577361e-01,\n","          4.0158603e-02]],\n"," \n","        [[4.8103710e-03, 9.9775428e-01, 5.5017805e-04, 8.6278826e-02,\n","          9.9926597e-01, 9.1078454e-01, 5.6285463e-02, 9.9890220e-01,\n","          1.4037000e-02],\n","         [9.9929547e-01, 2.1839286e-03, 6.4711985e-03, 1.8640662e-03,\n","          9.5645692e-03, 9.9989414e-01, 9.9989176e-01, 9.0808356e-03,\n","          9.4456496e-03]]], dtype=float32), array([[[9.9982649e-01],\n","         [8.0303282e-05]],\n"," \n","        [[7.4155569e-06],\n","         [1.8389865e-03]]], dtype=float32)]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"W4BKiW5CeBsm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623228082151,"user_tz":-480,"elapsed":6,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"2628e6cc-5507-44ed-ef49-b28dec2bd28e"},"source":["trainY[0:2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [0, 1, 0, 0, 1, 1, 0, 1, 0]],\n","\n","       [[0, 1, 0, 0, 1, 1, 0, 1, 0],\n","        [1, 0, 0, 0, 0, 1, 1, 0, 0]]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"tiahjcKPz2Mh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623228082152,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jacky Free","photoUrl":"","userId":"10613091817770638083"}},"outputId":"f019cf4d-93db-4a88-b1e0-a9167e07219b"},"source":["testY_RW[0:2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0],\n","        [0]],\n","\n","       [[0],\n","        [0]]])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"gOIj8idxz1bP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUSeKem-_QEJ"},"source":[""],"execution_count":null,"outputs":[]}]}