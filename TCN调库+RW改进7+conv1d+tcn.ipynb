{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TCN调库+RW改进7+conv1d+tcn.ipynb","provenance":[{"file_id":"1blxquNGX76VLmqUCrwhoIfeUBxLTHMax","timestamp":1623203623331},{"file_id":"1JtFWMeXCcrMbTDJ2Rg7N1IPK64bSOtP_","timestamp":1622640195520},{"file_id":"1RcfoEvn4I0gV1j5WkMOXl23zI-RSuOEM","timestamp":1622382721188},{"file_id":"1vaNf0HRXR3E-dJO4vpWSs5V0sNgVjC1R","timestamp":1622356193317},{"file_id":"1bQa6QL4ORvg1Loe9WF6fp5aftI9hyLuW","timestamp":1622262491358},{"file_id":"1wQXUyPnj2ll0N7sJ3yvxF4ErF1qwjU1C","timestamp":1620531531429},{"file_id":"1bZPiq4pRkHVgTQUjHCsziJ0OmhigPygK","timestamp":1614738073155},{"file_id":"1APMCIHNRt2NDM1-papP-8oT2xBpw26Th","timestamp":1614324965689},{"file_id":"1ES2hTLd47ynWZ5bZD3Thx8X5Ly_Y8JqR","timestamp":1614003702181},{"file_id":"1iHtyaEmu6GYHWlYXtqnBJRqFLMHlwjL_","timestamp":1613965734903},{"file_id":"1kqc8mHLk9exL_48y3ZnmtmzM9B4_93t3","timestamp":1608117183616},{"file_id":"1PqQDgTJjBTQmMVw7WMGA6tBPeEIqAufP","timestamp":1606922827989},{"file_id":"1p-rTF5Apa_4zzv8RpMaG6bW9IsoK32wW","timestamp":1606836180688},{"file_id":"1PvDnsIyqJbA8muhVQrRGgatHjzNvXmOz","timestamp":1606661874647},{"file_id":"1PIHkfvfEXhpqp7MUa21azXNg8e-_AKHZ","timestamp":1605965023134},{"file_id":"19_JkylFRwWNs7kq8-slLsW7MI2zjrSxY","timestamp":1605940409417}],"collapsed_sections":[],"authorship_tag":"ABX9TyMeBKmP6QBZ9WhR8kbt9TmC"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BhqH-sPqNAUI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632655869364,"user_tz":-480,"elapsed":534,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"8f40541f-577c-48f9-b5a2-c0a6a6bea25f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"iX7UiIiYNW73","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632655872061,"user_tz":-480,"elapsed":1931,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"1f4d7564-dab4-44ed-88d3-f0cb5967a602"},"source":["#多输入多输出\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","!/opt/bin/nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep 26 11:31:12 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"G50r8UONNZO8","executionInfo":{"status":"ok","timestamp":1632655872879,"user_tz":-480,"elapsed":820,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from pandas import read_csv\n","import math\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Multiply, Lambda\n","from tensorflow.keras.layers import LSTM, GRU, Embedding, Flatten, Reshape, Layer, Average, Add, Lambda, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Conv1D, Convolution1D, MaxPooling1D, concatenate, Input, Bidirectional, RepeatVector, Concatenate, Dot, Permute, TimeDistributed, SpatialDropout1D, AveragePooling1D, UpSampling1D\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.layers import Activation, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","#from tensorflow.keras.optimizers import *\n","tf.compat.v1.disable_eager_execution()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiQEZz15Q1zl","executionInfo":{"status":"ok","timestamp":1632655872880,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["#调用self attention\n","#!pip install keras-self-attention\n","#!pip uninstall keras-tcn --no-dependencies\n","#!kill -9 -1\n","#!pip install attention"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"ty-px0wRNkgx","executionInfo":{"status":"ok","timestamp":1632655876369,"user_tz":-480,"elapsed":3493,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"96efc931-c9ba-4727-f951-d14fdb23374f"},"source":["#导入文件\n","#out版本\n","input_file=\"/content/drive/My Drive/Benchmark_PageGap/PageGap_RW/PageGap_526_RW.out\" #521  \n","dataset=read_csv(input_file, header=None, index_col=None, sep=' ')\n","start=2800000  #5060000\n","end=3000000  #5260000\n","datasetRW=dataset[0].values[start:end]\n","#dataset.drop(, axis=1, inplace=True)\n","datasetD=dataset[1].values[start:end]\n","dataset\n"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>-43</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>-43</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8136228</th>\n","      <td>0</td>\n","      <td>-7552</td>\n","    </tr>\n","    <tr>\n","      <th>8136229</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8136230</th>\n","      <td>1</td>\n","      <td>7552</td>\n","    </tr>\n","    <tr>\n","      <th>8136231</th>\n","      <td>0</td>\n","      <td>-7552</td>\n","    </tr>\n","    <tr>\n","      <th>8136232</th>\n","      <td>1</td>\n","      <td>7552</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8136233 rows × 2 columns</p>\n","</div>"],"text/plain":["         0     1\n","0        0    43\n","1        0   -43\n","2        0    43\n","3        0   -43\n","4        0    40\n","...     ..   ...\n","8136228  0 -7552\n","8136229  0     0\n","8136230  1  7552\n","8136231  0 -7552\n","8136232  1  7552\n","\n","[8136233 rows x 2 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"8qjpIrm88iQR","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1632655876370,"user_tz":-480,"elapsed":24,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"781fcb59-da1b-49b0-d217-fca06d1dc960"},"source":["#查看数据分布\n","plt.plot(datasetD, '.')\n","plt.show()"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHFUUa0Bs39K","executionInfo":{"status":"ok","timestamp":1632655876371,"user_tz":-480,"elapsed":23,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"b1aae1e9-f253-444c-efe3-a7e719864a5c"},"source":["#读写数据\n","datasetRW"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 1, 0, 0])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGeo7YvUWVgU","executionInfo":{"status":"ok","timestamp":1632655876372,"user_tz":-480,"elapsed":18,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"40641c77-3ac7-4073-ec3f-a12275131b01"},"source":["#delta数据\n","datasetD"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([     0,  15703, -15703, ...,   -241, -15296,  15537])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPW886To9k5j","executionInfo":{"status":"ok","timestamp":1632655876372,"user_tz":-480,"elapsed":15,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"e8be7b74-e8de-4b71-b2da-32c51e341f5f"},"source":["#提前先分好训练集和测试集\n","train_size = int(len(datasetD) * 0.8)\n","print(train_size)\n","train_Delta = datasetD[:train_size]\n","test_Delta = datasetD[train_size:]\n","train_RW = datasetRW[:train_size]\n","test_RW = datasetRW[train_size:]\n","print(train_Delta.shape)\n","print(test_Delta.shape)\n","print(train_RW.shape)\n","print(test_RW.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["160000\n","(160000,)\n","(40000,)\n","(160000,)\n","(40000,)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vongkSjsKYM","executionInfo":{"status":"ok","timestamp":1632655876373,"user_tz":-480,"elapsed":12,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"3a886f01-ca0f-4185-b28e-ef7f6d4c4e23"},"source":["#将训练集的delta转成list和set\n","list_a=train_Delta.tolist()\n","print(\"Train_List长度：\",len(list_a))\n","\n","list_b=test_Delta.tolist()\n","print(\"Test_List长度：\",len(list_b))\n","\n","#查看delta的种类\n","set_a=set(list_a)\n","type_n_a=len(set_a)\n","print(\"Train_Set种类：\",type_n_a)\n","\n","set_b=set(list_b)\n","type_n_b=len(set_b)\n","print(\"Test_Set种类：\",type_n_b)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Train_List长度： 160000\n","Test_List长度： 40000\n","Train_Set种类： 1304\n","Test_Set种类： 566\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhRc4LN87L-h","executionInfo":{"status":"ok","timestamp":1632655877311,"user_tz":-480,"elapsed":29,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"154ba820-b19b-4f1a-a178-73a75bb9db30"},"source":["value_cnt_a = {}\n","for delta in list_a:\n","\t# get(value, num)函数的作用是获取字典中value对应的键值, num=0指示初始值大小。\n","\tvalue_cnt_a[delta] = value_cnt_a.get(delta, 0) + 1\n","\n","# 打印输出结果\n","print(\"Train字典收集训练集（delta类别-次数）：\",value_cnt_a)\n","print(\"Train各delta值：\",[key for key in value_cnt_a.keys()])\n","Vals_a=[delta for delta in value_cnt_a.values()]\n","print(\"Train各delta值的次数：\",Vals_a)\n","print(\"Train_delta种类：\",len(Vals_a))\n","\n","value_cnt_b = {}\n","for delta in list_b:\n","\t# get(value, num)函数的作用是获取字典中value对应的键值, num=0指示初始值大小。\n","\tvalue_cnt_b[delta] = value_cnt_b.get(delta, 0) + 1\n","\n","# 打印输出结果\n","print(\"Test字典收集训练集（delta类别-次数）：\",value_cnt_b)\n","print(\"Test各delta值：\",[key for key in value_cnt_b.keys()])\n","Vals_b=[delta for delta in value_cnt_b.values()]\n","print(\"Test各delta值的次数：\",Vals_b)\n","print(\"Test_delta种类：\",len(Vals_b))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Train字典收集训练集（delta类别-次数）： {0: 33074, 15703: 32, -15703: 1, -256: 16200, -15447: 28, 256: 9467, -15446: 161, 15702: 163, 255: 354, -199: 1, -15247: 1, 1: 519, -15445: 162, 15701: 163, -201: 1, -15245: 1, 15488: 198, -15488: 82, -203: 1, -15242: 1, 213: 263, 15700: 146, -15444: 147, -15701: 1, -206: 1, -15239: 1, 212: 162, -207: 1, -15237: 1, -15443: 166, 15699: 149, -15700: 3, -210: 2, -15233: 1, 15698: 162, -15442: 135, 210: 193, 211: 196, -212: 6, -15486: 7, -15441: 133, 15697: 148, -15698: 24, -470: 1, -15484: 4, -15697: 16, 209: 135, -216: 1, -15481: 3, -15440: 154, 15696: 171, -219: 1, -15477: 4, -15439: 161, 15695: 160, -15696: 19, -220: 1, -15475: 4, 208: 185, 207: 173, -223: 40, -15472: 3, 15694: 132, -15438: 148, 206: 193, -15695: 1, -225: 61, -15469: 9, 15693: 156, -15437: 166, -15693: 2, -1: 318, -227: 159, -15466: 7, 205: 200, 15692: 155, -15436: 162, -230: 68, -15463: 1, -232: 1, -15460: 1, 203: 166, -15435: 153, 204: 194, 15691: 150, -234: 1, -15458: 2, 257: 338, -15692: 1, -15434: 169, -236: 1, -15455: 3, 15690: 167, -15690: 1, -239: 1, -15451: 1, 202: 227, 15689: 167, -15433: 161, -240: 1, -15449: 3, -15432: 148, 15688: 145, -243: 1, -15190: 1, 200: 173, -15689: 1, -15688: 1, -245: 1, -15187: 1, -15431: 162, 15687: 170, -15687: 8, -247: 1, -15184: 1, 15686: 161, -15430: 164, -249: 1, 198: 16, -252: 1, -15178: 1, 15685: 146, -15429: 127, -254: 1, 197: 10, 196: 23, -15428: 148, 15684: 147, -15685: 18, -512: 3, 512: 1, -15173: 1, -15684: 16, -192: 956, -15492: 5, -15427: 159, 15683: 154, -259: 1, -15169: 1, 195: 14, 15168: 64, -15168: 46, -260: 1, 320: 30, -15487: 39, -15426: 170, 15682: 168, -15683: 1, -263: 1, 194: 2, -15425: 277, -265: 1, 15681: 243, -267: 1, -15478: 2, 15680: 112, -15424: 6646, -15681: 7, 15424: 15783, 258: 5, 193: 13, 253: 139, -15678: 157, 192: 451, 252: 167, -15676: 168, -15680: 71, -15679: 131, -15423: 105, -7: 3, -15672: 161, 15679: 21, 254: 165, -9: 2, -15669: 161, -11: 4, -15667: 166, -15677: 135, 15677: 10, 15678: 1, -14: 2, -15664: 173, -15485: 15, -16: 2, -15660: 160, -18: 1, -15658: 165, -15675: 161, 251: 164, 15676: 1, -20: 4, -15655: 171, -15674: 147, 250: 155, -23: 3, -15652: 168, -15482: 2, -24: 3, -15650: 150, 249: 193, -15673: 169, 15674: 1, -27: 1, -15646: 137, 248: 223, -29: 5, -15643: 169, 15672: 3, -31: 2, -15641: 143, 247: 250, -15671: 147, -15480: 1, -33: 2, -15638: 250, -15670: 168, 246: 460, 15671: 1, 15670: 12, -36: 1, -15634: 160, 245: 588, -38: 1, -15632: 174, 15669: 22, 15104: 2, 565: 1, -15104: 1, 182: 2, -40: 2, -15629: 168, 244: 384, -15668: 142, 15668: 1, -43: 2, -15626: 207, -15476: 8, -44: 2, -15624: 189, 243: 348, -47: 3, -15620: 85, 242: 309, -15666: 129, -49: 2, -15617: 83, -15474: 29, -576: 1, -15090: 1, 15666: 27, -15665: 143, 241: 329, -15473: 7, -51: 3, 15665: 2, -53: 1, -15420: 4, 240: 337, -56: 2, -15416: 2, 15664: 30, -15663: 162, 239: 336, -57: 2, -15414: 12, -60: 2, -15411: 3, 238: 300, -15662: 138, 15662: 12, -62: 5, -15408: 1, -15470: 1, 237: 342, -15661: 140, -15406: 17, -15597: 155, -15405: 37, 236: 334, -67: 4, -15594: 149, 15661: 1, -69: 3, -15591: 129, 235: 320, -15659: 112, -71: 3, -15589: 239, -15468: 2, -15467: 37, 15659: 1, -73: 3, -15586: 192, 234: 345, -76: 2, -15582: 135, 233: 347, -15657: 153, -77: 3, -15580: 123, -15465: 8, -15656: 148, 232: 301, -80: 2, -15577: 11, 15657: 2, -82: 3, -15574: 36, 231: 287, 15656: 1, 15655: 5, -84: 1, -15571: 12, 230: 286, -15654: 163, -86: 2, -15568: 26, -15462: 1, 15040: 1, 614: 1, 15654: 27, -89: 1, -15565: 28, 229: 360, -15653: 146, -91: 1, -15563: 18, 228: 355, -93: 1, -15559: 19, -15651: 160, 227: 355, -96: 2, -15556: 24, -97: 1, -15554: 4, 15651: 15, 226: 356, -100: 1, -128: 22, 15650: 3, -15649: 167, -102: 1, 225: 374, -104: 1, -15417: 1, -15457: 2, 15649: 8, 14784: 58, 865: 23, -14784: 22, -14783: 1, 224: 418, -15648: 161, -106: 2, 223: 316, -15647: 146, -109: 1, 863: 12, -110: 2, -15409: 7, 222: 248, -113: 1, -640: 11, -15006: 9, 640: 2, -15454: 24, -15453: 3, 221: 193, -15645: 159, -115: 2, -15402: 1, 15645: 1, -117: 1, -15400: 2, -15644: 150, 220: 170, -120: 1, -15397: 19, 219: 186, -122: 2, -15393: 25, -124: 1, -15391: 2, -15642: 159, 218: 168, -126: 2, -15388: 3, -15450: 2, 217: 160, 15641: 1, -15385: 9, -129: 2, -15513: 3, 15642: 2, -130: 1, -15511: 3, 216: 193, -15640: 170, -133: 2, -15507: 2, -15639: 168, 215: 160, -135: 1, -15504: 3, 15639: 21, 14976: 8, 663: 5, -14976: 3, -137: 1, -15502: 2, 214: 275, -139: 1, -15499: 2, -15637: 224, -2: 3, -15636: 149, -4: 1, -15633: 150, 15637: 1, -15630: 147, 15636: 3, -8: 2, -15628: 160, -15635: 166, -13: 3, -15621: 89, -15: 1, -15619: 100, 15633: 31, -17: 2, -15631: 162, -21: 5, -15418: 1, -15415: 19, -26: 2, -15412: 2, -28: 1, -15403: 1, -15627: 131, -35: 2, -15401: 1, -704: 84, -14923: 14, 704: 32, 15627: 3, -37: 1, -15398: 5, -15394: 43, 201: 288, -15625: 280, -41: 2, -15392: 2, -18477: 2, -1242: 1, 19520: 1, 199: 15, 2852: 1, -18278: 2, 18480: 3, -18480: 3, 2854: 2, -18279: 1, 2855: 1, -15390: 1, 15625: 7, 14912: 2, 714: 1, 15626: 3, -45: 1, -15386: 2, -15384: 6, -19528: 1, 19728: 1, -754: 1, -14670: 1, 191: 7, -52: 1, -54: 2, -15378: 2, 15624: 45, -15623: 83, -59: 1, -15373: 4, 15623: 70, -15369: 2, 15622: 93, -15622: 81, -63: 4, -15367: 2, -15366: 17, -66: 3, 15621: 89, -68: 3, -15553: 1, 15620: 81, -70: 4, -15551: 4, 15619: 101, -72: 4, -15547: 3, -75: 1, -15544: 1, 15618: 89, -15618: 87, -15542: 4, 15617: 84, -79: 1, -15539: 3, 15616: 101, -15616: 100, -15535: 2, -83: 2, -15533: 4, 15615: 89, -15615: 86, -15530: 2, -88: 1, -15527: 5, 15614: 83, -15614: 81, -15422: 1, -90: 1, -15525: 5, -15613: 100, 15613: 101, -92: 1, -15521: 5, 15612: 91, -15612: 85, -95: 1, -15518: 4, 14848: 5, -14848: 3, 765: 1, -14847: 1, -15516: 4, -15611: 84, 15611: 81, -99: 1, -15419: 1, 15610: 95, -15610: 99, -101: 1, -15509: 4, -103: 1, -15609: 87, 15609: 70, 15360: 14395, -15360: 9238, -15359: 161, 15608: 21, -15608: 80, -108: 1, -15501: 1, -15361: 18, -15498: 1, -15607: 92, -112: 1, -15495: 1, 15607: 12, -15351: 3, -15606: 288, 15606: 3, -19546: 1, 19584: 2, -116: 1, -15490: 1, -119: 1, -15605: 436, -15356: 1, -15413: 5, -123: 1, -15354: 3, -15349: 2, -131: 1, -15604: 238, -136: 1, -19570: 1, -3: 2, -15602: 179, -6: 2, -15599: 168, -15596: 171, -15593: 170, -15603: 179, 15604: 2, 13: 1, -19586: 1, 19830: 1, -22249: 1, 22016: 2, 10: 3, -22026: 1, -15365: 1, 15602: 6, -15601: 175, -15579: 21, 15601: 1, 15296: 280, -15296: 225, 306: 5, -15410: 1, -15575: 13, -15600: 209, -30: 1, 15600: 17, -32: 1, 15599: 3, -15564: 13, -15407: 5, -39: 1, -15560: 37, -320: 35, -15279: 4, -15598: 170, -15558: 31, 15598: 20, -1087: 1, -14511: 1, -48: 3, -15357: 1, -15350: 1, -15595: 162, -58: 1, -15346: 2, 15596: 1, -15343: 3, -61: 2, -15342: 4, -15341: 3, -15340: 2, -15339: 3, -64: 2, -15338: 15, -65: 3, -15529: 5, 24: 2, -15528: 4, 22: 1, -15382: 7, 15594: 1, -15526: 2, -15524: 4, -15523: 4, -15522: 3, -15592: 153, -15520: 5, 15592: 1, -15519: 4, -74: 2, -15515: 3, 15591: 4, 15232: 6, -15232: 3, 360: 2, -15231: 1, -15512: 3, -15590: 174, 83: 2, -2202: 1, 2240: 123, -2246: 5, -2262: 2, -18014: 1, 4: 2, -25: 1, 18048: 110, -18026: 2, -19699: 1, 19712: 1, -18243: 1, -18068: 2, -18049: 10, -2229: 2, -2260: 4, -2221: 8, -18047: 3, -2251: 4, -2222: 1, -2227: 1, -2231: 1, -2240: 8, -2238: 3, -2263: 2, -2219: 4, -18038: 4, -18046: 7, -22: 2, -18027: 3, -3871: 1, 21888: 3, -21879: 1, -15372: 3, -18013: 5, -2268: 1, -18037: 2, -18052: 7, -2245: 8, -18055: 3, -2249: 5, -2259: 3, -18025: 5, -19418: 1, 19392: 3, 32: 1, -2235: 4, -18035: 6, -2255: 2, -19391: 2, -18029: 4, -18053: 2, -2230: 2, -18050: 10, -2241: 4, -2243: 6, -18033: 4, -15759: 1, -18063: 2, -21896: 1, -2244: 3, -2217: 2, -18036: 1, -2250: 2, -2220: 2, -2237: 4, -2264: 2, -18040: 1, 3: 2, 15794: 1, -15368: 1, -19490: 1, 19460: 1, -2218: 3, -2261: 1, 2: 1, -18028: 4, 15796: 1, -18012: 3, -2248: 2, -2242: 4, -2247: 2, -18054: 1, -18031: 2, -18070: 2, -2254: 1, -18034: 2, -2236: 2, 2651: 1, -18305: 2, 18304: 4, -18051: 1, -15383: 3, -2213: 1, -2257: 3, -15757: 1, 5: 3, -18044: 2, -2223: 3, -2252: 2, -2228: 1, -2253: 1, -2224: 1, -15387: 6, -18011: 2, -18066: 2, -18015: 2, -18061: 1, -15380: 5, -18306: 1, -18284: 1, -18042: 1, -18067: 1, -18032: 1, -18048: 1, -19648: 1, 19619: 1, -19433: 1, 646: 1, 18816: 5, -19462: 1, 19456: 1, -18787: 1, -15389: 2, -18783: 6, 3423: 3, -19701: 1, 19899: 1, 15590: 96, -6723: 1, 22084: 3, -22084: 3, 22113: 2, -18065: 1, -22095: 1, -22116: 1, 22092: 2, -22090: 2, 22087: 3, -22086: 5, -22087: 1, 22090: 2, -22091: 1, 22085: 4, -22085: 4, 22086: 3, 22100: 1, -22100: 1, 22144: 2, -22144: 1, -22097: 2, -10: 1, 22097: 1, 22094: 1, -22094: 1, -22092: 1, 22105: 1, 17: 1, -15377: 2, 145: 1, -15505: 3, 28: 1, 18259: 1, -194: 1, 36: 16, -15396: 39, 15589: 154, -229: 123, 15502: 2, 88: 2, 38: 5, 37: 8, 139: 1, 15359: 7, -18009: 1, 2649: 1, -15588: 199, 15588: 150, -228: 118, -18019: 1, -15587: 198, -18023: 1, -15395: 33, 15587: 154, 35: 14, -15379: 5, -163: 1, -15331: 13, 15586: 139, -226: 148, -18039: 1, 34: 15, -18043: 1, -15362: 1, 15585: 92, -15585: 258, 2689: 1, 18279: 1, 33: 3, 15394: 1, -34: 1, -18056: 1, 2696: 1, -15584: 307, -2699: 1, 2700: 1, 15584: 53, 18288: 1, 2706: 1, 2710: 1, 288: 2, -15583: 221, 15583: 73, -18073: 1, 18297: 1, -18076: 1, 2716: 1, -18080: 1, 2720: 1, -2724: 1, 18307: 1, -22168: 1, 22391: 1, -2289: 1, 2512: 1, 15582: 54, -22401: 2, -600: 1, 2827: 1, -980: 1, 980: 1, -982: 2, 20928: 4, -22173: 1, 1246: 1, -22174: 1, 6808: 1, -18085: 1, 2725: 1, -20924: 1, 21147: 1, -18087: 1, 18310: 1, -2729: 1, 2729: 1, -15581: 206, -21150: 1, -2731: 1, 2731: 1, -20170: 1, -22172: 1, 6814: 1, 15581: 170, -18093: 1, 18315: 1, -18095: 1, 2735: 1, -2737: 1, 18318: 1, 15580: 125, -2739: 1, 2739: 1, -2741: 1, 2741: 1, -2742: 1, 2742: 1, -2744: 1, 2744: 1, -4415: 1, 19995: 1, -2746: 1, 18326: 1, -15260: 5, -4429: 1, 19776: 7, -18328: 1, 18112: 3, -323: 22, 323: 22, -195: 1, 324: 33, -324: 30, -196: 2, -467: 32, 467: 31, 468: 32, -468: 26, -994: 14, 994: 30, -768: 45, -162: 1, -832: 1, 768: 3, 995: 31, -995: 4, -1930: 1, 2157: 31, -237: 19, -1920: 39, -2157: 11, -173: 1, -1984: 1, -1925: 1, 1920: 3, -2158: 11, 2158: 30, -238: 22, -18096: 1, 18334: 28, -222: 3, -18112: 3, -18334: 24, -20200: 1, 19968: 1, -21982: 1, 792: 1, 1416: 1, -19774: 1, 1672: 1, 2746: 1, -2784: 1, 15832: 1, 2304: 5, -19770: 1, 4429: 1, 13048: 2, -15352: 3, -4183: 1, 4183: 1, 12996: 1, 2368: 3, -2316: 2, -18530: 1, 16200: 1, -15777: 1, -18133: 1, -3060: 1, 21248: 2, -18620: 1, 18560: 14, -18560: 29, -2628: 1, -19830: 1, -1418: 1, 3060: 1, 15772: 1, -2763: 1, -2327: 1, -17450: 1, 8: 1, -18892: 1, 18880: 2, -19772: 1, 892: 1, -15344: 1, -19502: 1, 4142: 1, 13034: 1, -13034: 1, -4431: 1, -15345: 1, -19793: 1, 4433: 1, -19794: 2, -4435: 3, 19840: 2, 177: 2, -4436: 1, 20017: 1, -4425: 1, 20007: 1, -4196: 1, 19549: 1, -15353: 2, 15578: 90, -15578: 14, 15577: 70, 15576: 80, -15576: 27, 15574: 114, -15573: 27, 15572: 74, -15572: 14, -15570: 16, -15324: 20, 15569: 82, -15569: 14, -15323: 72, 15579: 76, 15568: 70, 15566: 115, -15566: 23, 15563: 104, 128: 1, 15562: 121, -15562: 10, 15559: 143, -15322: 78, 15558: 146, 15556: 153, 15555: 140, -15555: 15, 15554: 118, 15552: 1, -15552: 1, 15551: 3, 15550: 3, -15550: 3, -15321: 69, 15549: 3, -15549: 3, 187: 1, 186: 1, -15546: 2, 15347: 1, -15347: 1, 15545: 3, -15545: 2, 15543: 2, -15543: 3, 15542: 2, 15541: 2, 15539: 2, -15320: 73, 15538: 2, -15538: 1, 15537: 2, -15537: 3, 152: 2, 175: 2, -15534: 1, 173: 1, -15319: 71, 15532: 2, -15532: 2, 15575: 67, 15531: 2, -15531: 3, 15529: 2, 15528: 2, 15527: 2, 165: 1, 15524: 1, 15523: 3, 15521: 2, -15318: 104, 15520: 3, 15519: 2, 15518: 2, 15516: 2, 15515: 3, 15514: 2, -15514: 2, 15512: 3, 151: 1, 15573: 89, -15317: 95, 15510: 2, -15510: 2, 15508: 3, -15316: 82, 15507: 1, -15315: 74, 146: 1, -15314: 70, 15504: 2, -15312: 60, -15381: 1, 15571: 76, 15567: 87, -15375: 1, 277: 5, -15308: 73, -19200: 3, 19199: 2, 3627: 1, -15307: 96, -15306: 137, 129: 1, 15560: 126, -15302: 117, 15557: 137, -15301: 126, -15299: 134, -15298: 95, 15553: 14, -15297: 11, 276: 8, -15295: 3, 15547: 2, -15355: 1, 15546: 2, 15570: 63, 183: 2, -3648: 2, 19218: 1, 3648: 1, 181: 1, -15541: 2, 15540: 2, -15540: 2, 12928: 2, -12928: 2, 15536: 1, -15536: 1, 15535: 2, 15533: 2, -15313: 84, 171: 1, -1344: 7, 1344: 7, 15526: 2, -15334: 2, 164: 1, 15522: 1, -15311: 84, -15567: 16, 15509: 2, -15508: 2, 15506: 1, -15506: 1, 15505: 2, -15310: 113, 142: 1, -15309: 101, 271: 4, 15500: 2, 138: 1, 15497: 2, -15305: 108, 15565: 105, 15496: 2, -15304: 130, -19264: 4, 19264: 3, 19265: 1, 15495: 1, -15303: 146, 15493: 1, 132: 1, -15300: 142, -15374: 2, 15564: 66, 15561: 143, -15561: 43, 269: 7, -15371: 1, -15370: 1, -14912: 1, 169: 1, -15337: 1, -20117: 1, 1856: 2, -2836: 1, 21120: 3, -15328: 1, 15525: 1, -5796: 1, -2470: 1, 2496: 1, -20122: 1, -2838: 1, -22365: 1, 22400: 9, -22399: 2, 6: 1, -22406: 1, -22352: 2, -22402: 1, -22404: 1, 15517: 1, 43: 1, 15513: 1, 264: 11, -384: 1, 384: 1, 15511: 1, 49: 1, -19968: 1, 4408: 1, 390: 2, 391: 15, -15167: 1, 15503: 1, -15503: 1, 57: 1, -4416: 1, 4416: 1, -15500: 1, -15497: 1, 62: 1, -2368: 5, -13190: 3, -15557: 39, -13189: 1, 15494: 1, -15494: 1, -197: 1, 15491: 1, -4672: 10, 20229: 2, 4672: 8, 15490: 1, 13184: 59, -13184: 43, 2373: 15, -13183: 1, 261: 8, 15485: 1, 71: 1, 15484: 1, -1216: 5, 1216: 5, 15482: 1, 14336: 41, -14336: 24, 1143: 1, 76: 1, 1219: 13, 1220: 4, 15300: 2, -964: 2, -1220: 14, 964: 14, -14335: 1, 15478: 1, 78: 1, 3005: 14, 18561: 1, 15475: 1, 15473: 1, -3008: 35, 3008: 23, 18562: 3, 18563: 9, 15472: 1, 15469: 1, 703: 2, -14851: 3, -14850: 31, 15467: 1, 87: 1, -14849: 2, 15466: 1}\n","Train各delta值： [0, 15703, -15703, -256, -15447, 256, -15446, 15702, 255, -199, -15247, 1, -15445, 15701, -201, -15245, 15488, -15488, -203, -15242, 213, 15700, -15444, -15701, -206, -15239, 212, -207, -15237, -15443, 15699, -15700, -210, -15233, 15698, -15442, 210, 211, -212, -15486, -15441, 15697, -15698, -470, -15484, -15697, 209, -216, -15481, -15440, 15696, -219, -15477, -15439, 15695, -15696, -220, -15475, 208, 207, -223, -15472, 15694, -15438, 206, -15695, -225, -15469, 15693, -15437, -15693, -1, -227, -15466, 205, 15692, -15436, -230, -15463, -232, -15460, 203, -15435, 204, 15691, -234, -15458, 257, -15692, -15434, -236, -15455, 15690, -15690, -239, -15451, 202, 15689, -15433, -240, -15449, -15432, 15688, -243, -15190, 200, -15689, -15688, -245, -15187, -15431, 15687, -15687, -247, -15184, 15686, -15430, -249, 198, -252, -15178, 15685, -15429, -254, 197, 196, -15428, 15684, -15685, -512, 512, -15173, -15684, -192, -15492, -15427, 15683, -259, -15169, 195, 15168, -15168, -260, 320, -15487, -15426, 15682, -15683, -263, 194, -15425, -265, 15681, -267, -15478, 15680, -15424, -15681, 15424, 258, 193, 253, -15678, 192, 252, -15676, -15680, -15679, -15423, -7, -15672, 15679, 254, -9, -15669, -11, -15667, -15677, 15677, 15678, -14, -15664, -15485, -16, -15660, -18, -15658, -15675, 251, 15676, -20, -15655, -15674, 250, -23, -15652, -15482, -24, -15650, 249, -15673, 15674, -27, -15646, 248, -29, -15643, 15672, -31, -15641, 247, -15671, -15480, -33, -15638, -15670, 246, 15671, 15670, -36, -15634, 245, -38, -15632, 15669, 15104, 565, -15104, 182, -40, -15629, 244, -15668, 15668, -43, -15626, -15476, -44, -15624, 243, -47, -15620, 242, -15666, -49, -15617, -15474, -576, -15090, 15666, -15665, 241, -15473, -51, 15665, -53, -15420, 240, -56, -15416, 15664, -15663, 239, -57, -15414, -60, -15411, 238, -15662, 15662, -62, -15408, -15470, 237, -15661, -15406, -15597, -15405, 236, -67, -15594, 15661, -69, -15591, 235, -15659, -71, -15589, -15468, -15467, 15659, -73, -15586, 234, -76, -15582, 233, -15657, -77, -15580, -15465, -15656, 232, -80, -15577, 15657, -82, -15574, 231, 15656, 15655, -84, -15571, 230, -15654, -86, -15568, -15462, 15040, 614, 15654, -89, -15565, 229, -15653, -91, -15563, 228, -93, -15559, -15651, 227, -96, -15556, -97, -15554, 15651, 226, -100, -128, 15650, -15649, -102, 225, -104, -15417, -15457, 15649, 14784, 865, -14784, -14783, 224, -15648, -106, 223, -15647, -109, 863, -110, -15409, 222, -113, -640, -15006, 640, -15454, -15453, 221, -15645, -115, -15402, 15645, -117, -15400, -15644, 220, -120, -15397, 219, -122, -15393, -124, -15391, -15642, 218, -126, -15388, -15450, 217, 15641, -15385, -129, -15513, 15642, -130, -15511, 216, -15640, -133, -15507, -15639, 215, -135, -15504, 15639, 14976, 663, -14976, -137, -15502, 214, -139, -15499, -15637, -2, -15636, -4, -15633, 15637, -15630, 15636, -8, -15628, -15635, -13, -15621, -15, -15619, 15633, -17, -15631, -21, -15418, -15415, -26, -15412, -28, -15403, -15627, -35, -15401, -704, -14923, 704, 15627, -37, -15398, -15394, 201, -15625, -41, -15392, -18477, -1242, 19520, 199, 2852, -18278, 18480, -18480, 2854, -18279, 2855, -15390, 15625, 14912, 714, 15626, -45, -15386, -15384, -19528, 19728, -754, -14670, 191, -52, -54, -15378, 15624, -15623, -59, -15373, 15623, -15369, 15622, -15622, -63, -15367, -15366, -66, 15621, -68, -15553, 15620, -70, -15551, 15619, -72, -15547, -75, -15544, 15618, -15618, -15542, 15617, -79, -15539, 15616, -15616, -15535, -83, -15533, 15615, -15615, -15530, -88, -15527, 15614, -15614, -15422, -90, -15525, -15613, 15613, -92, -15521, 15612, -15612, -95, -15518, 14848, -14848, 765, -14847, -15516, -15611, 15611, -99, -15419, 15610, -15610, -101, -15509, -103, -15609, 15609, 15360, -15360, -15359, 15608, -15608, -108, -15501, -15361, -15498, -15607, -112, -15495, 15607, -15351, -15606, 15606, -19546, 19584, -116, -15490, -119, -15605, -15356, -15413, -123, -15354, -15349, -131, -15604, -136, -19570, -3, -15602, -6, -15599, -15596, -15593, -15603, 15604, 13, -19586, 19830, -22249, 22016, 10, -22026, -15365, 15602, -15601, -15579, 15601, 15296, -15296, 306, -15410, -15575, -15600, -30, 15600, -32, 15599, -15564, -15407, -39, -15560, -320, -15279, -15598, -15558, 15598, -1087, -14511, -48, -15357, -15350, -15595, -58, -15346, 15596, -15343, -61, -15342, -15341, -15340, -15339, -64, -15338, -65, -15529, 24, -15528, 22, -15382, 15594, -15526, -15524, -15523, -15522, -15592, -15520, 15592, -15519, -74, -15515, 15591, 15232, -15232, 360, -15231, -15512, -15590, 83, -2202, 2240, -2246, -2262, -18014, 4, -25, 18048, -18026, -19699, 19712, -18243, -18068, -18049, -2229, -2260, -2221, -18047, -2251, -2222, -2227, -2231, -2240, -2238, -2263, -2219, -18038, -18046, -22, -18027, -3871, 21888, -21879, -15372, -18013, -2268, -18037, -18052, -2245, -18055, -2249, -2259, -18025, -19418, 19392, 32, -2235, -18035, -2255, -19391, -18029, -18053, -2230, -18050, -2241, -2243, -18033, -15759, -18063, -21896, -2244, -2217, -18036, -2250, -2220, -2237, -2264, -18040, 3, 15794, -15368, -19490, 19460, -2218, -2261, 2, -18028, 15796, -18012, -2248, -2242, -2247, -18054, -18031, -18070, -2254, -18034, -2236, 2651, -18305, 18304, -18051, -15383, -2213, -2257, -15757, 5, -18044, -2223, -2252, -2228, -2253, -2224, -15387, -18011, -18066, -18015, -18061, -15380, -18306, -18284, -18042, -18067, -18032, -18048, -19648, 19619, -19433, 646, 18816, -19462, 19456, -18787, -15389, -18783, 3423, -19701, 19899, 15590, -6723, 22084, -22084, 22113, -18065, -22095, -22116, 22092, -22090, 22087, -22086, -22087, 22090, -22091, 22085, -22085, 22086, 22100, -22100, 22144, -22144, -22097, -10, 22097, 22094, -22094, -22092, 22105, 17, -15377, 145, -15505, 28, 18259, -194, 36, -15396, 15589, -229, 15502, 88, 38, 37, 139, 15359, -18009, 2649, -15588, 15588, -228, -18019, -15587, -18023, -15395, 15587, 35, -15379, -163, -15331, 15586, -226, -18039, 34, -18043, -15362, 15585, -15585, 2689, 18279, 33, 15394, -34, -18056, 2696, -15584, -2699, 2700, 15584, 18288, 2706, 2710, 288, -15583, 15583, -18073, 18297, -18076, 2716, -18080, 2720, -2724, 18307, -22168, 22391, -2289, 2512, 15582, -22401, -600, 2827, -980, 980, -982, 20928, -22173, 1246, -22174, 6808, -18085, 2725, -20924, 21147, -18087, 18310, -2729, 2729, -15581, -21150, -2731, 2731, -20170, -22172, 6814, 15581, -18093, 18315, -18095, 2735, -2737, 18318, 15580, -2739, 2739, -2741, 2741, -2742, 2742, -2744, 2744, -4415, 19995, -2746, 18326, -15260, -4429, 19776, -18328, 18112, -323, 323, -195, 324, -324, -196, -467, 467, 468, -468, -994, 994, -768, -162, -832, 768, 995, -995, -1930, 2157, -237, -1920, -2157, -173, -1984, -1925, 1920, -2158, 2158, -238, -18096, 18334, -222, -18112, -18334, -20200, 19968, -21982, 792, 1416, -19774, 1672, 2746, -2784, 15832, 2304, -19770, 4429, 13048, -15352, -4183, 4183, 12996, 2368, -2316, -18530, 16200, -15777, -18133, -3060, 21248, -18620, 18560, -18560, -2628, -19830, -1418, 3060, 15772, -2763, -2327, -17450, 8, -18892, 18880, -19772, 892, -15344, -19502, 4142, 13034, -13034, -4431, -15345, -19793, 4433, -19794, -4435, 19840, 177, -4436, 20017, -4425, 20007, -4196, 19549, -15353, 15578, -15578, 15577, 15576, -15576, 15574, -15573, 15572, -15572, -15570, -15324, 15569, -15569, -15323, 15579, 15568, 15566, -15566, 15563, 128, 15562, -15562, 15559, -15322, 15558, 15556, 15555, -15555, 15554, 15552, -15552, 15551, 15550, -15550, -15321, 15549, -15549, 187, 186, -15546, 15347, -15347, 15545, -15545, 15543, -15543, 15542, 15541, 15539, -15320, 15538, -15538, 15537, -15537, 152, 175, -15534, 173, -15319, 15532, -15532, 15575, 15531, -15531, 15529, 15528, 15527, 165, 15524, 15523, 15521, -15318, 15520, 15519, 15518, 15516, 15515, 15514, -15514, 15512, 151, 15573, -15317, 15510, -15510, 15508, -15316, 15507, -15315, 146, -15314, 15504, -15312, -15381, 15571, 15567, -15375, 277, -15308, -19200, 19199, 3627, -15307, -15306, 129, 15560, -15302, 15557, -15301, -15299, -15298, 15553, -15297, 276, -15295, 15547, -15355, 15546, 15570, 183, -3648, 19218, 3648, 181, -15541, 15540, -15540, 12928, -12928, 15536, -15536, 15535, 15533, -15313, 171, -1344, 1344, 15526, -15334, 164, 15522, -15311, -15567, 15509, -15508, 15506, -15506, 15505, -15310, 142, -15309, 271, 15500, 138, 15497, -15305, 15565, 15496, -15304, -19264, 19264, 19265, 15495, -15303, 15493, 132, -15300, -15374, 15564, 15561, -15561, 269, -15371, -15370, -14912, 169, -15337, -20117, 1856, -2836, 21120, -15328, 15525, -5796, -2470, 2496, -20122, -2838, -22365, 22400, -22399, 6, -22406, -22352, -22402, -22404, 15517, 43, 15513, 264, -384, 384, 15511, 49, -19968, 4408, 390, 391, -15167, 15503, -15503, 57, -4416, 4416, -15500, -15497, 62, -2368, -13190, -15557, -13189, 15494, -15494, -197, 15491, -4672, 20229, 4672, 15490, 13184, -13184, 2373, -13183, 261, 15485, 71, 15484, -1216, 1216, 15482, 14336, -14336, 1143, 76, 1219, 1220, 15300, -964, -1220, 964, -14335, 15478, 78, 3005, 18561, 15475, 15473, -3008, 3008, 18562, 18563, 15472, 15469, 703, -14851, -14850, 15467, 87, -14849, 15466]\n","Train各delta值的次数： [33074, 32, 1, 16200, 28, 9467, 161, 163, 354, 1, 1, 519, 162, 163, 1, 1, 198, 82, 1, 1, 263, 146, 147, 1, 1, 1, 162, 1, 1, 166, 149, 3, 2, 1, 162, 135, 193, 196, 6, 7, 133, 148, 24, 1, 4, 16, 135, 1, 3, 154, 171, 1, 4, 161, 160, 19, 1, 4, 185, 173, 40, 3, 132, 148, 193, 1, 61, 9, 156, 166, 2, 318, 159, 7, 200, 155, 162, 68, 1, 1, 1, 166, 153, 194, 150, 1, 2, 338, 1, 169, 1, 3, 167, 1, 1, 1, 227, 167, 161, 1, 3, 148, 145, 1, 1, 173, 1, 1, 1, 1, 162, 170, 8, 1, 1, 161, 164, 1, 16, 1, 1, 146, 127, 1, 10, 23, 148, 147, 18, 3, 1, 1, 16, 956, 5, 159, 154, 1, 1, 14, 64, 46, 1, 30, 39, 170, 168, 1, 1, 2, 277, 1, 243, 1, 2, 112, 6646, 7, 15783, 5, 13, 139, 157, 451, 167, 168, 71, 131, 105, 3, 161, 21, 165, 2, 161, 4, 166, 135, 10, 1, 2, 173, 15, 2, 160, 1, 165, 161, 164, 1, 4, 171, 147, 155, 3, 168, 2, 3, 150, 193, 169, 1, 1, 137, 223, 5, 169, 3, 2, 143, 250, 147, 1, 2, 250, 168, 460, 1, 12, 1, 160, 588, 1, 174, 22, 2, 1, 1, 2, 2, 168, 384, 142, 1, 2, 207, 8, 2, 189, 348, 3, 85, 309, 129, 2, 83, 29, 1, 1, 27, 143, 329, 7, 3, 2, 1, 4, 337, 2, 2, 30, 162, 336, 2, 12, 2, 3, 300, 138, 12, 5, 1, 1, 342, 140, 17, 155, 37, 334, 4, 149, 1, 3, 129, 320, 112, 3, 239, 2, 37, 1, 3, 192, 345, 2, 135, 347, 153, 3, 123, 8, 148, 301, 2, 11, 2, 3, 36, 287, 1, 5, 1, 12, 286, 163, 2, 26, 1, 1, 1, 27, 1, 28, 360, 146, 1, 18, 355, 1, 19, 160, 355, 2, 24, 1, 4, 15, 356, 1, 22, 3, 167, 1, 374, 1, 1, 2, 8, 58, 23, 22, 1, 418, 161, 2, 316, 146, 1, 12, 2, 7, 248, 1, 11, 9, 2, 24, 3, 193, 159, 2, 1, 1, 1, 2, 150, 170, 1, 19, 186, 2, 25, 1, 2, 159, 168, 2, 3, 2, 160, 1, 9, 2, 3, 2, 1, 3, 193, 170, 2, 2, 168, 160, 1, 3, 21, 8, 5, 3, 1, 2, 275, 1, 2, 224, 3, 149, 1, 150, 1, 147, 3, 2, 160, 166, 3, 89, 1, 100, 31, 2, 162, 5, 1, 19, 2, 2, 1, 1, 131, 2, 1, 84, 14, 32, 3, 1, 5, 43, 288, 280, 2, 2, 2, 1, 1, 15, 1, 2, 3, 3, 2, 1, 1, 1, 7, 2, 1, 3, 1, 2, 6, 1, 1, 1, 1, 7, 1, 2, 2, 45, 83, 1, 4, 70, 2, 93, 81, 4, 2, 17, 3, 89, 3, 1, 81, 4, 4, 101, 4, 3, 1, 1, 89, 87, 4, 84, 1, 3, 101, 100, 2, 2, 4, 89, 86, 2, 1, 5, 83, 81, 1, 1, 5, 100, 101, 1, 5, 91, 85, 1, 4, 5, 3, 1, 1, 4, 84, 81, 1, 1, 95, 99, 1, 4, 1, 87, 70, 14395, 9238, 161, 21, 80, 1, 1, 18, 1, 92, 1, 1, 12, 3, 288, 3, 1, 2, 1, 1, 1, 436, 1, 5, 1, 3, 2, 1, 238, 1, 1, 2, 179, 2, 168, 171, 170, 179, 2, 1, 1, 1, 1, 2, 3, 1, 1, 6, 175, 21, 1, 280, 225, 5, 1, 13, 209, 1, 17, 1, 3, 13, 5, 1, 37, 35, 4, 170, 31, 20, 1, 1, 3, 1, 1, 162, 1, 2, 1, 3, 2, 4, 3, 2, 3, 2, 15, 3, 5, 2, 4, 1, 7, 1, 2, 4, 4, 3, 153, 5, 1, 4, 2, 3, 4, 6, 3, 2, 1, 3, 174, 2, 1, 123, 5, 2, 1, 2, 1, 110, 2, 1, 1, 1, 2, 10, 2, 4, 8, 3, 4, 1, 1, 1, 8, 3, 2, 4, 4, 7, 2, 3, 1, 3, 1, 3, 5, 1, 2, 7, 8, 3, 5, 3, 5, 1, 3, 1, 4, 6, 2, 2, 4, 2, 2, 10, 4, 6, 4, 1, 2, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 4, 1, 3, 2, 4, 2, 1, 2, 2, 1, 2, 2, 1, 2, 4, 1, 3, 1, 3, 1, 3, 2, 3, 2, 1, 1, 1, 6, 2, 2, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 2, 6, 3, 1, 1, 96, 1, 3, 3, 2, 1, 1, 1, 2, 2, 3, 5, 1, 2, 1, 4, 4, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 16, 39, 154, 123, 2, 2, 5, 8, 1, 7, 1, 1, 199, 150, 118, 1, 198, 1, 33, 154, 14, 5, 1, 13, 139, 148, 1, 15, 1, 1, 92, 258, 1, 1, 3, 1, 1, 1, 1, 307, 1, 1, 53, 1, 1, 1, 2, 221, 73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54, 2, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 206, 1, 1, 1, 1, 1, 1, 170, 1, 1, 1, 1, 1, 1, 125, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 7, 1, 3, 22, 22, 1, 33, 30, 2, 32, 31, 32, 26, 14, 30, 45, 1, 1, 3, 31, 4, 1, 31, 19, 39, 11, 1, 1, 1, 3, 11, 30, 22, 1, 28, 3, 3, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 14, 29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 1, 1, 1, 1, 1, 1, 2, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 1, 121, 10, 143, 78, 146, 153, 140, 15, 118, 1, 1, 3, 3, 3, 69, 3, 3, 1, 1, 2, 1, 1, 3, 2, 2, 3, 2, 2, 2, 73, 2, 1, 2, 3, 2, 2, 1, 1, 71, 2, 2, 67, 2, 3, 2, 2, 2, 1, 1, 3, 2, 104, 3, 2, 2, 2, 3, 2, 2, 3, 1, 89, 95, 2, 2, 3, 82, 1, 74, 1, 70, 2, 60, 1, 76, 87, 1, 5, 73, 3, 2, 1, 96, 137, 1, 126, 117, 137, 126, 134, 95, 14, 11, 8, 3, 2, 1, 2, 63, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 84, 1, 7, 7, 2, 2, 1, 1, 84, 16, 2, 2, 1, 1, 2, 113, 1, 101, 4, 2, 1, 2, 108, 105, 2, 130, 4, 3, 1, 1, 146, 1, 1, 142, 2, 66, 143, 43, 7, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 9, 2, 1, 1, 2, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 2, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 3, 39, 1, 1, 1, 1, 1, 10, 2, 8, 1, 59, 43, 15, 1, 8, 1, 1, 1, 5, 5, 1, 41, 24, 1, 1, 13, 4, 2, 2, 14, 14, 1, 1, 1, 14, 1, 1, 1, 35, 23, 3, 9, 1, 1, 2, 3, 31, 1, 1, 2, 1]\n","Train_delta种类： 1304\n","Test字典收集训练集（delta类别-次数）： {15554: 72, -256: 1669, 256: 1329, -15297: 230, 0: 9111, -15298: 76, 15360: 80, -15360: 65, -15554: 5, 15555: 1, 255: 122, 1: 177, 15463: 1, -15463: 1, 15553: 202, -15553: 18, 257: 37, 194: 6, 193: 18, -192: 517, -15362: 1, 15461: 1, 92: 1, -15361: 1, 15460: 1, -15460: 1, -1: 150, -1472: 1, -14081: 1, 15457: 1, -15457: 1, 15552: 44, -15296: 8332, 254: 153, 15296: 8974, 258: 8, -15295: 135, 15550: 7, 3: 6, -576: 64, 576: 40, 575: 1, -14978: 1, -14977: 21, 577: 1, 15548: 19, 5: 4, 12765: 1, 2560: 1, 228: 2, -7115: 1, 22464: 2, -48: 1, 252: 112, 15547: 4, -15547: 91, -15552: 65, 15546: 37, -15546: 104, -4704: 1, 20032: 1, 224: 3, 15350: 1, 203: 2, 15545: 396, -15545: 171, 15544: 127, -15544: 112, 15543: 483, -15543: 174, -7116: 1, 205: 2, 15542: 175, -15542: 126, 15541: 69, 12: 1, 244: 118, 13: 2, 178: 3, 14: 2, 15538: 110, 15: 1, 15536: 3, 16: 1, 15535: 1, 17: 2, 22: 2, 234: 3, 15534: 1, 18: 1, 15533: 1, 19: 1, 192: 346, -15551: 95, 236: 2, 20: 2, 235: 2, 21: 2, -19780: 1, 19840: 7, 196: 1, 233: 1, -15529: 1, 232: 2, -15528: 2, 231: 2, -15527: 1, 230: 2, -15526: 2, 229: 2, -15525: 2, 15551: 3, -15524: 1, -15359: 4, 227: 3, -15522: 3, 226: 3, -19815: 1, -15321: 2, -15520: 1, 223: 2, -15519: 2, 222: 2, -15518: 1, -15550: 155, 221: 2, -15517: 2, 4: 4, -15300: 1, 46: 1, -15342: 4, 220: 3, -15516: 1, 219: 2, -15515: 2, 218: 2, -15514: 2, 217: 2, -15513: 1, 216: 4, -15512: 2, 215: 1, -15511: 1, 213: 4, -15509: 2, 211: 1, -15507: 1, 210: 3, -15506: 1, 209: 3, -15505: 2, 208: 1, -15504: 1, -15358: 3, 207: 3, -15503: 2, 206: 1, -15502: 1, -15501: 1, 204: 1, -15500: 1, -15549: 177, -15499: 1, 201: 1, -15497: 1, 253: 180, 200: 5, -15496: 2, 199: 2, -15495: 1, -15357: 12, 198: 1, -15494: 1, 197: 2, -15493: 2, 15492: 1, -15492: 1, 195: 1, -15491: 1, -15490: 1, -15489: 1, 191: 7, 15488: 1, 190: 2, -15294: 3, 189: 1, -15292: 12, 187: 1, -15483: 1, -15548: 118, -2: 4, 2: 7, -2597: 1, 2624: 1, -21485: 1, 21248: 2, 30: 2, -15343: 2, -20250: 1, 20288: 1, 251: 88, 248: 99, -19834: 1, -20075: 1, 20096: 1, 15549: 20, -253: 14, -21236: 1, -22493: 1, 22528: 20, 15232: 11, -15232: 8, -15231: 2, 246: 102, 7: 2, -252: 9, 9: 1, 242: 76, -15537: 94, 239: 1, -15293: 2, 237: 1, -4: 1, 15292: 1, -15532: 1, -15530: 1, -15355: 1, 24: 1, 225: 1, -15329: 1, 27: 2, 250: 91, 34: 1, 214: 1, -15510: 1, 37: 1, 249: 144, 212: 1, -15508: 1, -250: 33, -249: 332, -20292: 1, -982: 1, 21312: 2, -22556: 1, -22542: 1, -22532: 1, -22531: 1, 8: 3, -22536: 3, -6: 3, -22522: 2, 40: 1, -15353: 45, -21527: 1, -15330: 1, -2650: 2, 2688: 122, -2709: 5, -2687: 4, -2686: 8, -18466: 2, 18496: 131, -18493: 4, -18520: 7, -18474: 4, -18516: 4, -18478: 2, -18517: 3, -18496: 7, -20149: 1, 20160: 6, -2656: 2, -2690: 11, -18505: 1, -18497: 10, -2691: 3, -2669: 5, -2688: 7, -18484: 2, -2698: 9, -19863: 1, -19844: 1, -18463: 2, -18508: 1, -18482: 2, -15338: 3, -2703: 3, -18473: 2, -2699: 3, -2668: 1, -2685: 3, -2712: 4, -2664: 2, -2689: 2, -2711: 1, -2666: 4, -18487: 4, -18472: 4, -18518: 2, -18475: 4, -2722: 1, -15313: 1, -2648: 1, -21: 2, -2667: 2, -18495: 4, -18476: 4, -18498: 9, -2696: 1, -18488: 3, 15785: 1, -2675: 2, -15796: 1, -15328: 1, -2658: 1, -18486: 8, -18509: 5, -18481: 3, -2701: 2, -2673: 4, -2707: 4, -19929: 1, 17228: 1, -2706: 1, -2660: 3, -10: 2, -2681: 1, -18: 2, -12: 1, -15336: 4, -18456: 2, -18454: 2, -18494: 1, -19873: 1, -19845: 1, 50: 5, -18511: 1, -18483: 1, -30: 2, -15306: 1, -18455: 1, -20: 1, -18489: 1, -18502: 2, -18477: 1, -15323: 1, -22356: 1, 22336: 5, 176: 1, -23385: 1, 23232: 1, -36: 1, -15341: 1, -20110: 1, -2708: 1, -15778: 1, -20138: 1, -18725: 1, -2697: 1, -22328: 1, -22005: 1, 22016: 4, -21986: 1, -22226: 1, 22208: 1, -21998: 2, -18500: 1, -2679: 1, -22366: 1, -17: 1, -22319: 1, -22336: 1, -15324: 2, -19876: 1, 19904: 1, -20139: 1, -20111: 1, -2651: 1, -2713: 2, -2662: 1, -18462: 1, -18499: 1, -18523: 2, -18491: 1, 18718: 1, -18705: 1, -2695: 1, -18460: 1, -18506: 1, -18749: 1, 18752: 1, -15332: 1, -15318: 1, -18450: 1, -18507: 1, -2700: 2, -15319: 2, -18485: 2, -2678: 1, -2682: 2, -2704: 1, -15810: 1, -2663: 1, -2665: 1, -18492: 1, -18521: 1, -1338: 2, 1344: 3, -15302: 4, -1587: 1, -4827: 1, -22521: 2, -7: 1, -22527: 1, 49: 4, -22584: 1, 22530: 1, -22530: 1, 23: 2, -22551: 2, 35: 1, -22563: 1, -22528: 1, -22529: 1, 55: 50, -22583: 1, 2901: 1, -2898: 1, -23: 1, -15344: 1, -18479: 1, -15333: 2, 42: 1, -18459: 1, 12624: 1, 3153: 1, 186: 1, -185: 4, 57: 20, -248: 104, -2915: 1, -15309: 1, -15352: 24, 15295: 5, 56: 10, -18736: 1, -2717: 1, 2752: 9, -2733: 1, -12606: 3, 13946: 4, -13946: 5, -1350: 1, -1351: 4, -13945: 6, -1599: 1, -1600: 1, 13944: 1, -13944: 1, -1352: 1, 1600: 1, -184: 1, 15352: 1, -56: 1, -1408: 1, 1408: 4, -15351: 81, -2661: 1, -18740: 1, 15796: 1, 3176: 1, -2676: 1, -15308: 2, -2694: 2, 168: 1, 247: 122, 183: 1, 184: 1, -247: 386, -18734: 1, -15304: 1, -2754: 1, -12604: 1, -15287: 5, -246: 133, -2923: 1, -2684: 1, 3206: 1, -3185: 1, -15474: 1, -182: 5, -15350: 19, 311: 1, 15231: 1, 65: 1, -183: 4, -9: 1, -15541: 69, 175: 1, -15471: 1, 15351: 2, -55: 1, 15467: 1, -15467: 1, 66: 1, 180: 1, 64: 1, -64: 1, 182: 1, -1442: 1, -1378: 2, -15326: 2, 18560: 6, 15808: 1, -18549: 1, 155: 1, -15459: 1, 245: 68, -245: 51, -15349: 31, 53: 18, 54: 3, 15540: 143, -244: 135, -15540: 110, -2952: 1, -2705: 1, -2753: 1, -18541: 1, 3208: 1, -18513: 1, -18560: 2, 15294: 2, -15348: 24, 15349: 2, -53: 2, -15285: 7, 52: 7, -18775: 1, -2732: 1, -15346: 10, 18672: 1, -15448: 1, 15539: 101, -243: 91, -15539: 101, 243: 104, -15347: 9, 51: 1, -15538: 113, -242: 67, -178: 1, -14: 1, -179: 1, 15431: 1, 107: 1, 241: 82, 15537: 87, -241: 71, -2734: 1, -15314: 1, -15345: 11, -15536: 3, -20201: 1, 4905: 1}\n","Test各delta值： [15554, -256, 256, -15297, 0, -15298, 15360, -15360, -15554, 15555, 255, 1, 15463, -15463, 15553, -15553, 257, 194, 193, -192, -15362, 15461, 92, -15361, 15460, -15460, -1, -1472, -14081, 15457, -15457, 15552, -15296, 254, 15296, 258, -15295, 15550, 3, -576, 576, 575, -14978, -14977, 577, 15548, 5, 12765, 2560, 228, -7115, 22464, -48, 252, 15547, -15547, -15552, 15546, -15546, -4704, 20032, 224, 15350, 203, 15545, -15545, 15544, -15544, 15543, -15543, -7116, 205, 15542, -15542, 15541, 12, 244, 13, 178, 14, 15538, 15, 15536, 16, 15535, 17, 22, 234, 15534, 18, 15533, 19, 192, -15551, 236, 20, 235, 21, -19780, 19840, 196, 233, -15529, 232, -15528, 231, -15527, 230, -15526, 229, -15525, 15551, -15524, -15359, 227, -15522, 226, -19815, -15321, -15520, 223, -15519, 222, -15518, -15550, 221, -15517, 4, -15300, 46, -15342, 220, -15516, 219, -15515, 218, -15514, 217, -15513, 216, -15512, 215, -15511, 213, -15509, 211, -15507, 210, -15506, 209, -15505, 208, -15504, -15358, 207, -15503, 206, -15502, -15501, 204, -15500, -15549, -15499, 201, -15497, 253, 200, -15496, 199, -15495, -15357, 198, -15494, 197, -15493, 15492, -15492, 195, -15491, -15490, -15489, 191, 15488, 190, -15294, 189, -15292, 187, -15483, -15548, -2, 2, -2597, 2624, -21485, 21248, 30, -15343, -20250, 20288, 251, 248, -19834, -20075, 20096, 15549, -253, -21236, -22493, 22528, 15232, -15232, -15231, 246, 7, -252, 9, 242, -15537, 239, -15293, 237, -4, 15292, -15532, -15530, -15355, 24, 225, -15329, 27, 250, 34, 214, -15510, 37, 249, 212, -15508, -250, -249, -20292, -982, 21312, -22556, -22542, -22532, -22531, 8, -22536, -6, -22522, 40, -15353, -21527, -15330, -2650, 2688, -2709, -2687, -2686, -18466, 18496, -18493, -18520, -18474, -18516, -18478, -18517, -18496, -20149, 20160, -2656, -2690, -18505, -18497, -2691, -2669, -2688, -18484, -2698, -19863, -19844, -18463, -18508, -18482, -15338, -2703, -18473, -2699, -2668, -2685, -2712, -2664, -2689, -2711, -2666, -18487, -18472, -18518, -18475, -2722, -15313, -2648, -21, -2667, -18495, -18476, -18498, -2696, -18488, 15785, -2675, -15796, -15328, -2658, -18486, -18509, -18481, -2701, -2673, -2707, -19929, 17228, -2706, -2660, -10, -2681, -18, -12, -15336, -18456, -18454, -18494, -19873, -19845, 50, -18511, -18483, -30, -15306, -18455, -20, -18489, -18502, -18477, -15323, -22356, 22336, 176, -23385, 23232, -36, -15341, -20110, -2708, -15778, -20138, -18725, -2697, -22328, -22005, 22016, -21986, -22226, 22208, -21998, -18500, -2679, -22366, -17, -22319, -22336, -15324, -19876, 19904, -20139, -20111, -2651, -2713, -2662, -18462, -18499, -18523, -18491, 18718, -18705, -2695, -18460, -18506, -18749, 18752, -15332, -15318, -18450, -18507, -2700, -15319, -18485, -2678, -2682, -2704, -15810, -2663, -2665, -18492, -18521, -1338, 1344, -15302, -1587, -4827, -22521, -7, -22527, 49, -22584, 22530, -22530, 23, -22551, 35, -22563, -22528, -22529, 55, -22583, 2901, -2898, -23, -15344, -18479, -15333, 42, -18459, 12624, 3153, 186, -185, 57, -248, -2915, -15309, -15352, 15295, 56, -18736, -2717, 2752, -2733, -12606, 13946, -13946, -1350, -1351, -13945, -1599, -1600, 13944, -13944, -1352, 1600, -184, 15352, -56, -1408, 1408, -15351, -2661, -18740, 15796, 3176, -2676, -15308, -2694, 168, 247, 183, 184, -247, -18734, -15304, -2754, -12604, -15287, -246, -2923, -2684, 3206, -3185, -15474, -182, -15350, 311, 15231, 65, -183, -9, -15541, 175, -15471, 15351, -55, 15467, -15467, 66, 180, 64, -64, 182, -1442, -1378, -15326, 18560, 15808, -18549, 155, -15459, 245, -245, -15349, 53, 54, 15540, -244, -15540, -2952, -2705, -2753, -18541, 3208, -18513, -18560, 15294, -15348, 15349, -53, -15285, 52, -18775, -2732, -15346, 18672, -15448, 15539, -243, -15539, 243, -15347, 51, -15538, -242, -178, -14, -179, 15431, 107, 241, 15537, -241, -2734, -15314, -15345, -15536, -20201, 4905]\n","Test各delta值的次数： [72, 1669, 1329, 230, 9111, 76, 80, 65, 5, 1, 122, 177, 1, 1, 202, 18, 37, 6, 18, 517, 1, 1, 1, 1, 1, 1, 150, 1, 1, 1, 1, 44, 8332, 153, 8974, 8, 135, 7, 6, 64, 40, 1, 1, 21, 1, 19, 4, 1, 1, 2, 1, 2, 1, 112, 4, 91, 65, 37, 104, 1, 1, 3, 1, 2, 396, 171, 127, 112, 483, 174, 1, 2, 175, 126, 69, 1, 118, 2, 3, 2, 110, 1, 3, 1, 1, 2, 2, 3, 1, 1, 1, 1, 346, 95, 2, 2, 2, 2, 1, 7, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 3, 1, 4, 3, 3, 3, 1, 2, 1, 2, 2, 2, 1, 155, 2, 2, 4, 1, 1, 4, 3, 1, 2, 2, 2, 2, 2, 1, 4, 2, 1, 1, 4, 2, 1, 1, 3, 1, 3, 2, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 177, 1, 1, 1, 180, 5, 2, 2, 1, 12, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 7, 1, 2, 3, 1, 12, 1, 1, 118, 4, 7, 1, 1, 1, 2, 2, 2, 1, 1, 88, 99, 1, 1, 1, 20, 14, 1, 1, 20, 11, 8, 2, 102, 2, 9, 1, 76, 94, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 91, 1, 1, 1, 1, 144, 1, 1, 33, 332, 1, 1, 2, 1, 1, 1, 1, 3, 3, 3, 2, 1, 45, 1, 1, 2, 122, 5, 4, 8, 2, 131, 4, 7, 4, 4, 2, 3, 7, 1, 6, 2, 11, 1, 10, 3, 5, 7, 2, 9, 1, 1, 2, 1, 2, 3, 3, 2, 3, 1, 3, 4, 2, 2, 1, 4, 4, 4, 2, 4, 1, 1, 1, 2, 2, 4, 4, 9, 1, 3, 1, 2, 1, 1, 1, 8, 5, 3, 2, 4, 4, 1, 1, 1, 3, 2, 1, 2, 1, 4, 2, 2, 1, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 3, 4, 1, 1, 2, 1, 1, 4, 1, 1, 1, 2, 2, 1, 1, 1, 1, 50, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 4, 20, 104, 1, 1, 24, 5, 10, 1, 1, 9, 1, 3, 4, 5, 1, 4, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 81, 1, 1, 1, 1, 1, 2, 2, 1, 122, 1, 1, 386, 1, 1, 1, 1, 5, 133, 1, 1, 1, 1, 1, 5, 19, 1, 1, 1, 4, 1, 69, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 6, 1, 1, 1, 1, 68, 51, 31, 18, 3, 143, 135, 110, 1, 1, 1, 1, 1, 1, 2, 2, 24, 2, 2, 7, 7, 1, 1, 10, 1, 1, 101, 91, 101, 104, 9, 1, 113, 67, 1, 1, 1, 1, 1, 82, 87, 71, 1, 1, 11, 3, 1, 1]\n","Test_delta种类： 566\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3A3rY5iy3X4Z","executionInfo":{"status":"ok","timestamp":1632655878417,"user_tz":-480,"elapsed":1121,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"e4d24597-07d2-4e2c-bf7d-a2a0f1e3559f"},"source":["from random import randint\n","#统计delta计数较少的index，考虑归为新类\n","#字典收集训练集的delta值\n","value_cnt = {}\n","for delta in list_a:\n","\t# get(value, num)函数的作用是获取字典中value对应的键值, num=0指示初始值大小。\n","\tvalue_cnt[delta] = value_cnt.get(delta, 0) + 1\n","\n","# 打印输出结果\n","print(\"字典收集训练集（delta类别-次数）：\",value_cnt)\n","print(\"各delta值：\",[key for key in value_cnt.keys()])\n","Vals=[delta for delta in value_cnt.values()]\n","print(\"各delta值的次数：\",Vals)\n","print(\"delta种类：\",len(Vals))\n","\n","Vals_Record=[delta for delta in value_cnt.keys() if value_cnt[delta] >5] #要保留记录的delta\n","Vals_Thr_Num=[delta for delta in Vals if delta >5] #要保留记录的delta次数\n","Vals_Thr_Dele=[delta for delta in value_cnt.keys() if value_cnt[delta] <=5] #次数小于等于10次的delta，归成一类\n","Vals_Thr_Dele_Num=[delta for delta in Vals if delta <=5] #要归为一类记录的delta次数\n","print(\"要保留的delta：\",Vals_Record)\n","print(\"要保留的delta次数：\",Vals_Thr_Num)\n","print(\"要保留的delta种类数：\",len(Vals_Thr_Num))\n","#将不保留记录的delta都归为一个新类\n","type_new=len(Vals_Thr_Num)+1\n","\n","print(\"要删除的delta：\",Vals_Thr_Dele)\n","print(\"要删除的delta次数：\",Vals_Thr_Num)\n","print(\"要删除的delta种类：\",len(Vals_Thr_Dele))\n","\n","#设置一个新类，统归为初始值0，利用随机生成一个0-type_new之间的数，作为最终新类值\n","init_val=0\n","while init_val in Vals_Record:\n","  init_val=randint(0,type_new)\n","print(\"新类的值: \",init_val)\n","\n","#对于训练集，不保留的类都归为值：init_val\n","print(len(set(train_Delta)))\n","print('不保留的类都归为值：',init_val)\n","for i in Vals_Thr_Dele:\n","  train_Delta[train_Delta==i]=init_val\n","\n","#对于测试集，同样将不保留的值归为：init_val\n","for i in range(len(test_Delta)):\n","  if test_Delta[i] not in Vals_Record:\n","    test_Delta[i]=init_val\n","\n","print(set(test_Delta))\n","\n","print(len(set(train_Delta)))\n","  \n","\n"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["字典收集训练集（delta类别-次数）： {0: 33074, 15703: 32, -15703: 1, -256: 16200, -15447: 28, 256: 9467, -15446: 161, 15702: 163, 255: 354, -199: 1, -15247: 1, 1: 519, -15445: 162, 15701: 163, -201: 1, -15245: 1, 15488: 198, -15488: 82, -203: 1, -15242: 1, 213: 263, 15700: 146, -15444: 147, -15701: 1, -206: 1, -15239: 1, 212: 162, -207: 1, -15237: 1, -15443: 166, 15699: 149, -15700: 3, -210: 2, -15233: 1, 15698: 162, -15442: 135, 210: 193, 211: 196, -212: 6, -15486: 7, -15441: 133, 15697: 148, -15698: 24, -470: 1, -15484: 4, -15697: 16, 209: 135, -216: 1, -15481: 3, -15440: 154, 15696: 171, -219: 1, -15477: 4, -15439: 161, 15695: 160, -15696: 19, -220: 1, -15475: 4, 208: 185, 207: 173, -223: 40, -15472: 3, 15694: 132, -15438: 148, 206: 193, -15695: 1, -225: 61, -15469: 9, 15693: 156, -15437: 166, -15693: 2, -1: 318, -227: 159, -15466: 7, 205: 200, 15692: 155, -15436: 162, -230: 68, -15463: 1, -232: 1, -15460: 1, 203: 166, -15435: 153, 204: 194, 15691: 150, -234: 1, -15458: 2, 257: 338, -15692: 1, -15434: 169, -236: 1, -15455: 3, 15690: 167, -15690: 1, -239: 1, -15451: 1, 202: 227, 15689: 167, -15433: 161, -240: 1, -15449: 3, -15432: 148, 15688: 145, -243: 1, -15190: 1, 200: 173, -15689: 1, -15688: 1, -245: 1, -15187: 1, -15431: 162, 15687: 170, -15687: 8, -247: 1, -15184: 1, 15686: 161, -15430: 164, -249: 1, 198: 16, -252: 1, -15178: 1, 15685: 146, -15429: 127, -254: 1, 197: 10, 196: 23, -15428: 148, 15684: 147, -15685: 18, -512: 3, 512: 1, -15173: 1, -15684: 16, -192: 956, -15492: 5, -15427: 159, 15683: 154, -259: 1, -15169: 1, 195: 14, 15168: 64, -15168: 46, -260: 1, 320: 30, -15487: 39, -15426: 170, 15682: 168, -15683: 1, -263: 1, 194: 2, -15425: 277, -265: 1, 15681: 243, -267: 1, -15478: 2, 15680: 112, -15424: 6646, -15681: 7, 15424: 15783, 258: 5, 193: 13, 253: 139, -15678: 157, 192: 451, 252: 167, -15676: 168, -15680: 71, -15679: 131, -15423: 105, -7: 3, -15672: 161, 15679: 21, 254: 165, -9: 2, -15669: 161, -11: 4, -15667: 166, -15677: 135, 15677: 10, 15678: 1, -14: 2, -15664: 173, -15485: 15, -16: 2, -15660: 160, -18: 1, -15658: 165, -15675: 161, 251: 164, 15676: 1, -20: 4, -15655: 171, -15674: 147, 250: 155, -23: 3, -15652: 168, -15482: 2, -24: 3, -15650: 150, 249: 193, -15673: 169, 15674: 1, -27: 1, -15646: 137, 248: 223, -29: 5, -15643: 169, 15672: 3, -31: 2, -15641: 143, 247: 250, -15671: 147, -15480: 1, -33: 2, -15638: 250, -15670: 168, 246: 460, 15671: 1, 15670: 12, -36: 1, -15634: 160, 245: 588, -38: 1, -15632: 174, 15669: 22, 15104: 2, 565: 1, -15104: 1, 182: 2, -40: 2, -15629: 168, 244: 384, -15668: 142, 15668: 1, -43: 2, -15626: 207, -15476: 8, -44: 2, -15624: 189, 243: 348, -47: 3, -15620: 85, 242: 309, -15666: 129, -49: 2, -15617: 83, -15474: 29, -576: 1, -15090: 1, 15666: 27, -15665: 143, 241: 329, -15473: 7, -51: 3, 15665: 2, -53: 1, -15420: 4, 240: 337, -56: 2, -15416: 2, 15664: 30, -15663: 162, 239: 336, -57: 2, -15414: 12, -60: 2, -15411: 3, 238: 300, -15662: 138, 15662: 12, -62: 5, -15408: 1, -15470: 1, 237: 342, -15661: 140, -15406: 17, -15597: 155, -15405: 37, 236: 334, -67: 4, -15594: 149, 15661: 1, -69: 3, -15591: 129, 235: 320, -15659: 112, -71: 3, -15589: 239, -15468: 2, -15467: 37, 15659: 1, -73: 3, -15586: 192, 234: 345, -76: 2, -15582: 135, 233: 347, -15657: 153, -77: 3, -15580: 123, -15465: 8, -15656: 148, 232: 301, -80: 2, -15577: 11, 15657: 2, -82: 3, -15574: 36, 231: 287, 15656: 1, 15655: 5, -84: 1, -15571: 12, 230: 286, -15654: 163, -86: 2, -15568: 26, -15462: 1, 15040: 1, 614: 1, 15654: 27, -89: 1, -15565: 28, 229: 360, -15653: 146, -91: 1, -15563: 18, 228: 355, -93: 1, -15559: 19, -15651: 160, 227: 355, -96: 2, -15556: 24, -97: 1, -15554: 4, 15651: 15, 226: 356, -100: 1, -128: 22, 15650: 3, -15649: 167, -102: 1, 225: 374, -104: 1, -15417: 1, -15457: 2, 15649: 8, 14784: 58, 865: 23, -14784: 22, -14783: 1, 224: 418, -15648: 161, -106: 2, 223: 316, -15647: 146, -109: 1, 863: 12, -110: 2, -15409: 7, 222: 248, -113: 1, -640: 11, -15006: 9, 640: 2, -15454: 24, -15453: 3, 221: 193, -15645: 159, -115: 2, -15402: 1, 15645: 1, -117: 1, -15400: 2, -15644: 150, 220: 170, -120: 1, -15397: 19, 219: 186, -122: 2, -15393: 25, -124: 1, -15391: 2, -15642: 159, 218: 168, -126: 2, -15388: 3, -15450: 2, 217: 160, 15641: 1, -15385: 9, -129: 2, -15513: 3, 15642: 2, -130: 1, -15511: 3, 216: 193, -15640: 170, -133: 2, -15507: 2, -15639: 168, 215: 160, -135: 1, -15504: 3, 15639: 21, 14976: 8, 663: 5, -14976: 3, -137: 1, -15502: 2, 214: 275, -139: 1, -15499: 2, -15637: 224, -2: 3, -15636: 149, -4: 1, -15633: 150, 15637: 1, -15630: 147, 15636: 3, -8: 2, -15628: 160, -15635: 166, -13: 3, -15621: 89, -15: 1, -15619: 100, 15633: 31, -17: 2, -15631: 162, -21: 5, -15418: 1, -15415: 19, -26: 2, -15412: 2, -28: 1, -15403: 1, -15627: 131, -35: 2, -15401: 1, -704: 84, -14923: 14, 704: 32, 15627: 3, -37: 1, -15398: 5, -15394: 43, 201: 288, -15625: 280, -41: 2, -15392: 2, -18477: 2, -1242: 1, 19520: 1, 199: 15, 2852: 1, -18278: 2, 18480: 3, -18480: 3, 2854: 2, -18279: 1, 2855: 1, -15390: 1, 15625: 7, 14912: 2, 714: 1, 15626: 3, -45: 1, -15386: 2, -15384: 6, -19528: 1, 19728: 1, -754: 1, -14670: 1, 191: 7, -52: 1, -54: 2, -15378: 2, 15624: 45, -15623: 83, -59: 1, -15373: 4, 15623: 70, -15369: 2, 15622: 93, -15622: 81, -63: 4, -15367: 2, -15366: 17, -66: 3, 15621: 89, -68: 3, -15553: 1, 15620: 81, -70: 4, -15551: 4, 15619: 101, -72: 4, -15547: 3, -75: 1, -15544: 1, 15618: 89, -15618: 87, -15542: 4, 15617: 84, -79: 1, -15539: 3, 15616: 101, -15616: 100, -15535: 2, -83: 2, -15533: 4, 15615: 89, -15615: 86, -15530: 2, -88: 1, -15527: 5, 15614: 83, -15614: 81, -15422: 1, -90: 1, -15525: 5, -15613: 100, 15613: 101, -92: 1, -15521: 5, 15612: 91, -15612: 85, -95: 1, -15518: 4, 14848: 5, -14848: 3, 765: 1, -14847: 1, -15516: 4, -15611: 84, 15611: 81, -99: 1, -15419: 1, 15610: 95, -15610: 99, -101: 1, -15509: 4, -103: 1, -15609: 87, 15609: 70, 15360: 14395, -15360: 9238, -15359: 161, 15608: 21, -15608: 80, -108: 1, -15501: 1, -15361: 18, -15498: 1, -15607: 92, -112: 1, -15495: 1, 15607: 12, -15351: 3, -15606: 288, 15606: 3, -19546: 1, 19584: 2, -116: 1, -15490: 1, -119: 1, -15605: 436, -15356: 1, -15413: 5, -123: 1, -15354: 3, -15349: 2, -131: 1, -15604: 238, -136: 1, -19570: 1, -3: 2, -15602: 179, -6: 2, -15599: 168, -15596: 171, -15593: 170, -15603: 179, 15604: 2, 13: 1, -19586: 1, 19830: 1, -22249: 1, 22016: 2, 10: 3, -22026: 1, -15365: 1, 15602: 6, -15601: 175, -15579: 21, 15601: 1, 15296: 280, -15296: 225, 306: 5, -15410: 1, -15575: 13, -15600: 209, -30: 1, 15600: 17, -32: 1, 15599: 3, -15564: 13, -15407: 5, -39: 1, -15560: 37, -320: 35, -15279: 4, -15598: 170, -15558: 31, 15598: 20, -1087: 1, -14511: 1, -48: 3, -15357: 1, -15350: 1, -15595: 162, -58: 1, -15346: 2, 15596: 1, -15343: 3, -61: 2, -15342: 4, -15341: 3, -15340: 2, -15339: 3, -64: 2, -15338: 15, -65: 3, -15529: 5, 24: 2, -15528: 4, 22: 1, -15382: 7, 15594: 1, -15526: 2, -15524: 4, -15523: 4, -15522: 3, -15592: 153, -15520: 5, 15592: 1, -15519: 4, -74: 2, -15515: 3, 15591: 4, 15232: 6, -15232: 3, 360: 2, -15231: 1, -15512: 3, -15590: 174, 83: 2, -2202: 1, 2240: 123, -2246: 5, -2262: 2, -18014: 1, 4: 2, -25: 1, 18048: 110, -18026: 2, -19699: 1, 19712: 1, -18243: 1, -18068: 2, -18049: 10, -2229: 2, -2260: 4, -2221: 8, -18047: 3, -2251: 4, -2222: 1, -2227: 1, -2231: 1, -2240: 8, -2238: 3, -2263: 2, -2219: 4, -18038: 4, -18046: 7, -22: 2, -18027: 3, -3871: 1, 21888: 3, -21879: 1, -15372: 3, -18013: 5, -2268: 1, -18037: 2, -18052: 7, -2245: 8, -18055: 3, -2249: 5, -2259: 3, -18025: 5, -19418: 1, 19392: 3, 32: 1, -2235: 4, -18035: 6, -2255: 2, -19391: 2, -18029: 4, -18053: 2, -2230: 2, -18050: 10, -2241: 4, -2243: 6, -18033: 4, -15759: 1, -18063: 2, -21896: 1, -2244: 3, -2217: 2, -18036: 1, -2250: 2, -2220: 2, -2237: 4, -2264: 2, -18040: 1, 3: 2, 15794: 1, -15368: 1, -19490: 1, 19460: 1, -2218: 3, -2261: 1, 2: 1, -18028: 4, 15796: 1, -18012: 3, -2248: 2, -2242: 4, -2247: 2, -18054: 1, -18031: 2, -18070: 2, -2254: 1, -18034: 2, -2236: 2, 2651: 1, -18305: 2, 18304: 4, -18051: 1, -15383: 3, -2213: 1, -2257: 3, -15757: 1, 5: 3, -18044: 2, -2223: 3, -2252: 2, -2228: 1, -2253: 1, -2224: 1, -15387: 6, -18011: 2, -18066: 2, -18015: 2, -18061: 1, -15380: 5, -18306: 1, -18284: 1, -18042: 1, -18067: 1, -18032: 1, -18048: 1, -19648: 1, 19619: 1, -19433: 1, 646: 1, 18816: 5, -19462: 1, 19456: 1, -18787: 1, -15389: 2, -18783: 6, 3423: 3, -19701: 1, 19899: 1, 15590: 96, -6723: 1, 22084: 3, -22084: 3, 22113: 2, -18065: 1, -22095: 1, -22116: 1, 22092: 2, -22090: 2, 22087: 3, -22086: 5, -22087: 1, 22090: 2, -22091: 1, 22085: 4, -22085: 4, 22086: 3, 22100: 1, -22100: 1, 22144: 2, -22144: 1, -22097: 2, -10: 1, 22097: 1, 22094: 1, -22094: 1, -22092: 1, 22105: 1, 17: 1, -15377: 2, 145: 1, -15505: 3, 28: 1, 18259: 1, -194: 1, 36: 16, -15396: 39, 15589: 154, -229: 123, 15502: 2, 88: 2, 38: 5, 37: 8, 139: 1, 15359: 7, -18009: 1, 2649: 1, -15588: 199, 15588: 150, -228: 118, -18019: 1, -15587: 198, -18023: 1, -15395: 33, 15587: 154, 35: 14, -15379: 5, -163: 1, -15331: 13, 15586: 139, -226: 148, -18039: 1, 34: 15, -18043: 1, -15362: 1, 15585: 92, -15585: 258, 2689: 1, 18279: 1, 33: 3, 15394: 1, -34: 1, -18056: 1, 2696: 1, -15584: 307, -2699: 1, 2700: 1, 15584: 53, 18288: 1, 2706: 1, 2710: 1, 288: 2, -15583: 221, 15583: 73, -18073: 1, 18297: 1, -18076: 1, 2716: 1, -18080: 1, 2720: 1, -2724: 1, 18307: 1, -22168: 1, 22391: 1, -2289: 1, 2512: 1, 15582: 54, -22401: 2, -600: 1, 2827: 1, -980: 1, 980: 1, -982: 2, 20928: 4, -22173: 1, 1246: 1, -22174: 1, 6808: 1, -18085: 1, 2725: 1, -20924: 1, 21147: 1, -18087: 1, 18310: 1, -2729: 1, 2729: 1, -15581: 206, -21150: 1, -2731: 1, 2731: 1, -20170: 1, -22172: 1, 6814: 1, 15581: 170, -18093: 1, 18315: 1, -18095: 1, 2735: 1, -2737: 1, 18318: 1, 15580: 125, -2739: 1, 2739: 1, -2741: 1, 2741: 1, -2742: 1, 2742: 1, -2744: 1, 2744: 1, -4415: 1, 19995: 1, -2746: 1, 18326: 1, -15260: 5, -4429: 1, 19776: 7, -18328: 1, 18112: 3, -323: 22, 323: 22, -195: 1, 324: 33, -324: 30, -196: 2, -467: 32, 467: 31, 468: 32, -468: 26, -994: 14, 994: 30, -768: 45, -162: 1, -832: 1, 768: 3, 995: 31, -995: 4, -1930: 1, 2157: 31, -237: 19, -1920: 39, -2157: 11, -173: 1, -1984: 1, -1925: 1, 1920: 3, -2158: 11, 2158: 30, -238: 22, -18096: 1, 18334: 28, -222: 3, -18112: 3, -18334: 24, -20200: 1, 19968: 1, -21982: 1, 792: 1, 1416: 1, -19774: 1, 1672: 1, 2746: 1, -2784: 1, 15832: 1, 2304: 5, -19770: 1, 4429: 1, 13048: 2, -15352: 3, -4183: 1, 4183: 1, 12996: 1, 2368: 3, -2316: 2, -18530: 1, 16200: 1, -15777: 1, -18133: 1, -3060: 1, 21248: 2, -18620: 1, 18560: 14, -18560: 29, -2628: 1, -19830: 1, -1418: 1, 3060: 1, 15772: 1, -2763: 1, -2327: 1, -17450: 1, 8: 1, -18892: 1, 18880: 2, -19772: 1, 892: 1, -15344: 1, -19502: 1, 4142: 1, 13034: 1, -13034: 1, -4431: 1, -15345: 1, -19793: 1, 4433: 1, -19794: 2, -4435: 3, 19840: 2, 177: 2, -4436: 1, 20017: 1, -4425: 1, 20007: 1, -4196: 1, 19549: 1, -15353: 2, 15578: 90, -15578: 14, 15577: 70, 15576: 80, -15576: 27, 15574: 114, -15573: 27, 15572: 74, -15572: 14, -15570: 16, -15324: 20, 15569: 82, -15569: 14, -15323: 72, 15579: 76, 15568: 70, 15566: 115, -15566: 23, 15563: 104, 128: 1, 15562: 121, -15562: 10, 15559: 143, -15322: 78, 15558: 146, 15556: 153, 15555: 140, -15555: 15, 15554: 118, 15552: 1, -15552: 1, 15551: 3, 15550: 3, -15550: 3, -15321: 69, 15549: 3, -15549: 3, 187: 1, 186: 1, -15546: 2, 15347: 1, -15347: 1, 15545: 3, -15545: 2, 15543: 2, -15543: 3, 15542: 2, 15541: 2, 15539: 2, -15320: 73, 15538: 2, -15538: 1, 15537: 2, -15537: 3, 152: 2, 175: 2, -15534: 1, 173: 1, -15319: 71, 15532: 2, -15532: 2, 15575: 67, 15531: 2, -15531: 3, 15529: 2, 15528: 2, 15527: 2, 165: 1, 15524: 1, 15523: 3, 15521: 2, -15318: 104, 15520: 3, 15519: 2, 15518: 2, 15516: 2, 15515: 3, 15514: 2, -15514: 2, 15512: 3, 151: 1, 15573: 89, -15317: 95, 15510: 2, -15510: 2, 15508: 3, -15316: 82, 15507: 1, -15315: 74, 146: 1, -15314: 70, 15504: 2, -15312: 60, -15381: 1, 15571: 76, 15567: 87, -15375: 1, 277: 5, -15308: 73, -19200: 3, 19199: 2, 3627: 1, -15307: 96, -15306: 137, 129: 1, 15560: 126, -15302: 117, 15557: 137, -15301: 126, -15299: 134, -15298: 95, 15553: 14, -15297: 11, 276: 8, -15295: 3, 15547: 2, -15355: 1, 15546: 2, 15570: 63, 183: 2, -3648: 2, 19218: 1, 3648: 1, 181: 1, -15541: 2, 15540: 2, -15540: 2, 12928: 2, -12928: 2, 15536: 1, -15536: 1, 15535: 2, 15533: 2, -15313: 84, 171: 1, -1344: 7, 1344: 7, 15526: 2, -15334: 2, 164: 1, 15522: 1, -15311: 84, -15567: 16, 15509: 2, -15508: 2, 15506: 1, -15506: 1, 15505: 2, -15310: 113, 142: 1, -15309: 101, 271: 4, 15500: 2, 138: 1, 15497: 2, -15305: 108, 15565: 105, 15496: 2, -15304: 130, -19264: 4, 19264: 3, 19265: 1, 15495: 1, -15303: 146, 15493: 1, 132: 1, -15300: 142, -15374: 2, 15564: 66, 15561: 143, -15561: 43, 269: 7, -15371: 1, -15370: 1, -14912: 1, 169: 1, -15337: 1, -20117: 1, 1856: 2, -2836: 1, 21120: 3, -15328: 1, 15525: 1, -5796: 1, -2470: 1, 2496: 1, -20122: 1, -2838: 1, -22365: 1, 22400: 9, -22399: 2, 6: 1, -22406: 1, -22352: 2, -22402: 1, -22404: 1, 15517: 1, 43: 1, 15513: 1, 264: 11, -384: 1, 384: 1, 15511: 1, 49: 1, -19968: 1, 4408: 1, 390: 2, 391: 15, -15167: 1, 15503: 1, -15503: 1, 57: 1, -4416: 1, 4416: 1, -15500: 1, -15497: 1, 62: 1, -2368: 5, -13190: 3, -15557: 39, -13189: 1, 15494: 1, -15494: 1, -197: 1, 15491: 1, -4672: 10, 20229: 2, 4672: 8, 15490: 1, 13184: 59, -13184: 43, 2373: 15, -13183: 1, 261: 8, 15485: 1, 71: 1, 15484: 1, -1216: 5, 1216: 5, 15482: 1, 14336: 41, -14336: 24, 1143: 1, 76: 1, 1219: 13, 1220: 4, 15300: 2, -964: 2, -1220: 14, 964: 14, -14335: 1, 15478: 1, 78: 1, 3005: 14, 18561: 1, 15475: 1, 15473: 1, -3008: 35, 3008: 23, 18562: 3, 18563: 9, 15472: 1, 15469: 1, 703: 2, -14851: 3, -14850: 31, 15467: 1, 87: 1, -14849: 2, 15466: 1}\n","各delta值： [0, 15703, -15703, -256, -15447, 256, -15446, 15702, 255, -199, -15247, 1, -15445, 15701, -201, -15245, 15488, -15488, -203, -15242, 213, 15700, -15444, -15701, -206, -15239, 212, -207, -15237, -15443, 15699, -15700, -210, -15233, 15698, -15442, 210, 211, -212, -15486, -15441, 15697, -15698, -470, -15484, -15697, 209, -216, -15481, -15440, 15696, -219, -15477, -15439, 15695, -15696, -220, -15475, 208, 207, -223, -15472, 15694, -15438, 206, -15695, -225, -15469, 15693, -15437, -15693, -1, -227, -15466, 205, 15692, -15436, -230, -15463, -232, -15460, 203, -15435, 204, 15691, -234, -15458, 257, -15692, -15434, -236, -15455, 15690, -15690, -239, -15451, 202, 15689, -15433, -240, -15449, -15432, 15688, -243, -15190, 200, -15689, -15688, -245, -15187, -15431, 15687, -15687, -247, -15184, 15686, -15430, -249, 198, -252, -15178, 15685, -15429, -254, 197, 196, -15428, 15684, -15685, -512, 512, -15173, -15684, -192, -15492, -15427, 15683, -259, -15169, 195, 15168, -15168, -260, 320, -15487, -15426, 15682, -15683, -263, 194, -15425, -265, 15681, -267, -15478, 15680, -15424, -15681, 15424, 258, 193, 253, -15678, 192, 252, -15676, -15680, -15679, -15423, -7, -15672, 15679, 254, -9, -15669, -11, -15667, -15677, 15677, 15678, -14, -15664, -15485, -16, -15660, -18, -15658, -15675, 251, 15676, -20, -15655, -15674, 250, -23, -15652, -15482, -24, -15650, 249, -15673, 15674, -27, -15646, 248, -29, -15643, 15672, -31, -15641, 247, -15671, -15480, -33, -15638, -15670, 246, 15671, 15670, -36, -15634, 245, -38, -15632, 15669, 15104, 565, -15104, 182, -40, -15629, 244, -15668, 15668, -43, -15626, -15476, -44, -15624, 243, -47, -15620, 242, -15666, -49, -15617, -15474, -576, -15090, 15666, -15665, 241, -15473, -51, 15665, -53, -15420, 240, -56, -15416, 15664, -15663, 239, -57, -15414, -60, -15411, 238, -15662, 15662, -62, -15408, -15470, 237, -15661, -15406, -15597, -15405, 236, -67, -15594, 15661, -69, -15591, 235, -15659, -71, -15589, -15468, -15467, 15659, -73, -15586, 234, -76, -15582, 233, -15657, -77, -15580, -15465, -15656, 232, -80, -15577, 15657, -82, -15574, 231, 15656, 15655, -84, -15571, 230, -15654, -86, -15568, -15462, 15040, 614, 15654, -89, -15565, 229, -15653, -91, -15563, 228, -93, -15559, -15651, 227, -96, -15556, -97, -15554, 15651, 226, -100, -128, 15650, -15649, -102, 225, -104, -15417, -15457, 15649, 14784, 865, -14784, -14783, 224, -15648, -106, 223, -15647, -109, 863, -110, -15409, 222, -113, -640, -15006, 640, -15454, -15453, 221, -15645, -115, -15402, 15645, -117, -15400, -15644, 220, -120, -15397, 219, -122, -15393, -124, -15391, -15642, 218, -126, -15388, -15450, 217, 15641, -15385, -129, -15513, 15642, -130, -15511, 216, -15640, -133, -15507, -15639, 215, -135, -15504, 15639, 14976, 663, -14976, -137, -15502, 214, -139, -15499, -15637, -2, -15636, -4, -15633, 15637, -15630, 15636, -8, -15628, -15635, -13, -15621, -15, -15619, 15633, -17, -15631, -21, -15418, -15415, -26, -15412, -28, -15403, -15627, -35, -15401, -704, -14923, 704, 15627, -37, -15398, -15394, 201, -15625, -41, -15392, -18477, -1242, 19520, 199, 2852, -18278, 18480, -18480, 2854, -18279, 2855, -15390, 15625, 14912, 714, 15626, -45, -15386, -15384, -19528, 19728, -754, -14670, 191, -52, -54, -15378, 15624, -15623, -59, -15373, 15623, -15369, 15622, -15622, -63, -15367, -15366, -66, 15621, -68, -15553, 15620, -70, -15551, 15619, -72, -15547, -75, -15544, 15618, -15618, -15542, 15617, -79, -15539, 15616, -15616, -15535, -83, -15533, 15615, -15615, -15530, -88, -15527, 15614, -15614, -15422, -90, -15525, -15613, 15613, -92, -15521, 15612, -15612, -95, -15518, 14848, -14848, 765, -14847, -15516, -15611, 15611, -99, -15419, 15610, -15610, -101, -15509, -103, -15609, 15609, 15360, -15360, -15359, 15608, -15608, -108, -15501, -15361, -15498, -15607, -112, -15495, 15607, -15351, -15606, 15606, -19546, 19584, -116, -15490, -119, -15605, -15356, -15413, -123, -15354, -15349, -131, -15604, -136, -19570, -3, -15602, -6, -15599, -15596, -15593, -15603, 15604, 13, -19586, 19830, -22249, 22016, 10, -22026, -15365, 15602, -15601, -15579, 15601, 15296, -15296, 306, -15410, -15575, -15600, -30, 15600, -32, 15599, -15564, -15407, -39, -15560, -320, -15279, -15598, -15558, 15598, -1087, -14511, -48, -15357, -15350, -15595, -58, -15346, 15596, -15343, -61, -15342, -15341, -15340, -15339, -64, -15338, -65, -15529, 24, -15528, 22, -15382, 15594, -15526, -15524, -15523, -15522, -15592, -15520, 15592, -15519, -74, -15515, 15591, 15232, -15232, 360, -15231, -15512, -15590, 83, -2202, 2240, -2246, -2262, -18014, 4, -25, 18048, -18026, -19699, 19712, -18243, -18068, -18049, -2229, -2260, -2221, -18047, -2251, -2222, -2227, -2231, -2240, -2238, -2263, -2219, -18038, -18046, -22, -18027, -3871, 21888, -21879, -15372, -18013, -2268, -18037, -18052, -2245, -18055, -2249, -2259, -18025, -19418, 19392, 32, -2235, -18035, -2255, -19391, -18029, -18053, -2230, -18050, -2241, -2243, -18033, -15759, -18063, -21896, -2244, -2217, -18036, -2250, -2220, -2237, -2264, -18040, 3, 15794, -15368, -19490, 19460, -2218, -2261, 2, -18028, 15796, -18012, -2248, -2242, -2247, -18054, -18031, -18070, -2254, -18034, -2236, 2651, -18305, 18304, -18051, -15383, -2213, -2257, -15757, 5, -18044, -2223, -2252, -2228, -2253, -2224, -15387, -18011, -18066, -18015, -18061, -15380, -18306, -18284, -18042, -18067, -18032, -18048, -19648, 19619, -19433, 646, 18816, -19462, 19456, -18787, -15389, -18783, 3423, -19701, 19899, 15590, -6723, 22084, -22084, 22113, -18065, -22095, -22116, 22092, -22090, 22087, -22086, -22087, 22090, -22091, 22085, -22085, 22086, 22100, -22100, 22144, -22144, -22097, -10, 22097, 22094, -22094, -22092, 22105, 17, -15377, 145, -15505, 28, 18259, -194, 36, -15396, 15589, -229, 15502, 88, 38, 37, 139, 15359, -18009, 2649, -15588, 15588, -228, -18019, -15587, -18023, -15395, 15587, 35, -15379, -163, -15331, 15586, -226, -18039, 34, -18043, -15362, 15585, -15585, 2689, 18279, 33, 15394, -34, -18056, 2696, -15584, -2699, 2700, 15584, 18288, 2706, 2710, 288, -15583, 15583, -18073, 18297, -18076, 2716, -18080, 2720, -2724, 18307, -22168, 22391, -2289, 2512, 15582, -22401, -600, 2827, -980, 980, -982, 20928, -22173, 1246, -22174, 6808, -18085, 2725, -20924, 21147, -18087, 18310, -2729, 2729, -15581, -21150, -2731, 2731, -20170, -22172, 6814, 15581, -18093, 18315, -18095, 2735, -2737, 18318, 15580, -2739, 2739, -2741, 2741, -2742, 2742, -2744, 2744, -4415, 19995, -2746, 18326, -15260, -4429, 19776, -18328, 18112, -323, 323, -195, 324, -324, -196, -467, 467, 468, -468, -994, 994, -768, -162, -832, 768, 995, -995, -1930, 2157, -237, -1920, -2157, -173, -1984, -1925, 1920, -2158, 2158, -238, -18096, 18334, -222, -18112, -18334, -20200, 19968, -21982, 792, 1416, -19774, 1672, 2746, -2784, 15832, 2304, -19770, 4429, 13048, -15352, -4183, 4183, 12996, 2368, -2316, -18530, 16200, -15777, -18133, -3060, 21248, -18620, 18560, -18560, -2628, -19830, -1418, 3060, 15772, -2763, -2327, -17450, 8, -18892, 18880, -19772, 892, -15344, -19502, 4142, 13034, -13034, -4431, -15345, -19793, 4433, -19794, -4435, 19840, 177, -4436, 20017, -4425, 20007, -4196, 19549, -15353, 15578, -15578, 15577, 15576, -15576, 15574, -15573, 15572, -15572, -15570, -15324, 15569, -15569, -15323, 15579, 15568, 15566, -15566, 15563, 128, 15562, -15562, 15559, -15322, 15558, 15556, 15555, -15555, 15554, 15552, -15552, 15551, 15550, -15550, -15321, 15549, -15549, 187, 186, -15546, 15347, -15347, 15545, -15545, 15543, -15543, 15542, 15541, 15539, -15320, 15538, -15538, 15537, -15537, 152, 175, -15534, 173, -15319, 15532, -15532, 15575, 15531, -15531, 15529, 15528, 15527, 165, 15524, 15523, 15521, -15318, 15520, 15519, 15518, 15516, 15515, 15514, -15514, 15512, 151, 15573, -15317, 15510, -15510, 15508, -15316, 15507, -15315, 146, -15314, 15504, -15312, -15381, 15571, 15567, -15375, 277, -15308, -19200, 19199, 3627, -15307, -15306, 129, 15560, -15302, 15557, -15301, -15299, -15298, 15553, -15297, 276, -15295, 15547, -15355, 15546, 15570, 183, -3648, 19218, 3648, 181, -15541, 15540, -15540, 12928, -12928, 15536, -15536, 15535, 15533, -15313, 171, -1344, 1344, 15526, -15334, 164, 15522, -15311, -15567, 15509, -15508, 15506, -15506, 15505, -15310, 142, -15309, 271, 15500, 138, 15497, -15305, 15565, 15496, -15304, -19264, 19264, 19265, 15495, -15303, 15493, 132, -15300, -15374, 15564, 15561, -15561, 269, -15371, -15370, -14912, 169, -15337, -20117, 1856, -2836, 21120, -15328, 15525, -5796, -2470, 2496, -20122, -2838, -22365, 22400, -22399, 6, -22406, -22352, -22402, -22404, 15517, 43, 15513, 264, -384, 384, 15511, 49, -19968, 4408, 390, 391, -15167, 15503, -15503, 57, -4416, 4416, -15500, -15497, 62, -2368, -13190, -15557, -13189, 15494, -15494, -197, 15491, -4672, 20229, 4672, 15490, 13184, -13184, 2373, -13183, 261, 15485, 71, 15484, -1216, 1216, 15482, 14336, -14336, 1143, 76, 1219, 1220, 15300, -964, -1220, 964, -14335, 15478, 78, 3005, 18561, 15475, 15473, -3008, 3008, 18562, 18563, 15472, 15469, 703, -14851, -14850, 15467, 87, -14849, 15466]\n","各delta值的次数： [33074, 32, 1, 16200, 28, 9467, 161, 163, 354, 1, 1, 519, 162, 163, 1, 1, 198, 82, 1, 1, 263, 146, 147, 1, 1, 1, 162, 1, 1, 166, 149, 3, 2, 1, 162, 135, 193, 196, 6, 7, 133, 148, 24, 1, 4, 16, 135, 1, 3, 154, 171, 1, 4, 161, 160, 19, 1, 4, 185, 173, 40, 3, 132, 148, 193, 1, 61, 9, 156, 166, 2, 318, 159, 7, 200, 155, 162, 68, 1, 1, 1, 166, 153, 194, 150, 1, 2, 338, 1, 169, 1, 3, 167, 1, 1, 1, 227, 167, 161, 1, 3, 148, 145, 1, 1, 173, 1, 1, 1, 1, 162, 170, 8, 1, 1, 161, 164, 1, 16, 1, 1, 146, 127, 1, 10, 23, 148, 147, 18, 3, 1, 1, 16, 956, 5, 159, 154, 1, 1, 14, 64, 46, 1, 30, 39, 170, 168, 1, 1, 2, 277, 1, 243, 1, 2, 112, 6646, 7, 15783, 5, 13, 139, 157, 451, 167, 168, 71, 131, 105, 3, 161, 21, 165, 2, 161, 4, 166, 135, 10, 1, 2, 173, 15, 2, 160, 1, 165, 161, 164, 1, 4, 171, 147, 155, 3, 168, 2, 3, 150, 193, 169, 1, 1, 137, 223, 5, 169, 3, 2, 143, 250, 147, 1, 2, 250, 168, 460, 1, 12, 1, 160, 588, 1, 174, 22, 2, 1, 1, 2, 2, 168, 384, 142, 1, 2, 207, 8, 2, 189, 348, 3, 85, 309, 129, 2, 83, 29, 1, 1, 27, 143, 329, 7, 3, 2, 1, 4, 337, 2, 2, 30, 162, 336, 2, 12, 2, 3, 300, 138, 12, 5, 1, 1, 342, 140, 17, 155, 37, 334, 4, 149, 1, 3, 129, 320, 112, 3, 239, 2, 37, 1, 3, 192, 345, 2, 135, 347, 153, 3, 123, 8, 148, 301, 2, 11, 2, 3, 36, 287, 1, 5, 1, 12, 286, 163, 2, 26, 1, 1, 1, 27, 1, 28, 360, 146, 1, 18, 355, 1, 19, 160, 355, 2, 24, 1, 4, 15, 356, 1, 22, 3, 167, 1, 374, 1, 1, 2, 8, 58, 23, 22, 1, 418, 161, 2, 316, 146, 1, 12, 2, 7, 248, 1, 11, 9, 2, 24, 3, 193, 159, 2, 1, 1, 1, 2, 150, 170, 1, 19, 186, 2, 25, 1, 2, 159, 168, 2, 3, 2, 160, 1, 9, 2, 3, 2, 1, 3, 193, 170, 2, 2, 168, 160, 1, 3, 21, 8, 5, 3, 1, 2, 275, 1, 2, 224, 3, 149, 1, 150, 1, 147, 3, 2, 160, 166, 3, 89, 1, 100, 31, 2, 162, 5, 1, 19, 2, 2, 1, 1, 131, 2, 1, 84, 14, 32, 3, 1, 5, 43, 288, 280, 2, 2, 2, 1, 1, 15, 1, 2, 3, 3, 2, 1, 1, 1, 7, 2, 1, 3, 1, 2, 6, 1, 1, 1, 1, 7, 1, 2, 2, 45, 83, 1, 4, 70, 2, 93, 81, 4, 2, 17, 3, 89, 3, 1, 81, 4, 4, 101, 4, 3, 1, 1, 89, 87, 4, 84, 1, 3, 101, 100, 2, 2, 4, 89, 86, 2, 1, 5, 83, 81, 1, 1, 5, 100, 101, 1, 5, 91, 85, 1, 4, 5, 3, 1, 1, 4, 84, 81, 1, 1, 95, 99, 1, 4, 1, 87, 70, 14395, 9238, 161, 21, 80, 1, 1, 18, 1, 92, 1, 1, 12, 3, 288, 3, 1, 2, 1, 1, 1, 436, 1, 5, 1, 3, 2, 1, 238, 1, 1, 2, 179, 2, 168, 171, 170, 179, 2, 1, 1, 1, 1, 2, 3, 1, 1, 6, 175, 21, 1, 280, 225, 5, 1, 13, 209, 1, 17, 1, 3, 13, 5, 1, 37, 35, 4, 170, 31, 20, 1, 1, 3, 1, 1, 162, 1, 2, 1, 3, 2, 4, 3, 2, 3, 2, 15, 3, 5, 2, 4, 1, 7, 1, 2, 4, 4, 3, 153, 5, 1, 4, 2, 3, 4, 6, 3, 2, 1, 3, 174, 2, 1, 123, 5, 2, 1, 2, 1, 110, 2, 1, 1, 1, 2, 10, 2, 4, 8, 3, 4, 1, 1, 1, 8, 3, 2, 4, 4, 7, 2, 3, 1, 3, 1, 3, 5, 1, 2, 7, 8, 3, 5, 3, 5, 1, 3, 1, 4, 6, 2, 2, 4, 2, 2, 10, 4, 6, 4, 1, 2, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 4, 1, 3, 2, 4, 2, 1, 2, 2, 1, 2, 2, 1, 2, 4, 1, 3, 1, 3, 1, 3, 2, 3, 2, 1, 1, 1, 6, 2, 2, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 2, 6, 3, 1, 1, 96, 1, 3, 3, 2, 1, 1, 1, 2, 2, 3, 5, 1, 2, 1, 4, 4, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 16, 39, 154, 123, 2, 2, 5, 8, 1, 7, 1, 1, 199, 150, 118, 1, 198, 1, 33, 154, 14, 5, 1, 13, 139, 148, 1, 15, 1, 1, 92, 258, 1, 1, 3, 1, 1, 1, 1, 307, 1, 1, 53, 1, 1, 1, 2, 221, 73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54, 2, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 206, 1, 1, 1, 1, 1, 1, 170, 1, 1, 1, 1, 1, 1, 125, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 7, 1, 3, 22, 22, 1, 33, 30, 2, 32, 31, 32, 26, 14, 30, 45, 1, 1, 3, 31, 4, 1, 31, 19, 39, 11, 1, 1, 1, 3, 11, 30, 22, 1, 28, 3, 3, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 14, 29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 1, 1, 1, 1, 1, 1, 2, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 1, 121, 10, 143, 78, 146, 153, 140, 15, 118, 1, 1, 3, 3, 3, 69, 3, 3, 1, 1, 2, 1, 1, 3, 2, 2, 3, 2, 2, 2, 73, 2, 1, 2, 3, 2, 2, 1, 1, 71, 2, 2, 67, 2, 3, 2, 2, 2, 1, 1, 3, 2, 104, 3, 2, 2, 2, 3, 2, 2, 3, 1, 89, 95, 2, 2, 3, 82, 1, 74, 1, 70, 2, 60, 1, 76, 87, 1, 5, 73, 3, 2, 1, 96, 137, 1, 126, 117, 137, 126, 134, 95, 14, 11, 8, 3, 2, 1, 2, 63, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 84, 1, 7, 7, 2, 2, 1, 1, 84, 16, 2, 2, 1, 1, 2, 113, 1, 101, 4, 2, 1, 2, 108, 105, 2, 130, 4, 3, 1, 1, 146, 1, 1, 142, 2, 66, 143, 43, 7, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 9, 2, 1, 1, 2, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 2, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 3, 39, 1, 1, 1, 1, 1, 10, 2, 8, 1, 59, 43, 15, 1, 8, 1, 1, 1, 5, 5, 1, 41, 24, 1, 1, 13, 4, 2, 2, 14, 14, 1, 1, 1, 14, 1, 1, 1, 35, 23, 3, 9, 1, 1, 2, 3, 31, 1, 1, 2, 1]\n","delta种类： 1304\n","要保留的delta： [0, 15703, -256, -15447, 256, -15446, 15702, 255, 1, -15445, 15701, 15488, -15488, 213, 15700, -15444, 212, -15443, 15699, 15698, -15442, 210, 211, -212, -15486, -15441, 15697, -15698, -15697, 209, -15440, 15696, -15439, 15695, -15696, 208, 207, -223, 15694, -15438, 206, -225, -15469, 15693, -15437, -1, -227, -15466, 205, 15692, -15436, -230, 203, -15435, 204, 15691, 257, -15434, 15690, 202, 15689, -15433, -15432, 15688, 200, -15431, 15687, -15687, 15686, -15430, 198, 15685, -15429, 197, 196, -15428, 15684, -15685, -15684, -192, -15427, 15683, 195, 15168, -15168, 320, -15487, -15426, 15682, -15425, 15681, 15680, -15424, -15681, 15424, 193, 253, -15678, 192, 252, -15676, -15680, -15679, -15423, -15672, 15679, 254, -15669, -15667, -15677, 15677, -15664, -15485, -15660, -15658, -15675, 251, -15655, -15674, 250, -15652, -15650, 249, -15673, -15646, 248, -15643, -15641, 247, -15671, -15638, -15670, 246, 15670, -15634, 245, -15632, 15669, -15629, 244, -15668, -15626, -15476, -15624, 243, -15620, 242, -15666, -15617, -15474, 15666, -15665, 241, -15473, 240, 15664, -15663, 239, -15414, 238, -15662, 15662, 237, -15661, -15406, -15597, -15405, 236, -15594, -15591, 235, -15659, -15589, -15467, -15586, 234, -15582, 233, -15657, -15580, -15465, -15656, 232, -15577, -15574, 231, -15571, 230, -15654, -15568, 15654, -15565, 229, -15653, -15563, 228, -15559, -15651, 227, -15556, 15651, 226, -128, -15649, 225, 15649, 14784, 865, -14784, 224, -15648, 223, -15647, 863, -15409, 222, -640, -15006, -15454, 221, -15645, -15644, 220, -15397, 219, -15393, -15642, 218, 217, -15385, 216, -15640, -15639, 215, 15639, 14976, 214, -15637, -15636, -15633, -15630, -15628, -15635, -15621, -15619, 15633, -15631, -15415, -15627, -704, -14923, 704, -15394, 201, -15625, 199, 15625, -15384, 191, 15624, -15623, 15623, 15622, -15622, -15366, 15621, 15620, 15619, 15618, -15618, 15617, 15616, -15616, 15615, -15615, 15614, -15614, -15613, 15613, 15612, -15612, -15611, 15611, 15610, -15610, -15609, 15609, 15360, -15360, -15359, 15608, -15608, -15361, -15607, 15607, -15606, -15605, -15604, -15602, -15599, -15596, -15593, -15603, 15602, -15601, -15579, 15296, -15296, -15575, -15600, 15600, -15564, -15560, -320, -15598, -15558, 15598, -15595, -15338, -15382, -15592, 15232, -15590, 2240, 18048, -18049, -2221, -2240, -18046, -18052, -2245, -18035, -18050, -2243, -15387, -18783, 15590, 36, -15396, 15589, -229, 37, 15359, -15588, 15588, -228, -15587, -15395, 15587, 35, -15331, 15586, -226, 34, 15585, -15585, -15584, 15584, -15583, 15583, 15582, -15581, 15581, 15580, 19776, -323, 323, 324, -324, -467, 467, 468, -468, -994, 994, -768, 995, 2157, -237, -1920, -2157, -2158, 2158, -238, 18334, -18334, 18560, -18560, 15578, -15578, 15577, 15576, -15576, 15574, -15573, 15572, -15572, -15570, -15324, 15569, -15569, -15323, 15579, 15568, 15566, -15566, 15563, 15562, -15562, 15559, -15322, 15558, 15556, 15555, -15555, 15554, -15321, -15320, -15319, 15575, -15318, 15573, -15317, -15316, -15315, -15314, -15312, 15571, 15567, -15308, -15307, -15306, 15560, -15302, 15557, -15301, -15299, -15298, 15553, -15297, 276, 15570, -15313, -1344, 1344, -15311, -15567, -15310, -15309, -15305, 15565, -15304, -15303, -15300, 15564, 15561, -15561, 269, 22400, 264, 391, -15557, -4672, 4672, 13184, -13184, 2373, 261, 14336, -14336, 1219, -1220, 964, 3005, -3008, 3008, 18563, -14850]\n","要保留的delta次数： [33074, 32, 16200, 28, 9467, 161, 163, 354, 519, 162, 163, 198, 82, 263, 146, 147, 162, 166, 149, 162, 135, 193, 196, 6, 7, 133, 148, 24, 16, 135, 154, 171, 161, 160, 19, 185, 173, 40, 132, 148, 193, 61, 9, 156, 166, 318, 159, 7, 200, 155, 162, 68, 166, 153, 194, 150, 338, 169, 167, 227, 167, 161, 148, 145, 173, 162, 170, 8, 161, 164, 16, 146, 127, 10, 23, 148, 147, 18, 16, 956, 159, 154, 14, 64, 46, 30, 39, 170, 168, 277, 243, 112, 6646, 7, 15783, 13, 139, 157, 451, 167, 168, 71, 131, 105, 161, 21, 165, 161, 166, 135, 10, 173, 15, 160, 165, 161, 164, 171, 147, 155, 168, 150, 193, 169, 137, 223, 169, 143, 250, 147, 250, 168, 460, 12, 160, 588, 174, 22, 168, 384, 142, 207, 8, 189, 348, 85, 309, 129, 83, 29, 27, 143, 329, 7, 337, 30, 162, 336, 12, 300, 138, 12, 342, 140, 17, 155, 37, 334, 149, 129, 320, 112, 239, 37, 192, 345, 135, 347, 153, 123, 8, 148, 301, 11, 36, 287, 12, 286, 163, 26, 27, 28, 360, 146, 18, 355, 19, 160, 355, 24, 15, 356, 22, 167, 374, 8, 58, 23, 22, 418, 161, 316, 146, 12, 7, 248, 11, 9, 24, 193, 159, 150, 170, 19, 186, 25, 159, 168, 160, 9, 193, 170, 168, 160, 21, 8, 275, 224, 149, 150, 147, 160, 166, 89, 100, 31, 162, 19, 131, 84, 14, 32, 43, 288, 280, 15, 7, 6, 7, 45, 83, 70, 93, 81, 17, 89, 81, 101, 89, 87, 84, 101, 100, 89, 86, 83, 81, 100, 101, 91, 85, 84, 81, 95, 99, 87, 70, 14395, 9238, 161, 21, 80, 18, 92, 12, 288, 436, 238, 179, 168, 171, 170, 179, 6, 175, 21, 280, 225, 13, 209, 17, 13, 37, 35, 170, 31, 20, 162, 15, 7, 153, 6, 174, 123, 110, 10, 8, 8, 7, 7, 8, 6, 10, 6, 6, 6, 96, 16, 39, 154, 123, 8, 7, 199, 150, 118, 198, 33, 154, 14, 13, 139, 148, 15, 92, 258, 307, 53, 221, 73, 54, 206, 170, 125, 7, 22, 22, 33, 30, 32, 31, 32, 26, 14, 30, 45, 31, 31, 19, 39, 11, 11, 30, 22, 28, 24, 14, 29, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 121, 10, 143, 78, 146, 153, 140, 15, 118, 69, 73, 71, 67, 104, 89, 95, 82, 74, 70, 60, 76, 87, 73, 96, 137, 126, 117, 137, 126, 134, 95, 14, 11, 8, 63, 84, 7, 7, 84, 16, 113, 101, 108, 105, 130, 146, 142, 66, 143, 43, 7, 9, 11, 15, 39, 10, 8, 59, 43, 15, 8, 41, 24, 13, 14, 14, 14, 35, 23, 9, 31]\n","要保留的delta种类数： 478\n","要删除的delta： [-15703, -199, -15247, -201, -15245, -203, -15242, -15701, -206, -15239, -207, -15237, -15700, -210, -15233, -470, -15484, -216, -15481, -219, -15477, -220, -15475, -15472, -15695, -15693, -15463, -232, -15460, -234, -15458, -15692, -236, -15455, -15690, -239, -15451, -240, -15449, -243, -15190, -15689, -15688, -245, -15187, -247, -15184, -249, -252, -15178, -254, -512, 512, -15173, -15492, -259, -15169, -260, -15683, -263, 194, -265, -267, -15478, 258, -7, -9, -11, 15678, -14, -16, -18, 15676, -20, -23, -15482, -24, 15674, -27, -29, 15672, -31, -15480, -33, 15671, -36, -38, 15104, 565, -15104, 182, -40, 15668, -43, -44, -47, -49, -576, -15090, -51, 15665, -53, -15420, -56, -15416, -57, -60, -15411, -62, -15408, -15470, -67, 15661, -69, -71, -15468, 15659, -73, -76, -77, -80, 15657, -82, 15656, 15655, -84, -86, -15462, 15040, 614, -89, -91, -93, -96, -97, -15554, -100, 15650, -102, -104, -15417, -15457, -14783, -106, -109, -110, -113, 640, -15453, -115, -15402, 15645, -117, -15400, -120, -122, -124, -15391, -126, -15388, -15450, 15641, -129, -15513, 15642, -130, -15511, -133, -15507, -135, -15504, 663, -14976, -137, -15502, -139, -15499, -2, -4, 15637, 15636, -8, -13, -15, -17, -21, -15418, -26, -15412, -28, -15403, -35, -15401, 15627, -37, -15398, -41, -15392, -18477, -1242, 19520, 2852, -18278, 18480, -18480, 2854, -18279, 2855, -15390, 14912, 714, 15626, -45, -15386, -19528, 19728, -754, -14670, -52, -54, -15378, -59, -15373, -15369, -63, -15367, -66, -68, -15553, -70, -15551, -72, -15547, -75, -15544, -15542, -79, -15539, -15535, -83, -15533, -15530, -88, -15527, -15422, -90, -15525, -92, -15521, -95, -15518, 14848, -14848, 765, -14847, -15516, -99, -15419, -101, -15509, -103, -108, -15501, -15498, -112, -15495, -15351, 15606, -19546, 19584, -116, -15490, -119, -15356, -15413, -123, -15354, -15349, -131, -136, -19570, -3, -6, 15604, 13, -19586, 19830, -22249, 22016, 10, -22026, -15365, 15601, 306, -15410, -30, -32, 15599, -15407, -39, -15279, -1087, -14511, -48, -15357, -15350, -58, -15346, 15596, -15343, -61, -15342, -15341, -15340, -15339, -64, -65, -15529, 24, -15528, 22, 15594, -15526, -15524, -15523, -15522, -15520, 15592, -15519, -74, -15515, 15591, -15232, 360, -15231, -15512, 83, -2202, -2246, -2262, -18014, 4, -25, -18026, -19699, 19712, -18243, -18068, -2229, -2260, -18047, -2251, -2222, -2227, -2231, -2238, -2263, -2219, -18038, -22, -18027, -3871, 21888, -21879, -15372, -18013, -2268, -18037, -18055, -2249, -2259, -18025, -19418, 19392, 32, -2235, -2255, -19391, -18029, -18053, -2230, -2241, -18033, -15759, -18063, -21896, -2244, -2217, -18036, -2250, -2220, -2237, -2264, -18040, 3, 15794, -15368, -19490, 19460, -2218, -2261, 2, -18028, 15796, -18012, -2248, -2242, -2247, -18054, -18031, -18070, -2254, -18034, -2236, 2651, -18305, 18304, -18051, -15383, -2213, -2257, -15757, 5, -18044, -2223, -2252, -2228, -2253, -2224, -18011, -18066, -18015, -18061, -15380, -18306, -18284, -18042, -18067, -18032, -18048, -19648, 19619, -19433, 646, 18816, -19462, 19456, -18787, -15389, 3423, -19701, 19899, -6723, 22084, -22084, 22113, -18065, -22095, -22116, 22092, -22090, 22087, -22086, -22087, 22090, -22091, 22085, -22085, 22086, 22100, -22100, 22144, -22144, -22097, -10, 22097, 22094, -22094, -22092, 22105, 17, -15377, 145, -15505, 28, 18259, -194, 15502, 88, 38, 139, -18009, 2649, -18019, -18023, -15379, -163, -18039, -18043, -15362, 2689, 18279, 33, 15394, -34, -18056, 2696, -2699, 2700, 18288, 2706, 2710, 288, -18073, 18297, -18076, 2716, -18080, 2720, -2724, 18307, -22168, 22391, -2289, 2512, -22401, -600, 2827, -980, 980, -982, 20928, -22173, 1246, -22174, 6808, -18085, 2725, -20924, 21147, -18087, 18310, -2729, 2729, -21150, -2731, 2731, -20170, -22172, 6814, -18093, 18315, -18095, 2735, -2737, 18318, -2739, 2739, -2741, 2741, -2742, 2742, -2744, 2744, -4415, 19995, -2746, 18326, -15260, -4429, -18328, 18112, -195, -196, -162, -832, 768, -995, -1930, -173, -1984, -1925, 1920, -18096, -222, -18112, -20200, 19968, -21982, 792, 1416, -19774, 1672, 2746, -2784, 15832, 2304, -19770, 4429, 13048, -15352, -4183, 4183, 12996, 2368, -2316, -18530, 16200, -15777, -18133, -3060, 21248, -18620, -2628, -19830, -1418, 3060, 15772, -2763, -2327, -17450, 8, -18892, 18880, -19772, 892, -15344, -19502, 4142, 13034, -13034, -4431, -15345, -19793, 4433, -19794, -4435, 19840, 177, -4436, 20017, -4425, 20007, -4196, 19549, -15353, 128, 15552, -15552, 15551, 15550, -15550, 15549, -15549, 187, 186, -15546, 15347, -15347, 15545, -15545, 15543, -15543, 15542, 15541, 15539, 15538, -15538, 15537, -15537, 152, 175, -15534, 173, 15532, -15532, 15531, -15531, 15529, 15528, 15527, 165, 15524, 15523, 15521, 15520, 15519, 15518, 15516, 15515, 15514, -15514, 15512, 151, 15510, -15510, 15508, 15507, 146, 15504, -15381, -15375, 277, -19200, 19199, 3627, 129, -15295, 15547, -15355, 15546, 183, -3648, 19218, 3648, 181, -15541, 15540, -15540, 12928, -12928, 15536, -15536, 15535, 15533, 171, 15526, -15334, 164, 15522, 15509, -15508, 15506, -15506, 15505, 142, 271, 15500, 138, 15497, 15496, -19264, 19264, 19265, 15495, 15493, 132, -15374, -15371, -15370, -14912, 169, -15337, -20117, 1856, -2836, 21120, -15328, 15525, -5796, -2470, 2496, -20122, -2838, -22365, -22399, 6, -22406, -22352, -22402, -22404, 15517, 43, 15513, -384, 384, 15511, 49, -19968, 4408, 390, -15167, 15503, -15503, 57, -4416, 4416, -15500, -15497, 62, -2368, -13190, -13189, 15494, -15494, -197, 15491, 20229, 15490, -13183, 15485, 71, 15484, -1216, 1216, 15482, 1143, 76, 1220, 15300, -964, -14335, 15478, 78, 18561, 15475, 15473, 18562, 15472, 15469, 703, -14851, 15467, 87, -14849, 15466]\n","要删除的delta次数： [33074, 32, 16200, 28, 9467, 161, 163, 354, 519, 162, 163, 198, 82, 263, 146, 147, 162, 166, 149, 162, 135, 193, 196, 6, 7, 133, 148, 24, 16, 135, 154, 171, 161, 160, 19, 185, 173, 40, 132, 148, 193, 61, 9, 156, 166, 318, 159, 7, 200, 155, 162, 68, 166, 153, 194, 150, 338, 169, 167, 227, 167, 161, 148, 145, 173, 162, 170, 8, 161, 164, 16, 146, 127, 10, 23, 148, 147, 18, 16, 956, 159, 154, 14, 64, 46, 30, 39, 170, 168, 277, 243, 112, 6646, 7, 15783, 13, 139, 157, 451, 167, 168, 71, 131, 105, 161, 21, 165, 161, 166, 135, 10, 173, 15, 160, 165, 161, 164, 171, 147, 155, 168, 150, 193, 169, 137, 223, 169, 143, 250, 147, 250, 168, 460, 12, 160, 588, 174, 22, 168, 384, 142, 207, 8, 189, 348, 85, 309, 129, 83, 29, 27, 143, 329, 7, 337, 30, 162, 336, 12, 300, 138, 12, 342, 140, 17, 155, 37, 334, 149, 129, 320, 112, 239, 37, 192, 345, 135, 347, 153, 123, 8, 148, 301, 11, 36, 287, 12, 286, 163, 26, 27, 28, 360, 146, 18, 355, 19, 160, 355, 24, 15, 356, 22, 167, 374, 8, 58, 23, 22, 418, 161, 316, 146, 12, 7, 248, 11, 9, 24, 193, 159, 150, 170, 19, 186, 25, 159, 168, 160, 9, 193, 170, 168, 160, 21, 8, 275, 224, 149, 150, 147, 160, 166, 89, 100, 31, 162, 19, 131, 84, 14, 32, 43, 288, 280, 15, 7, 6, 7, 45, 83, 70, 93, 81, 17, 89, 81, 101, 89, 87, 84, 101, 100, 89, 86, 83, 81, 100, 101, 91, 85, 84, 81, 95, 99, 87, 70, 14395, 9238, 161, 21, 80, 18, 92, 12, 288, 436, 238, 179, 168, 171, 170, 179, 6, 175, 21, 280, 225, 13, 209, 17, 13, 37, 35, 170, 31, 20, 162, 15, 7, 153, 6, 174, 123, 110, 10, 8, 8, 7, 7, 8, 6, 10, 6, 6, 6, 96, 16, 39, 154, 123, 8, 7, 199, 150, 118, 198, 33, 154, 14, 13, 139, 148, 15, 92, 258, 307, 53, 221, 73, 54, 206, 170, 125, 7, 22, 22, 33, 30, 32, 31, 32, 26, 14, 30, 45, 31, 31, 19, 39, 11, 11, 30, 22, 28, 24, 14, 29, 90, 14, 70, 80, 27, 114, 27, 74, 14, 16, 20, 82, 14, 72, 76, 70, 115, 23, 104, 121, 10, 143, 78, 146, 153, 140, 15, 118, 69, 73, 71, 67, 104, 89, 95, 82, 74, 70, 60, 76, 87, 73, 96, 137, 126, 117, 137, 126, 134, 95, 14, 11, 8, 63, 84, 7, 7, 84, 16, 113, 101, 108, 105, 130, 146, 142, 66, 143, 43, 7, 9, 11, 15, 39, 10, 8, 59, 43, 15, 8, 41, 24, 13, 14, 14, 14, 35, 23, 9, 31]\n","要删除的delta种类： 826\n","新类的值:  363\n","1304\n","不保留的类都归为值： 363\n","{0, 15360, -15360, 1, -15359, -15338, 34, 35, -15324, 37, -15323, -15321, -15319, -15318, -15314, -15313, -15309, -15308, -15306, -15304, -15302, -15300, -15298, -15297, -15296, 15488, 18560, 191, 192, 15553, 15554, 15555, 193, 196, 195, 197, 200, 201, 199, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, -256, 256, 257, -192, 1344, 363, 15232, -18560, -15474, -15467, 15296, 198, -1, -15361}\n","479\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDlKfU0_rMjg","executionInfo":{"status":"ok","timestamp":1632655878418,"user_tz":-480,"elapsed":14,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"0cad593a-936f-4134-ba15-37ec071c6a96"},"source":["trainl=test_Delta.tolist()\n","setl=set(trainl)\n","typel=len(setl)\n","print(\"Train_Set种类：\",typel)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Train_Set种类： 104\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYbl776YBhY7","executionInfo":{"status":"ok","timestamp":1632655878418,"user_tz":-480,"elapsed":10,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"927e37d0-5ad5-452e-a573-9416f16c57d2"},"source":["#编码转换\n","from numpy import array\n","from numpy import argmax\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n","\n","#查看train_Delta和test_Delta种类：\n","listD_train=train_Delta.tolist()\n","listD_test=test_Delta.tolist()\n","set_d_train=set(listD_train)\n","set_d_test=set(listD_test)\n","print(\"查看train_Delta种类：\",len(set_d_train))\n","print(\"查看test_Delta种类：\",len(set_d_test))\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["查看train_Delta种类： 479\n","查看test_Delta种类： 104\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eocxAHlCoj8Y","executionInfo":{"status":"ok","timestamp":1632655878419,"user_tz":-480,"elapsed":9,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"bf984a67-1849-4114-987a-a31e11355735"},"source":["train_Delta[500:590]"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-15446,  15702,   -256,    256,   -256, -15446,      0,      0,\n","        15702,   -256, -15446,  15702,   -256,    256,   -256, -15445,\n","            0,      0,  15701,   -256, -15445,  15701,   -256,    256,\n","         -256, -15445,      0,      0,  15702,   -256, -15446,  15702,\n","         -256, -15446,      0,  15702,   -256, -15446,  15702,   -256,\n","       -15446,      0,  15702,   -256, -15446,  15702,   -256, -15446,\n","            0,      0,  15702,   -256,    256,   -256,    256,   -256,\n","          363,    363,  15702,   -256, -15446,  15702,   -256, -15446,\n","        15702,   -256, -15446,      0,  15702,   -256, -15446,  15702,\n","         -256, -15446,      0,  15702,   -256, -15446,  15702,   -256,\n","       -15446,      0,  15702,   -256, -15446,  15702,   -256, -15446,\n","        15702,   -256])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quqBWxbx4hXZ","executionInfo":{"status":"ok","timestamp":1632655879889,"user_tz":-480,"elapsed":1477,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"ec0e07c5-9cfa-4b0d-aa50-b69cf691f136"},"source":["#编码的时候先合并train和test的数据，相当于用同样的方法transform\n","whole_delta=np.concatenate((train_Delta,test_Delta))\n","#integer encoding方法（LabelEncoder）\n","values=whole_delta\n","label_encoder=LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(values)\n","print(\"训练集推出的Integer_encoder：\",integer_encoded)\n","\n","#找种类的最大值\n","max_integer_encoded=max(integer_encoded)+1\n","print(\"最大的integer_encoded_num：\",max_integer_encoded)\n","max_integer=type_new-1\n","num_bit=int(math.log(max_integer, 2)+1)\n","print(\"需要转的二进制：\",max_integer)\n","print(\"应该用多少bit表示：\",num_bit)\n","\n","# define convert_to_binary method\n","# 每个值转特定位数的binary编码\n","def convert_to_binary(origin, num_bit=1):\n","  binary_encoded = []\n","  num_bit_str=str(num_bit)\n","  for element in origin:\n","    binary_encoded.append(list(map(int,list(('{:0'+num_bit_str+'b}').format(element)))))\n","  return np.array(binary_encoded)\n","\n","binary_encoded = convert_to_binary(integer_encoded, num_bit)\n","print(\"Binary_encoded：\",binary_encoded)\n","\n","# define convert_to_onehot method\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","print(\"Onehot_encoded：\",onehot_encoded)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["训练集推出的Integer_encoder： [268 472 347 ... 347 226 347]\n","最大的integer_encoded_num： 479\n","需要转的二进制： 478\n","应该用多少bit表示： 9\n","Binary_encoded： [[1 0 0 ... 1 0 0]\n"," [1 1 1 ... 0 0 0]\n"," [1 0 1 ... 0 1 1]\n"," ...\n"," [1 0 1 ... 0 1 1]\n"," [0 1 1 ... 0 1 0]\n"," [1 0 1 ... 0 1 1]]\n","Onehot_encoded： [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMujBqqHW5bZ","executionInfo":{"status":"ok","timestamp":1632655879890,"user_tz":-480,"elapsed":13,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"74980b92-8709-4980-b1e2-a81e78018d3b"},"source":["\n","print(type(integer_encoded))\n","print(\"整数编码shape：\",integer_encoded.shape)\n","print(type(onehot_encoded))\n","print(\"独热编码shape：\",onehot_encoded.shape)\n","print(type(binary_encoded))\n","print(\"二进制编码：\",binary_encoded.shape)\n","\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","整数编码shape： (200000, 1)\n","<class 'numpy.ndarray'>\n","独热编码shape： (200000, 479)\n","<class 'numpy.ndarray'>\n","二进制编码： (200000, 9)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpfO6fCUOQKv","executionInfo":{"status":"ok","timestamp":1632655879890,"user_tz":-480,"elapsed":6,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"9893d02d-f614-452c-9b1a-b74b501ff6a0"},"source":["#读写数据\n","RW = datasetRW[:]\n","RW = np.reshape(RW, (RW.shape[0],1))\n","RW.shape"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200000, 1)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"WdRNDnIBOMjV","executionInfo":{"status":"ok","timestamp":1632655879891,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["#创建数据集的输入和输出\n","def create_dataset(datasetX, datasetY, look_back=1, pridict_size=1):\n","\tdataX, dataY = [], []\n","\tfor i in range(len(datasetX)-look_back-pridict_size):\n","\t\ta = datasetX[i:(i+look_back), :]\n","\t\tdataX.append(a)\n","\t\tdataY.append(datasetY[i + look_back:(i+look_back+pridict_size), :])\n","\treturn np.array(dataX), np.array(dataY)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_Hseg2COYOZ","executionInfo":{"status":"ok","timestamp":1632655880997,"user_tz":-480,"elapsed":1109,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"f3693f92-36c8-4cd5-9a36-89fbac8667c6"},"source":["#确定lookback的size\n","look_back = 80\n","pridict_size = 2\n","type_new\n","\n","#按lookback的size创建训练集输入和输出,trainX的维度((train_size-look_back-pridict_size),look_back)\n","#训练集的数据\n","integer_encoded_train=integer_encoded[:train_size]\n","binary_encoded_train=binary_encoded[:train_size]\n","integer_encoded_test=integer_encoded[train_size:]\n","binary_encoded_test=binary_encoded[train_size:]\n","#训练集的读写数据\n","RW_train=RW[:train_size]\n","RW_test=RW[train_size:]\n","\n","#valuesX, valuesY = create_dataset(integer_encoded, onehot_encoded, look_back, pridict_size)    #onehot_encoded\n","trainX, trainY = create_dataset(integer_encoded_train, binary_encoded_train, look_back, pridict_size)    #binary_encoded\n","testX, testY = create_dataset(integer_encoded_test, binary_encoded_test, look_back, pridict_size)\n","\n","trainX_RW, trainY_RW = create_dataset(RW_train, RW_train, look_back, pridict_size) \n","testX_RW, testY_RW = create_dataset(RW_test, RW_test, look_back, pridict_size)\n","\n","\n","print('TrainX Shape: ',trainX.shape)\n","print('TrainY Shape: ',trainY.shape)\n","print('TestX Shape: ',testX.shape)\n","print('TestY Shape: ',testY.shape)\n","\n","print('TrainX_RW Shape: ',trainX_RW.shape)\n","print('TrainY_RW Shape: ',trainY_RW.shape)\n","print('TestX_RW Shape: ',testX_RW.shape)\n","print('TestY_RW Shape: ',testY_RW.shape)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["TrainX Shape:  (159918, 80, 1)\n","TrainY Shape:  (159918, 2, 9)\n","TestX Shape:  (39918, 80, 1)\n","TestY Shape:  (39918, 2, 9)\n","TrainX_RW Shape:  (159918, 80, 1)\n","TrainY_RW Shape:  (159918, 2, 1)\n","TestX_RW Shape:  (39918, 80, 1)\n","TestY_RW Shape:  (39918, 2, 1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xL-kObVNO30F","executionInfo":{"status":"ok","timestamp":1632655880998,"user_tz":-480,"elapsed":9,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"bd4994e6-34fc-4fd1-b71d-53a4870caf57"},"source":["# reshape input to be [samples, time steps, features]\n","trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[2]*trainX.shape[1]))\n","print(\"feature:\",trainX.shape)\n","testX = np.reshape(testX, (testX.shape[0], testX.shape[2]*testX.shape[1]))\n","print('TrainX Shape: ',trainX.shape)\n","print('TrainY Shape: ',trainY.shape)\n","\n","print('trainX type: ',type(trainX))\n","\n","trainX_RW=np.reshape(trainX_RW, (trainX_RW.shape[0], trainX_RW.shape[2]*trainX_RW.shape[1]))\n","testX_RW = np.reshape(testX_RW, (testX_RW.shape[0], testX_RW.shape[2]*testX_RW.shape[1]))\n","print('trainX_RW Shape: ',trainX_RW.shape)\n","print('trainY_RW Shape: ',trainY_RW.shape)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["feature: (159918, 80)\n","TrainX Shape:  (159918, 80)\n","TrainY Shape:  (159918, 2, 9)\n","trainX type:  <class 'numpy.ndarray'>\n","trainX_RW Shape:  (159918, 80)\n","trainY_RW Shape:  (159918, 2, 1)\n"]}]},{"cell_type":"code","metadata":{"id":"DLswuLF1oeT6","executionInfo":{"status":"ok","timestamp":1632655880998,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["from keras import backend as K"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlXRb3xl9f4C","executionInfo":{"status":"ok","timestamp":1632655881577,"user_tz":-480,"elapsed":582,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["\n","import inspect\n","from typing import List\n","\n","from tensorflow.keras import backend as K, Model, Input, optimizers\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda\n","from tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization\n","\n","\n","def is_power_of_two(num: int):\n","    return num != 0 and ((num & (num - 1)) == 0)\n","\n","\n","def adjust_dilations(dilations: list):\n","    if all([is_power_of_two(i) for i in dilations]):\n","        return dilations\n","    else:\n","        new_dilations = [2 ** i for i in dilations]\n","        return new_dilations\n","\n","\n","class ResidualBlock(Layer):\n","\n","    def __init__(self,\n","                 dilation_rate: int,\n","                 nb_filters: int,\n","                 kernel_size: int,\n","                 padding: str,\n","                 activation: str = 'relu',\n","                 dropout_rate: float = 0,\n","                 kernel_initializer: str = 'he_normal',\n","                 use_batch_norm: bool = False,\n","                 use_layer_norm: bool = False,\n","                 use_weight_norm: bool = False,\n","                 **kwargs):\n","        \"\"\"Defines the residual block for the WaveNet TCN\n","        Args:\n","            x: The previous layer in the model\n","            training: boolean indicating whether the layer should behave in training mode or in inference mode\n","            dilation_rate: The dilation power of 2 we are using for this residual block\n","            nb_filters: The number of convolutional filters to use in this block\n","            kernel_size: The size of the convolutional kernel\n","            padding: The padding used in the convolutional layers, 'same' or 'causal'.\n","            activation: The final activation used in o = Activation(x + F(x))\n","            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n","            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n","            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n","            use_weight_norm: Whether to use weight normalization in the residual layers or not.\n","            kwargs: Any initializers for Layer class.\n","        \"\"\"\n","\n","        self.dilation_rate = dilation_rate\n","        self.nb_filters = nb_filters\n","        self.kernel_size = kernel_size\n","        self.padding = padding\n","        self.activation = activation\n","        self.dropout_rate = dropout_rate\n","        self.use_batch_norm = use_batch_norm\n","        self.use_layer_norm = use_layer_norm\n","        self.use_weight_norm = use_weight_norm\n","        self.kernel_initializer = kernel_initializer\n","        self.layers = []\n","        self.layers_outputs = []\n","        self.shape_match_conv = None\n","        self.res_output_shape = None\n","        self.final_activation = None\n","\n","        super(ResidualBlock, self).__init__(**kwargs)\n","\n","    def _build_layer(self, layer):\n","        \"\"\"Helper function for building layer\n","        Args:\n","            layer: Appends layer to internal layer list and builds it based on the current output\n","                   shape of ResidualBlocK. Updates current output shape.\n","        \"\"\"\n","        self.layers.append(layer)\n","        self.layers[-1].build(self.res_output_shape)\n","        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n","\n","    def build(self, input_shape):\n","\n","        with K.name_scope(self.name):  # name scope used to make sure weights get unique names\n","            self.layers = []\n","            self.res_output_shape = input_shape\n","\n","            for k in range(2):\n","                name = 'conv1D_{}'.format(k)\n","                with K.name_scope(name):  # name scope used to make sure weights get unique names\n","                    conv = Conv1D(\n","                        filters=self.nb_filters,\n","                        kernel_size=self.kernel_size,\n","                        dilation_rate=self.dilation_rate,\n","                        padding=self.padding,\n","                        name=name,\n","                        kernel_initializer=self.kernel_initializer\n","                    )\n","                    if self.use_weight_norm:\n","                        from tensorflow_addons.layers import WeightNormalization\n","                        # wrap it. WeightNormalization API is different than BatchNormalization or LayerNormalization.\n","                        with K.name_scope('norm_{}'.format(k)):\n","                            conv = WeightNormalization(conv)\n","                    self._build_layer(conv)\n","\n","                with K.name_scope('norm_{}'.format(k)):\n","                    if self.use_batch_norm:\n","                        self._build_layer(BatchNormalization())\n","                    elif self.use_layer_norm:\n","                        self._build_layer(LayerNormalization())\n","                    elif self.use_weight_norm:\n","                        pass  # done above.\n","\n","                self._build_layer(Activation(self.activation))\n","                self._build_layer(SpatialDropout1D(rate=self.dropout_rate))\n","\n","            if self.nb_filters != input_shape[-1]:\n","                # 1x1 conv to match the shapes (channel dimension).\n","                name = 'matching_conv1D'\n","                with K.name_scope(name):\n","                    # make and build this layer separately because it directly uses input_shape\n","                    self.shape_match_conv = Conv1D(filters=self.nb_filters,\n","                                                   kernel_size=1,\n","                                                   padding='same',\n","                                                   name=name,\n","                                                   kernel_initializer=self.kernel_initializer)\n","            else:\n","                name = 'matching_identity'\n","                self.shape_match_conv = Lambda(lambda x: x, name=name)\n","\n","            with K.name_scope(name):\n","                self.shape_match_conv.build(input_shape)\n","                self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n","\n","            self._build_layer(Activation(self.activation))\n","            self.final_activation = Activation(self.activation)\n","            self.final_activation.build(self.res_output_shape)  # probably isn't necessary\n","\n","            # this is done to force Keras to add the layers in the list to self._layers\n","            for layer in self.layers:\n","                self.__setattr__(layer.name, layer)\n","            self.__setattr__(self.shape_match_conv.name, self.shape_match_conv)\n","            self.__setattr__(self.final_activation.name, self.final_activation)\n","\n","            super(ResidualBlock, self).build(input_shape)  # done to make sure self.built is set True\n","\n","    def call(self, inputs, training=None):\n","        \"\"\"\n","        Returns: A tuple where the first element is the residual model tensor, and the second\n","                 is the skip connection tensor.\n","        \"\"\"\n","        x = inputs\n","        self.layers_outputs = [x]\n","        for layer in self.layers:\n","            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n","            x = layer(x, training=training) if training_flag else layer(x)\n","            self.layers_outputs.append(x)\n","        x2 = self.shape_match_conv(inputs)\n","        self.layers_outputs.append(x2)\n","        res_x = layers.add([x2, x])\n","        self.layers_outputs.append(res_x)\n","\n","        res_act_x = self.final_activation(res_x)\n","        self.layers_outputs.append(res_act_x)\n","        return [res_act_x, x]\n","\n","    def compute_output_shape(self, input_shape):\n","        return [self.res_output_shape, self.res_output_shape]\n","\n","\n","class TCN(Layer):\n","    \"\"\"Creates a TCN layer.\n","        Input shape:\n","            A tensor of shape (batch_size, timesteps, input_dim).\n","        Args:\n","            nb_filters: The number of filters to use in the convolutional layers. Can be a list.\n","            kernel_size: The size of the kernel to use in each convolutional layer.\n","            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n","            nb_stacks : The number of stacks of residual blocks to use.\n","            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n","            use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n","            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n","            activation: The activation used in the residual blocks o = Activation(x + F(x)).\n","            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n","            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n","            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n","            use_weight_norm: Whether to use weight normalization in the residual layers or not.\n","            kwargs: Any other arguments for configuring parent class Layer. For example \"name=str\", Name of the model.\n","                    Use unique names when using multiple TCN.\n","        Returns:\n","            A TCN layer.\n","        \"\"\"\n","\n","    def __init__(self,\n","                 nb_filters=64,\n","                 kernel_size=2,\n","                 nb_stacks=1,\n","                 dilations=(1, 2, 4, 8, 16, 32),\n","                 padding='causal',\n","                 use_skip_connections=False,\n","                 dropout_rate=0.0,\n","                 return_sequences=False,\n","                 activation='relu',\n","                 kernel_initializer='he_normal',\n","                 use_batch_norm=False,\n","                 use_layer_norm=False,\n","                 use_weight_norm=False,\n","                 **kwargs):\n","\n","        self.return_sequences = return_sequences\n","        self.dropout_rate = dropout_rate\n","        self.use_skip_connections = use_skip_connections\n","        self.dilations = dilations\n","        self.nb_stacks = nb_stacks\n","        self.kernel_size = kernel_size\n","        self.nb_filters = nb_filters\n","        self.activation = activation\n","        self.padding = padding\n","        self.kernel_initializer = kernel_initializer\n","        self.use_batch_norm = use_batch_norm\n","        self.use_layer_norm = use_layer_norm\n","        self.use_weight_norm = use_weight_norm\n","        self.skip_connections = []\n","        self.residual_blocks = []\n","        self.layers_outputs = []\n","        self.build_output_shape = None\n","        self.slicer_layer = None  # in case return_sequence=False\n","        self.output_slice_index = None  # in case return_sequence=False\n","        self.padding_same_and_time_dim_unknown = False  # edge case if padding='same' and time_dim = None\n","\n","        if self.use_batch_norm + self.use_layer_norm + self.use_weight_norm > 1:\n","            raise ValueError('Only one normalization can be specified at once.')\n","\n","        if isinstance(self.nb_filters, list):\n","            assert len(self.nb_filters) == len(self.dilations)\n","\n","        if padding != 'causal' and padding != 'same':\n","            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n","\n","        # initialize parent class\n","        super(TCN, self).__init__(**kwargs)\n","\n","    @property\n","    def receptive_field(self):\n","        return 1 + self.nb_stacks * sum([d * self.kernel_size for d in self.dilations])\n","\n","    def build(self, input_shape):\n","\n","        # member to hold current output shape of the layer for building purposes\n","        self.build_output_shape = input_shape\n","\n","        # list to hold all the member ResidualBlocks\n","        self.residual_blocks = []\n","        total_num_blocks = self.nb_stacks * len(self.dilations)\n","        if not self.use_skip_connections:\n","            total_num_blocks += 1  # cheap way to do a false case for below\n","\n","        for s in range(self.nb_stacks):\n","            for i, d in enumerate(self.dilations):\n","                res_block_filters = self.nb_filters[i] if isinstance(self.nb_filters, list) else self.nb_filters\n","                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n","                                                          nb_filters=res_block_filters,\n","                                                          kernel_size=self.kernel_size,\n","                                                          padding=self.padding,\n","                                                          activation=self.activation,\n","                                                          dropout_rate=self.dropout_rate,\n","                                                          use_batch_norm=self.use_batch_norm,\n","                                                          use_layer_norm=self.use_layer_norm,\n","                                                          use_weight_norm=self.use_weight_norm,\n","                                                          kernel_initializer=self.kernel_initializer,\n","                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n","                # build newest residual block\n","                self.residual_blocks[-1].build(self.build_output_shape)\n","                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n","\n","        # this is done to force keras to add the layers in the list to self._layers\n","        for layer in self.residual_blocks:\n","            self.__setattr__(layer.name, layer)\n","\n","        self.output_slice_index = None\n","        if self.padding == 'same':\n","            time = self.build_output_shape.as_list()[1]\n","            if time is not None:  # if time dimension is defined. e.g. shape = (bs, 500, input_dim).\n","                self.output_slice_index = int(self.build_output_shape.as_list()[1] / 2)\n","            else:\n","                # It will known at call time. c.f. self.call.\n","                self.padding_same_and_time_dim_unknown = True\n","\n","        else:\n","            self.output_slice_index = -1  # causal case.\n","        self.slicer_layer = Lambda(lambda tt: tt[:, self.output_slice_index, :])\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\"\n","        Overridden in case keras uses it somewhere... no idea. Just trying to avoid future errors.\n","        \"\"\"\n","        if not self.built:\n","            self.build(input_shape)\n","        if not self.return_sequences:\n","            batch_size = self.build_output_shape[0]\n","            batch_size = batch_size.value if hasattr(batch_size, 'value') else batch_size\n","            nb_filters = self.build_output_shape[-1]\n","            return [batch_size, nb_filters]\n","        else:\n","            # Compatibility tensorflow 1.x\n","            return [v.value if hasattr(v, 'value') else v for v in self.build_output_shape]\n","\n","    def call(self, inputs, training=None):\n","        x = inputs\n","        self.layers_outputs = [x]\n","        self.skip_connections = []\n","        for layer in self.residual_blocks:\n","            try:\n","                x, skip_out = layer(x, training=training)\n","            except TypeError:  # compatibility with tensorflow 1.x\n","                x, skip_out = layer(K.cast(x, 'float32'), training=training)\n","            self.skip_connections.append(skip_out)\n","            self.layers_outputs.append(x)\n","\n","        if self.use_skip_connections:\n","            x = layers.add(self.skip_connections)\n","            self.layers_outputs.append(x)\n","\n","        if not self.return_sequences:\n","            # case: time dimension is unknown. e.g. (bs, None, input_dim).\n","            if self.padding_same_and_time_dim_unknown:\n","                self.output_slice_index = K.shape(self.layers_outputs[-1])[1] // 2\n","            x = self.slicer_layer(x)\n","            self.layers_outputs.append(x)\n","        return x\n","\n","    def get_config(self):\n","        \"\"\"\n","        Returns the config of a the layer. This is used for saving and loading from a model\n","        :return: python dictionary with specs to rebuild layer\n","        \"\"\"\n","        config = super(TCN, self).get_config()\n","        config['nb_filters'] = self.nb_filters\n","        config['kernel_size'] = self.kernel_size\n","        config['nb_stacks'] = self.nb_stacks\n","        config['dilations'] = self.dilations\n","        config['padding'] = self.padding\n","        config['use_skip_connections'] = self.use_skip_connections\n","        config['dropout_rate'] = self.dropout_rate\n","        config['return_sequences'] = self.return_sequences\n","        config['activation'] = self.activation\n","        config['use_batch_norm'] = self.use_batch_norm\n","        config['use_layer_norm'] = self.use_layer_norm\n","        config['use_weight_norm'] = self.use_weight_norm\n","        config['kernel_initializer'] = self.kernel_initializer\n","        return config\n","\n","\n","def compiled_tcn(num_feat,  # type: int\n","                 num_classes,  # type: int\n","                 nb_filters,  # type: int\n","                 kernel_size,  # type: int\n","                 dilations,  # type: List[int]\n","                 nb_stacks,  # type: int\n","                 max_len,  # type: int\n","                 output_len=1,  # type: int\n","                 padding='causal',  # type: str\n","                 use_skip_connections=False,  # type: bool\n","                 return_sequences=True,\n","                 regression=False,  # type: bool\n","                 dropout_rate=0.05,  # type: float\n","                 name='tcn',  # type: str,\n","                 kernel_initializer='he_normal',  # type: str,\n","                 activation='relu',  # type:str,\n","                 opt='adam',\n","                 lr=0.002,\n","                 use_batch_norm=False,\n","                 use_layer_norm=False,\n","                 use_weight_norm=False):\n","    # type: (...) -> Model\n","    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n","    Classification uses a sparse categorical loss. Please input class ids and not one-hot encodings.\n","    Args:\n","        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n","        num_classes: The size of the final dense layer, how many classes we are predicting.\n","        nb_filters: The number of filters to use in the convolutional layers.\n","        kernel_size: The size of the kernel to use in each convolutional layer.\n","        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n","        nb_stacks : The number of stacks of residual blocks to use.\n","        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n","        padding: The padding to use in the convolutional layers.\n","        use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n","        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n","        regression: Whether the output should be continuous or discrete.\n","        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","        activation: The activation used in the residual blocks o = Activation(x + F(x)).\n","        name: Name of the model. Useful when having multiple TCN.\n","        kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n","        opt: Optimizer name.\n","        lr: Learning rate.\n","        use_batch_norm: Whether to use batch normalization in the residual layers or not.\n","        use_layer_norm: Whether to use layer normalization in the residual layers or not.\n","        use_weight_norm: Whether to use weight normalization in the residual layers or not.\n","    Returns:\n","        A compiled keras TCN.\n","    \"\"\"\n","\n","    dilations = adjust_dilations(dilations)\n","\n","    input_layer = Input(shape=(max_len, num_feat))\n","\n","    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, padding,\n","            use_skip_connections, dropout_rate, return_sequences,\n","            activation, kernel_initializer, use_batch_norm, use_layer_norm,\n","            use_weight_norm, name=name)(input_layer)\n","\n","    print('x.shape=', x.shape)\n","\n","    def get_opt():\n","        if opt == 'adam':\n","            return optimizers.Adam(lr=lr, clipnorm=1.)\n","        elif opt == 'rmsprop':\n","            return optimizers.RMSprop(lr=lr, clipnorm=1.)\n","        else:\n","            raise Exception('Only Adam and RMSProp are available here')\n","\n","    if not regression:\n","        # classification\n","        x = Dense(num_classes)(x)\n","        x = Activation('softmax')(x)\n","        output_layer = x\n","        model = Model(input_layer, output_layer)\n","\n","        # https://github.com/keras-team/keras/pull/11373\n","        # It's now in Keras@master but still not available with pip.\n","        # TODO remove later.\n","        def accuracy(y_true, y_pred):\n","            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n","            if K.ndim(y_true) == K.ndim(y_pred):\n","                y_true = K.squeeze(y_true, -1)\n","            # convert dense predictions to labels\n","            y_pred_labels = K.argmax(y_pred, axis=-1)\n","            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n","            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n","\n","        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=[accuracy])\n","    else:\n","        # regression\n","        x = Dense(output_len)(x)\n","        x = Activation('linear')(x)\n","        output_layer = x\n","        model = Model(input_layer, output_layer)\n","        model.compile(get_opt(), loss='mean_squared_error')\n","    print('model.x = {}'.format(input_layer.shape))\n","    print('model.y = {}'.format(output_layer.shape))\n","    return model\n","\n","\n","def tcn_full_summary(model: Model, expand_residual_blocks=True):\n","    layers = model._layers.copy()  # store existing layers\n","    model._layers.clear()  # clear layers\n","\n","    for i in range(len(layers)):\n","        if isinstance(layers[i], TCN):\n","            for layer in layers[i]._layers:\n","                if not isinstance(layer, ResidualBlock):\n","                    if not hasattr(layer, '__iter__'):\n","                        model._layers.append(layer)\n","                else:\n","                    if expand_residual_blocks:\n","                        for lyr in layer._layers:\n","                            if not hasattr(lyr, '__iter__'):\n","                                model._layers.append(lyr)\n","                    else:\n","                        model._layers.append(layer)\n","        else:\n","            model._layers.append(layers[i])\n","\n","    model.summary()  # print summary\n","\n","    # restore original layers\n","    model._layers.clear()\n","    [model._layers.append(lyr) for lyr in layers]\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYqQyzc8cwgF","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1632655881577,"user_tz":-480,"elapsed":7,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"a481202b-251e-4b32-84da-57b84203ba74"},"source":["'''\n","from keras.utils import to_categorical\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Multiply, Lambda\n","from keras.layers import LSTM, GRU, Embedding, Flatten, Reshape, Layer, merge, Average, Lambda, Conv2D, MaxPool2D, GlobalAveragePooling2D, Conv1D, MaxPooling1D, concatenate, Input, Bidirectional, RepeatVector, Concatenate, Dot, Permute, TimeDistributed, SpatialDropout1D\n","'''\n","#from keras_self_attention import SeqSelfAttention\n","#from tcn import TCN, tcn_full_summary\n","#from attention import Attention"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfrom keras.utils import to_categorical\\nfrom keras.models import Sequential, Model\\nfrom keras.layers import Dense, Multiply, Lambda\\nfrom keras.layers import LSTM, GRU, Embedding, Flatten, Reshape, Layer, merge, Average, Lambda, Conv2D, MaxPool2D, GlobalAveragePooling2D, Conv1D, MaxPooling1D, concatenate, Input, Bidirectional, RepeatVector, Concatenate, Dot, Permute, TimeDistributed, SpatialDropout1D\\n'"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"fHHpncfNBYPV","executionInfo":{"status":"ok","timestamp":1632655881578,"user_tz":-480,"elapsed":6,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["#tri-positional encoding\n","from tensorflow.keras import backend as K\n","\n","class TrigPosEmbedding(keras.layers.Layer):\n","    \"\"\"Position embedding use sine and cosine functions.\n","    See: https://arxiv.org/pdf/1706.03762\n","    Expand mode:\n","        # Input shape\n","            2D tensor with shape: `(batch_size, sequence_length)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n","    Add mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","    Concat mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim + output_dim)`.\n","    \"\"\"\n","    MODE_EXPAND = 'expand'\n","    MODE_ADD = 'add'\n","    MODE_CONCAT = 'concat'\n","\n","    def __init__(self,\n","                 mode=MODE_ADD,\n","                 output_dim=None,\n","                 **kwargs):\n","        \"\"\"\n","        :param output_dim: The embedding dimension.\n","        :param kwargs:\n","        \"\"\"\n","        if mode in [self.MODE_EXPAND, self.MODE_CONCAT]:\n","            if output_dim is None:\n","                raise NotImplementedError('`output_dim` is required in `%s` mode' % mode)\n","            if output_dim % 2 != 0:\n","                raise NotImplementedError('It does not make sense to use an odd output dimension: %d' % output_dim)\n","        self.mode = mode\n","        self.output_dim = output_dim\n","        self.supports_masking = True\n","        super(TrigPosEmbedding, self).__init__(**kwargs)\n","\n","    def get_config(self):\n","        config = {\n","            'mode': self.mode,\n","            'output_dim': self.output_dim,\n","        }\n","        base_config = super(TrigPosEmbedding, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.mode == self.MODE_EXPAND:\n","            return input_shape + (self.output_dim,)\n","        if self.mode == self.MODE_CONCAT:\n","            return input_shape[:-1] + (input_shape[-1] + self.output_dim,)\n","        return input_shape\n","\n","    def call(self, inputs, mask=None):\n","        input_shape = K.shape(inputs)\n","        if self.mode == self.MODE_ADD:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n","            pos_input = K.tile(K.expand_dims(K.arange(0, seq_len), axis=0), [batch_size, 1])\n","        elif self.mode == self.MODE_CONCAT:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], self.output_dim\n","            pos_input = K.tile(K.expand_dims(K.arange(0, seq_len), axis=0), [batch_size, 1])\n","        else:\n","            output_dim = self.output_dim\n","            pos_input = inputs\n","        if K.dtype(pos_input) != K.floatx():\n","            pos_input = K.cast(pos_input, K.floatx())\n","        evens = K.arange(0, output_dim // 2) * 2\n","        odds = K.arange(0, output_dim // 2) * 2 + 1\n","        even_embd = K.sin(\n","            K.dot(\n","                K.expand_dims(pos_input, -1),\n","                K.expand_dims(1.0 / K.pow(\n","                    10000.0,\n","                    K.cast(evens, K.floatx()) / K.cast(output_dim, K.floatx())\n","                ), 0)\n","            )\n","        )\n","        odd_embd = K.cos(\n","            K.dot(\n","                K.expand_dims(pos_input, -1),\n","                K.expand_dims(1.0 / K.pow(\n","                    10000.0, K.cast((odds - 1), K.floatx()) / K.cast(output_dim, K.floatx())\n","                ), 0)\n","            )\n","        )\n","        embd = K.stack([even_embd, odd_embd], axis=-1)\n","        output = K.reshape(embd, [-1, K.shape(inputs)[1], output_dim])\n","        if self.mode == self.MODE_CONCAT:\n","            output = K.concatenate([inputs, output], axis=-1)\n","        if self.mode == self.MODE_ADD:\n","            output += inputs\n","        return output"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"VO_ZQ-FeHWsV","executionInfo":{"status":"ok","timestamp":1632655882213,"user_tz":-480,"elapsed":640,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["#Trainable Embedding\n","\n","class PositionEmbedding(keras.layers.Layer):\n","    \"\"\"Turn integers (positions) into dense vectors of fixed size.\n","    eg. [[-4], [10]] -> [[0.25, 0.1], [0.6, -0.2]]\n","    Expand mode: negative integers (relative position) could be used in this mode.\n","        # Input shape\n","            2D tensor with shape: `(batch_size, sequence_length)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n","    Add mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","    Concat mode:\n","        # Input shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n","        # Output shape\n","            3D tensor with shape: `(batch_size, sequence_length, feature_dim + output_dim)`.\n","    \"\"\"\n","    MODE_EXPAND = 'expand'\n","    MODE_ADD = 'add'\n","    MODE_CONCAT = 'concat'\n","\n","    def __init__(self,\n","                 input_dim,\n","                 output_dim,\n","                 mode=MODE_EXPAND,\n","                 embeddings_initializer='uniform',\n","                 embeddings_regularizer=None,\n","                 activity_regularizer=None,\n","                 embeddings_constraint=None,\n","                 mask_zero=False,\n","                 **kwargs):\n","        \"\"\"\n","        :param input_dim: The maximum absolute value of positions.\n","        :param output_dim: The embedding dimension.\n","        :param embeddings_initializer:\n","        :param embeddings_regularizer:\n","        :param activity_regularizer:\n","        :param embeddings_constraint:\n","        :param mask_zero: The index that represents padding. Only works in `append` mode.\n","        :param kwargs:\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.mode = mode\n","        self.embeddings_initializer = keras.initializers.get(embeddings_initializer)\n","        self.embeddings_regularizer = keras.regularizers.get(embeddings_regularizer)\n","        self.activity_regularizer = keras.regularizers.get(activity_regularizer)\n","        self.embeddings_constraint = keras.constraints.get(embeddings_constraint)\n","        self.mask_zero = mask_zero\n","        self.supports_masking = mask_zero is not False\n","\n","        self.embeddings = None\n","        super(PositionEmbedding, self).__init__(**kwargs)\n","\n","    def get_config(self):\n","        config = {'input_dim': self.input_dim,\n","                  'output_dim': self.output_dim,\n","                  'mode': self.mode,\n","                  'embeddings_initializer': keras.initializers.serialize(self.embeddings_initializer),\n","                  'embeddings_regularizer': keras.regularizers.serialize(self.embeddings_regularizer),\n","                  'activity_regularizer': keras.regularizers.serialize(self.activity_regularizer),\n","                  'embeddings_constraint': keras.constraints.serialize(self.embeddings_constraint),\n","                  'mask_zero': self.mask_zero}\n","        base_config = super(PositionEmbedding, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def build(self, input_shape):\n","        if self.mode == self.MODE_EXPAND:\n","            self.embeddings = self.add_weight(\n","                shape=(self.input_dim * 2 + 1, self.output_dim),\n","                initializer=self.embeddings_initializer,\n","                name='embeddings',\n","                regularizer=self.embeddings_regularizer,\n","                constraint=self.embeddings_constraint,\n","            )\n","        else:\n","            self.embeddings = self.add_weight(\n","                shape=(self.input_dim, self.output_dim),\n","                initializer=self.embeddings_initializer,\n","                name='embeddings',\n","                regularizer=self.embeddings_regularizer,\n","                constraint=self.embeddings_constraint,\n","            )\n","        super(PositionEmbedding, self).build(input_shape)\n","\n","    def compute_mask(self, inputs, mask=None):\n","        if self.mode == self.MODE_EXPAND:\n","            if self.mask_zero:\n","                output_mask = K.not_equal(inputs, self.mask_zero)\n","            else:\n","                output_mask = None\n","        else:\n","            output_mask = mask\n","        return output_mask\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.mode == self.MODE_EXPAND:\n","            return input_shape + (self.output_dim,)\n","        if self.mode == self.MODE_CONCAT:\n","            return input_shape[:-1] + (input_shape[-1] + self.output_dim,)\n","        return input_shape\n","\n","    def call(self, inputs, **kwargs):\n","        if self.mode == self.MODE_EXPAND:\n","            if K.dtype(inputs) != 'int32':\n","                inputs = K.cast(inputs, 'int32')\n","            return K.gather(\n","                self.embeddings,\n","                K.minimum(K.maximum(inputs, -self.input_dim), self.input_dim) + self.input_dim,\n","            )\n","        input_shape = K.shape(inputs)\n","        if self.mode == self.MODE_ADD:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n","        else:\n","            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], self.output_dim\n","        pos_embeddings = K.tile(\n","            K.expand_dims(self.embeddings[:seq_len, :self.output_dim], axis=0),\n","            [batch_size, 1, 1],\n","        )\n","        if self.mode == self.MODE_ADD:\n","            return inputs + pos_embeddings\n","        return K.concatenate([inputs, pos_embeddings], axis=-1)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Pjzua27aIwb","executionInfo":{"status":"ok","timestamp":1632655882214,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":[""],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSF2T5WVSxPK","executionInfo":{"status":"ok","timestamp":1632655884188,"user_tz":-480,"elapsed":1978,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"0662b9d3-f604-4726-8429-2e4ab6646db4"},"source":["#读写\n","#keras.backend.set_image_data_format('channels_first')\n","input_RW=Input(shape=(look_back,))\n","em_R=Embedding(2, 32, input_length=look_back)(input_RW)\n","em_R=Dropout(0.2)(em_R)\n","#R=Reshape((look_back,1))(input_RW)\n","\n","#Delta值\n","input_D=Input(shape=(look_back,))\n","em=Embedding(type_new, 32, input_length=look_back)(input_D)\n","em=TrigPosEmbedding(output_dim=look_back, mode=TrigPosEmbedding.MODE_ADD)(em)\n","#em=PositionEmbedding(input_dim=type_new , output_dim=look_back, mode=PositionEmbedding.MODE_ADD)(em)\n","\n","#将RW和Delta合为一体\n","merges = Add()([em, em_R])\n","#merges = Concatenate(axis=2)([em, em_R])\n","#merges = Reshape((look_back, 32, 1))(merges)\n","#merges = Reshape((1, look_back, 32))(merges)\n","spec_cnn = merges\n","\n","pool_size = [2, 1]\n","nb_cnn2d_filt = 128\n","dropout_rate = 0.0\n","\n","# CONVOLUTIONAL LAYERS =========================================================\n","for i, convCnt in enumerate(pool_size):\n","    spec_cnn = Conv1D(filters=nb_cnn2d_filt, kernel_size=(3), padding='same')(spec_cnn)\n","    #spec_cnn = Conv2D(filters=nb_cnn2d_filt, kernel_size=(3, 3), padding='same')(spec_cnn)\n","    spec_cnn = BatchNormalization()(spec_cnn)\n","    spec_cnn = Activation('relu')(spec_cnn)\n","    spec_cnn = MaxPooling1D(pool_size=(pool_size[i]))(spec_cnn)\n","    #spec_cnn = MaxPooling2D(pool_size=(1, pool_size[i]))(spec_cnn)\n","    spec_cnn = Dropout(dropout_rate)(spec_cnn)\n","#spec_cnn = Permute((2, 1, 3))(spec_cnn)\n","#resblock_input = Reshape((50, -1))(spec_cnn)\n","#resblock_input = TimeDistributed((Dense(128)))(resblock_input)\n","resblock_input = spec_cnn\n","\n","# TCN layer ===================================================================\n","# residual blocks ------------------------\n","skip_connections = []\n","\n","for d in range(6):\n","\n","        # 1D convolution\n","    spec_conv1d = keras.layers.Convolution1D(filters=256,\n","                                                kernel_size=(3),\n","                                                padding='same',\n","                                                dilation_rate=2**d)(resblock_input)\n","    spec_conv1d = BatchNormalization()(spec_conv1d)\n","\n","        # activations\n","    tanh_out = keras.layers.Activation('tanh')(spec_conv1d)\n","    sigm_out = keras.layers.Activation('sigmoid')(spec_conv1d)\n","    spec_act = keras.layers.Multiply()([tanh_out, sigm_out])\n","\n","        # spatial dropout\n","    spec_drop = keras.layers.SpatialDropout1D(rate=0.5)(spec_act)\n","\n","        # 1D convolution\n","    skip_output = keras.layers.Convolution1D(filters=128,\n","                                                 kernel_size=(1),\n","                                                 padding='same')(spec_drop)\n","\n","    res_output = keras.layers.Add()([resblock_input, skip_output])\n","\n","    if skip_output is not None:\n","        skip_connections.append(skip_output)\n","\n","    resblock_input = res_output\n","# ---------------------------------------\n","# Residual blocks sum\n","spec_sum = keras.layers.Add()(skip_connections)\n","spec_sum = keras.layers.Activation('relu')(spec_sum)\n","\n","    # 1D convolution\n","spec_conv1d_2 = keras.layers.Convolution1D(filters=128,\n","                                          kernel_size=(1),\n","                                          padding='same')(spec_sum)\n","spec_conv1d_2 = keras.layers.Activation('relu')(spec_conv1d_2)\n","\n","    # 1D convolution\n","spec_tcn = keras.layers.Convolution1D(filters=128,\n","                                          kernel_size=(1),\n","                                          padding='same')(spec_conv1d_2)\n","spec_tcn = keras.layers.Activation('tanh')(spec_tcn)\n","\n","#attention\n","#attention = Attention(128)(spec_tcn)\n","#attention = RepeatVector(50)(attention)\n","\n","#TCN调库\n","#tcn1=TCN(kernel_size=5, nb_stacks=2,return_sequences=True, dilations=(1,2,4,8,16,32), use_layer_norm=True)(em) #merges\n","#conv_out = Conv1D(8, 1, activation=\"relu\")(tcn1)\n","\n","fnn_size = [128, 128]\n","# Delta_part ==================================================================\n","Delta_part = spec_tcn #attention\n","for nb_fnn_filt in fnn_size:\n","    Delta_part = TimeDistributed(Dense(nb_fnn_filt))(Delta_part)\n","    Delta_part = Dropout(dropout_rate)(Delta_part)\n","Delta_part = Flatten()(Delta_part)\n","Delta_part = Dense(num_bit*pridict_size)(Delta_part)\n","Delta_part = Reshape((pridict_size, num_bit))(Delta_part)\n","Delta_part = Activation('sigmoid', name='Delta_part')(Delta_part)\n","\n","    # RW_part ==================================================================\n","RW_part = spec_tcn\n","for nb_fnn_filt in fnn_size:\n","    RW_part = TimeDistributed(Dense(nb_fnn_filt))(RW_part)\n","    RW_part = Dropout(dropout_rate)(RW_part)\n","\n","RW_part = Flatten()(RW_part)\n","RW_part = Dense(1*pridict_size)(RW_part)\n","RW_part = Reshape((pridict_size, 1))(RW_part)\n","RW_part = Activation('sigmoid', name='RW_part')(RW_part)\n","\n","# Output of convolutional layers\n","#conv_out = Flatten()(conv_out)\n","\n","\n","# Concatenate with categorical features\n","#x = concatenate([conv_out] + cat_flatten)\n","#x = Dense(pridict_size*num_bit, activation=\"relu\")(conv_out) #Delta_part\n","#x = Reshape((pridict_size,num_bit))(x)\n","#outputs = Activation('sigmoid')(x)\n","\n","\n","\n","# Define model interface\n","model = Model(inputs=[input_D, input_RW], outputs=[Delta_part, RW_part]) #, input_RW , outputs_rw\n","model.compile(loss=['binary_crossentropy', 'binary_crossentropy'], optimizer='adam', metrics=['acc'])#tf.compat.v1.keras.metrics.binary_accuracy\n","early_stopping=EarlyStopping(monitor='val_loss',patience=15,verbose=2)\n","print(model.summary())"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 80)]         0                                            \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            [(None, 80)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 80, 32)       15328       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 80, 32)       64          input_1[0][0]                    \n","__________________________________________________________________________________________________\n","trig_pos_embedding (TrigPosEmbe (None, 80, 32)       0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 80, 32)       0           embedding[0][0]                  \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 80, 32)       0           trig_pos_embedding[0][0]         \n","                                                                 dropout[0][0]                    \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 80, 128)      12416       add[0][0]                        \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 80, 128)      512         conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 80, 128)      0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","max_pooling1d (MaxPooling1D)    (None, 40, 128)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 40, 128)      0           max_pooling1d[0][0]              \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 40, 128)      49280       dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 40, 128)      512         conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 40, 128)      0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling1d_1 (MaxPooling1D)  (None, 40, 128)      0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 40, 128)      0           max_pooling1d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 40, 256)      98560       dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 40, 256)      1024        conv1d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 40, 256)      0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 40, 256)      0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","multiply (Multiply)             (None, 40, 256)      0           activation_2[0][0]               \n","                                                                 activation_3[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d (SpatialDropo (None, 40, 256)      0           multiply[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_3 (Conv1D)               (None, 40, 128)      32896       spatial_dropout1d[0][0]          \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 40, 128)      0           dropout_2[0][0]                  \n","                                                                 conv1d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_4 (Conv1D)               (None, 40, 256)      98560       add_1[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 40, 256)      1024        conv1d_4[0][0]                   \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 40, 256)      0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 40, 256)      0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, 40, 256)      0           activation_4[0][0]               \n","                                                                 activation_5[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d_1 (SpatialDro (None, 40, 256)      0           multiply_1[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_5 (Conv1D)               (None, 40, 128)      32896       spatial_dropout1d_1[0][0]        \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 40, 128)      0           add_1[0][0]                      \n","                                                                 conv1d_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_6 (Conv1D)               (None, 40, 256)      98560       add_2[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 40, 256)      1024        conv1d_6[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 40, 256)      0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 40, 256)      0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, 40, 256)      0           activation_6[0][0]               \n","                                                                 activation_7[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d_2 (SpatialDro (None, 40, 256)      0           multiply_2[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_7 (Conv1D)               (None, 40, 128)      32896       spatial_dropout1d_2[0][0]        \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 40, 128)      0           add_2[0][0]                      \n","                                                                 conv1d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_8 (Conv1D)               (None, 40, 256)      98560       add_3[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 40, 256)      1024        conv1d_8[0][0]                   \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 40, 256)      0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 40, 256)      0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","multiply_3 (Multiply)           (None, 40, 256)      0           activation_8[0][0]               \n","                                                                 activation_9[0][0]               \n","__________________________________________________________________________________________________\n","spatial_dropout1d_3 (SpatialDro (None, 40, 256)      0           multiply_3[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_9 (Conv1D)               (None, 40, 128)      32896       spatial_dropout1d_3[0][0]        \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 40, 128)      0           add_3[0][0]                      \n","                                                                 conv1d_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_10 (Conv1D)              (None, 40, 256)      98560       add_4[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 40, 256)      1024        conv1d_10[0][0]                  \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 40, 256)      0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 40, 256)      0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","multiply_4 (Multiply)           (None, 40, 256)      0           activation_10[0][0]              \n","                                                                 activation_11[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_4 (SpatialDro (None, 40, 256)      0           multiply_4[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_11 (Conv1D)              (None, 40, 128)      32896       spatial_dropout1d_4[0][0]        \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 40, 128)      0           add_4[0][0]                      \n","                                                                 conv1d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_12 (Conv1D)              (None, 40, 256)      98560       add_5[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 40, 256)      1024        conv1d_12[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 40, 256)      0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 40, 256)      0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","multiply_5 (Multiply)           (None, 40, 256)      0           activation_12[0][0]              \n","                                                                 activation_13[0][0]              \n","__________________________________________________________________________________________________\n","spatial_dropout1d_5 (SpatialDro (None, 40, 256)      0           multiply_5[0][0]                 \n","__________________________________________________________________________________________________\n","conv1d_13 (Conv1D)              (None, 40, 128)      32896       spatial_dropout1d_5[0][0]        \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 40, 128)      0           conv1d_3[0][0]                   \n","                                                                 conv1d_5[0][0]                   \n","                                                                 conv1d_7[0][0]                   \n","                                                                 conv1d_9[0][0]                   \n","                                                                 conv1d_11[0][0]                  \n","                                                                 conv1d_13[0][0]                  \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 40, 128)      0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","conv1d_14 (Conv1D)              (None, 40, 128)      16512       activation_14[0][0]              \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 40, 128)      0           conv1d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_15 (Conv1D)              (None, 40, 128)      16512       activation_15[0][0]              \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 40, 128)      0           conv1d_15[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 40, 128)      16512       activation_16[0][0]              \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 40, 128)      16512       activation_16[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 40, 128)      0           time_distributed[0][0]           \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 40, 128)      0           time_distributed_2[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 40, 128)      16512       dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 40, 128)      16512       dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 40, 128)      0           time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 40, 128)      0           time_distributed_3[0][0]         \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 5120)         0           dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 5120)         0           dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 18)           92178       flatten[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 2)            10242       flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 2, 9)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 2, 1)         0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","Delta_part (Activation)         (None, 2, 9)         0           reshape[0][0]                    \n","__________________________________________________________________________________________________\n","RW_part (Activation)            (None, 2, 1)         0           reshape_1[0][0]                  \n","==================================================================================================\n","Total params: 1,074,484\n","Trainable params: 1,070,900\n","Non-trainable params: 3,584\n","__________________________________________________________________________________________________\n","None\n"]}]},{"cell_type":"code","metadata":{"id":"p5gmEdpxq27K","executionInfo":{"status":"ok","timestamp":1632655884190,"user_tz":-480,"elapsed":7,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":["#prediction=model.predict([trainX[:5,:],trainX_RW[:5,:]])"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_2Bxgy8q94q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632656711317,"user_tz":-480,"elapsed":827133,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"29c59ce0-103b-42b6-f0eb-1dfac237721f"},"source":["#prediction\n","history=model.fit([trainX,trainX_RW],[trainY, trainY_RW], epochs=15, batch_size=128, \n","                  verbose=1\n","                  , validation_data=([testX,testX_RW], [testY,testY_RW]),\n","                  callbacks=[early_stopping])"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Train on 159918 samples, validate on 39918 samples\n","Epoch 1/15\n","159918/159918 [==============================] - ETA: 0s - loss: 0.7095 - Delta_part_loss: 0.4452 - RW_part_loss: 0.2641 - Delta_part_acc: 0.7814 - RW_part_acc: 0.8889"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"]},{"output_type":"stream","name":"stdout","text":["159918/159918 [==============================] - 73s 455us/sample - loss: 0.7095 - Delta_part_loss: 0.4452 - RW_part_loss: 0.2641 - Delta_part_acc: 0.7814 - RW_part_acc: 0.8889 - val_loss: 0.9037 - val_Delta_part_loss: 0.5044 - val_RW_part_loss: 0.3993 - val_Delta_part_acc: 0.7611 - val_RW_part_acc: 0.8276\n","Epoch 2/15\n","159918/159918 [==============================] - 54s 336us/sample - loss: 0.5190 - Delta_part_loss: 0.3444 - RW_part_loss: 0.1746 - Delta_part_acc: 0.8398 - RW_part_acc: 0.9359 - val_loss: 0.8527 - val_Delta_part_loss: 0.4797 - val_RW_part_loss: 0.3730 - val_Delta_part_acc: 0.7642 - val_RW_part_acc: 0.8215\n","Epoch 3/15\n","159918/159918 [==============================] - 54s 335us/sample - loss: 0.4707 - Delta_part_loss: 0.3113 - RW_part_loss: 0.1595 - Delta_part_acc: 0.8576 - RW_part_acc: 0.9429 - val_loss: 0.8626 - val_Delta_part_loss: 0.4728 - val_RW_part_loss: 0.3897 - val_Delta_part_acc: 0.7767 - val_RW_part_acc: 0.8225\n","Epoch 4/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.4409 - Delta_part_loss: 0.2905 - RW_part_loss: 0.1504 - Delta_part_acc: 0.8686 - RW_part_acc: 0.9465 - val_loss: 0.8337 - val_Delta_part_loss: 0.4665 - val_RW_part_loss: 0.3671 - val_Delta_part_acc: 0.7785 - val_RW_part_acc: 0.8344\n","Epoch 5/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.4210 - Delta_part_loss: 0.2758 - RW_part_loss: 0.1452 - Delta_part_acc: 0.8771 - RW_part_acc: 0.9490 - val_loss: 0.8312 - val_Delta_part_loss: 0.4534 - val_RW_part_loss: 0.3779 - val_Delta_part_acc: 0.7908 - val_RW_part_acc: 0.8265\n","Epoch 6/15\n","159918/159918 [==============================] - 53s 333us/sample - loss: 0.4049 - Delta_part_loss: 0.2641 - RW_part_loss: 0.1407 - Delta_part_acc: 0.8832 - RW_part_acc: 0.9510 - val_loss: 0.8249 - val_Delta_part_loss: 0.4565 - val_RW_part_loss: 0.3684 - val_Delta_part_acc: 0.7795 - val_RW_part_acc: 0.8478\n","Epoch 7/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3917 - Delta_part_loss: 0.2544 - RW_part_loss: 0.1374 - Delta_part_acc: 0.8883 - RW_part_acc: 0.9523 - val_loss: 0.8041 - val_Delta_part_loss: 0.4600 - val_RW_part_loss: 0.3442 - val_Delta_part_acc: 0.7773 - val_RW_part_acc: 0.8502\n","Epoch 8/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3795 - Delta_part_loss: 0.2453 - RW_part_loss: 0.1341 - Delta_part_acc: 0.8934 - RW_part_acc: 0.9536 - val_loss: 0.8529 - val_Delta_part_loss: 0.4597 - val_RW_part_loss: 0.3932 - val_Delta_part_acc: 0.7926 - val_RW_part_acc: 0.8157\n","Epoch 9/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3697 - Delta_part_loss: 0.2379 - RW_part_loss: 0.1319 - Delta_part_acc: 0.8981 - RW_part_acc: 0.9545 - val_loss: 0.8246 - val_Delta_part_loss: 0.4338 - val_RW_part_loss: 0.3908 - val_Delta_part_acc: 0.8001 - val_RW_part_acc: 0.8174\n","Epoch 10/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3604 - Delta_part_loss: 0.2309 - RW_part_loss: 0.1296 - Delta_part_acc: 0.9017 - RW_part_acc: 0.9555 - val_loss: 0.8255 - val_Delta_part_loss: 0.4632 - val_RW_part_loss: 0.3623 - val_Delta_part_acc: 0.7926 - val_RW_part_acc: 0.8455\n","Epoch 11/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3525 - Delta_part_loss: 0.2250 - RW_part_loss: 0.1274 - Delta_part_acc: 0.9048 - RW_part_acc: 0.9565 - val_loss: 0.8044 - val_Delta_part_loss: 0.4341 - val_RW_part_loss: 0.3703 - val_Delta_part_acc: 0.7978 - val_RW_part_acc: 0.8142\n","Epoch 12/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3447 - Delta_part_loss: 0.2194 - RW_part_loss: 0.1253 - Delta_part_acc: 0.9073 - RW_part_acc: 0.9571 - val_loss: 0.8519 - val_Delta_part_loss: 0.4568 - val_RW_part_loss: 0.3952 - val_Delta_part_acc: 0.7942 - val_RW_part_acc: 0.8226\n","Epoch 13/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3392 - Delta_part_loss: 0.2152 - RW_part_loss: 0.1240 - Delta_part_acc: 0.9095 - RW_part_acc: 0.9574 - val_loss: 0.7843 - val_Delta_part_loss: 0.4299 - val_RW_part_loss: 0.3545 - val_Delta_part_acc: 0.8002 - val_RW_part_acc: 0.8412\n","Epoch 14/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3344 - Delta_part_loss: 0.2119 - RW_part_loss: 0.1225 - Delta_part_acc: 0.9116 - RW_part_acc: 0.9582 - val_loss: 0.8133 - val_Delta_part_loss: 0.4294 - val_RW_part_loss: 0.3839 - val_Delta_part_acc: 0.7985 - val_RW_part_acc: 0.8112\n","Epoch 15/15\n","159918/159918 [==============================] - 53s 334us/sample - loss: 0.3281 - Delta_part_loss: 0.2076 - RW_part_loss: 0.1205 - Delta_part_acc: 0.9134 - RW_part_acc: 0.9591 - val_loss: 0.8239 - val_Delta_part_loss: 0.4238 - val_RW_part_loss: 0.4000 - val_Delta_part_acc: 0.8054 - val_RW_part_acc: 0.8215\n"]}]},{"cell_type":"code","metadata":{"id":"EVS9W5tciC5I","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1632656711317,"user_tz":-480,"elapsed":19,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"6ac0c9e4-94f3-46cc-b885-ad6a54ec7949"},"source":["'''\n","history=model.fit([trainX,trainX_RW],[trainY, trainY_RW], epochs=20, batch_size=128, \n","                  verbose=1\n","                  , validation_data=([testX,testX_RW], [testY,testY_RW]),\n","                  callbacks=[early_stopping])\n","                  \n","history=model.fit([trainX],[trainY], epochs=20, batch_size=128, \n","                  verbose=1\n","                  , validation_data=([testX], [testY]),\n","                  callbacks=[early_stopping])\n","\n","history=model.fit([trainX,trainRW_X],trainY, epochs=5, batch_size=32, \n","                  verbose=1, validation_data=([testX,testRW_X], testY),\n","                  callbacks=[early_stopping])\n","'''\n","'''\n","history=model.fit(trainRW_X, trainRW_Y, \n","                  epochs=20, batch_size=32, \n","                  verbose=1, validation_data=(testRW_X, testRW_Y),\n","                  callbacks=[early_stopping])'''"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nhistory=model.fit(trainRW_X, trainRW_Y, \\n                  epochs=20, batch_size=32, \\n                  verbose=1, validation_data=(testRW_X, testRW_Y),\\n                  callbacks=[early_stopping])'"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"akGL7oWxmaBl","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1632656711318,"user_tz":-480,"elapsed":9,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"c4623714-a950-4bb6-b2a2-9e240d813590"},"source":["plt.plot(history.history['Delta_part_acc'], label='trainD')\n","plt.plot(history.history['RW_part_acc'], label='trainRW')\n","plt.plot(history.history['val_Delta_part_acc'], label='testD')\n","plt.plot(history.history['val_RW_part_acc'], label='testRW')\n","plt.legend()\n","plt.show()"],"execution_count":31,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"KfRNS26Eksy9","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1632656711318,"user_tz":-480,"elapsed":8,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"dde610be-115c-40b3-c34e-6305d4e14622"},"source":["'''\n","plt.plot(history.history['activation_48_acc'], label='trainD')\n","plt.plot(history.history['activation_49_acc'], label='trainRW')\n","plt.plot(history.history['val_activation_48_acc'], label='testD')\n","plt.plot(history.history['val_activation_49_acc'], label='testRW')\n","plt.legend()\n","plt.show()\n","'''"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nplt.plot(history.history['activation_48_acc'], label='trainD')\\nplt.plot(history.history['activation_49_acc'], label='trainRW')\\nplt.plot(history.history['val_activation_48_acc'], label='testD')\\nplt.plot(history.history['val_activation_49_acc'], label='testRW')\\nplt.legend()\\nplt.show()\\n\""]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"r8C7r5fhZxjq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632656711879,"user_tz":-480,"elapsed":569,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"9dceefa5-e697-4856-b0f9-33ad36c28313"},"source":["prediction = model.predict([trainX[:2,:],trainX_RW[:2,:]])\n","prediction"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"]},{"output_type":"execute_result","data":{"text/plain":["[array([[[9.99989748e-01, 9.99966145e-01, 9.99972105e-01, 3.16190487e-03,\n","          9.97837007e-01, 1.10621295e-04, 1.44757137e-01, 9.83624339e-01,\n","          9.88545477e-01],\n","         [1.94121258e-05, 9.99070883e-01, 9.99772251e-01, 9.99999046e-01,\n","          9.99996901e-01, 9.99976516e-01, 9.99933004e-01, 9.99912262e-01,\n","          1.32118425e-04]],\n"," \n","        [[1.50504246e-04, 9.98102248e-01, 9.99187648e-01, 9.96469498e-01,\n","          9.99333203e-01, 9.99886513e-01, 9.99692321e-01, 9.99952435e-01,\n","          6.80438243e-04],\n","         [3.58761102e-02, 9.84406590e-01, 9.71165660e-04, 2.46898923e-03,\n","          9.87495005e-01, 9.94497836e-01, 3.09503525e-01, 9.84340310e-01,\n","          2.16743238e-02]]], dtype=float32), array([[[2.5283868e-04],\n","         [9.9999595e-01]],\n"," \n","        [[9.9355376e-01],\n","         [6.8914151e-04]]], dtype=float32)]"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"W4BKiW5CeBsm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632656711879,"user_tz":-480,"elapsed":8,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"0d7d9ff9-ee27-42a0-f77c-f23e1a847d71"},"source":["trainY[0:2]"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[1, 1, 1, 0, 1, 0, 1, 1, 1],\n","        [0, 1, 1, 1, 1, 1, 1, 1, 0]],\n","\n","       [[0, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [0, 1, 0, 0, 1, 1, 0, 1, 0]]])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"tiahjcKPz2Mh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632656711880,"user_tz":-480,"elapsed":6,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"76120ff6-bbf7-4046-d54c-d958b870f0f2"},"source":["testY_RW[0:2]"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0],\n","        [0]],\n","\n","       [[0],\n","        [1]]])"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"gOIj8idxz1bP","executionInfo":{"status":"ok","timestamp":1632656711880,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":[""],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUSeKem-_QEJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632656714976,"user_tz":-480,"elapsed":3099,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"b269f808-d3e3-4ca7-c943-00456fbc029b"},"source":["!pip3 install kmeans1d"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kmeans1d\n","  Downloading kmeans1d-0.3.1-cp37-cp37m-manylinux2014_x86_64.whl (93 kB)\n","\u001b[?25l\r\u001b[K     |███▌                            | 10 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 20 kB 39.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 30 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 40 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 51 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 61 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 71 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 81 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 92 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 93 kB 2.1 MB/s \n","\u001b[?25hInstalling collected packages: kmeans1d\n","Successfully installed kmeans1d-0.3.1\n"]}]},{"cell_type":"code","metadata":{"id":"hWjBBoQRV9QO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632656714976,"user_tz":-480,"elapsed":12,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}},"outputId":"d997f56c-4ef8-41b2-fa3e-6749d3066202"},"source":["import kmeans1d\n","\n","x = [4.0, 4.1, 4.2, -50, 200.2, 200.4, 200.9, 80, 100, 102]\n","k = 4\n","\n","clusters, centroids = kmeans1d.cluster(x, k)\n","\n","print(clusters)   # [1, 1, 1, 0, 3, 3, 3, 2, 2, 2]\n","print(centroids)  # [-50.0, 4.1, 94.0, 200.5]"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1, 1, 0, 3, 3, 3, 2, 2, 2]\n","[-50.0, 4.1, 94.0, 200.5]\n"]}]},{"cell_type":"code","metadata":{"id":"xEos9ltCWAXc","executionInfo":{"status":"ok","timestamp":1632656714977,"user_tz":-480,"elapsed":8,"user":{"displayName":"Jacky Free","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10613091817770638083"}}},"source":[""],"execution_count":37,"outputs":[]}]}